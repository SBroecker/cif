[
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html",
    "href": "Privacy Models for EHealth - Intro and Scope.html",
    "title": "Privacy Models for E-Health",
    "section": "",
    "text": "cover.png"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#use-case-physician-business-linkage-project",
    "href": "Privacy Models for EHealth - Intro and Scope.html#use-case-physician-business-linkage-project",
    "title": "Privacy Models for E-Health",
    "section": "Use Case: Physician-Business Linkage Project",
    "text": "Use Case: Physician-Business Linkage Project\n\nMotivation: The business of healthcare continues to evolve rapidly—but surprisingly little is known about how this restructuring of the industry impacts cost, quality, and access.\n\n\nDrivers of continued change include:\n\n\nRapid consolidation of firms, incl. vertically into comprehensive health “systems”\nShift of physicians from small business owners to employees\nGrowing reliance on healthcare staffing agencies and outsourcing\nProminent and controversial role of private equity in industry restructuring\n\nContinued effects of changing regulatory environment (ACA)\nImpact of COVID-19 pandemic\n\n\n\n\nOpportunity: This gap in knowledge is in part because there is no single authoritative source of data on the supply-side of health care—and the available data sources each present their own tradeoffs and limitations. As a consequence, “even seemingly simple concepts, such as the number of practitioners may be more slippery than researchers realize” (Maurer et al., HSR, 2021). How can Census help bridge this gap?\nGoal: Leverage Census data on workers, taxpayers, businesses, and employers to enhance data on physicians and the institutions in which they work.\n\n\n\n\nmindmap.png"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#privacy-models-motivation",
    "href": "Privacy Models for EHealth - Intro and Scope.html#privacy-models-motivation",
    "title": "Privacy Models for E-Health",
    "section": "Privacy Models: Motivation",
    "text": "Privacy Models: Motivation\n\nProgram-level: The EHealth Program deals with the most sensitive of data—personal health records and income statements. As such, we need to have a robust awareness of best practices and emerging innovations regarding privacy preservation and disclosure avoidance.\nProject-level: The physician-business linkage project offers insights that are often most helpful at the local levels at which healthcare markets operate. But traditional disclosure practices often limit releases at the state level (with explicit limitations for publishing statistics at geographies representing fewer than ~half a million residents). Research on privacy models may offer creative solutions for addressing this limitation.\nProduct-level: Our linkage project can support a wide range of data products and focus areas, but a limiting “privacy budget” will require us to prioritize what we release by balancing stakeholder demand vs. technical limitations. Research on privacy models can help surface technical limitations and opportunities."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#privacy-models-project-specific-challenges",
    "href": "Privacy Models for EHealth - Intro and Scope.html#privacy-models-project-specific-challenges",
    "title": "Privacy Models for E-Health",
    "section": "Privacy Models: Project-specific Challenges",
    "text": "Privacy Models: Project-specific Challenges\n\nMultiple governance regimes: Our work includes both Title 13 and Title 26 data, as well as SSA-based Numident data and restricted CMS data. Each will require sign-offs from the respective data owners and governance regimes.\nMultiple “privacy units”: Need to protect both individuals (physicians) and businesses simultaneously.\nMuch of the data already publicly available: The public availability of assets like the NPI registry of healthcare providers makes reidentification easier (much of our value-add is in the linking work rather than the individual characteristics).\nHigh dimensional data: We have brought together lots of different fields to describe physician and business characteristics. Releasing data on many different characteristics makes it challenging to prevent re-identification.\nUnable to fully anticipate all high-impact uses today: Shift towards formal privacy budgets benefits enormously from a clear roadmap and fully planned out data use cases vs. taking an incremental approach, as any early releases will draw down a limited privacy budget.\nUndetermined frequency of data collection/publication: Not simply a question of protecting our 2018 data, but also protecting physicians and businesses across time as we add more years of data."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#summary",
    "href": "Privacy Models for EHealth - Intro and Scope.html#summary",
    "title": "Privacy Models for E-Health",
    "section": "Summary",
    "text": "Summary\nOn August 3, 2022, the EHealth Team convened a knowledge sharing session to review and discuss introductory videos and readings on the topic of differential privacy, including:\n\nLay Audience: Minute Physics’ Protecting Privacy with Math (video: 10m)\nNon-Technical Overview: MIT Press’ Designing Access with Differential Privacy (webinar video: 25m) (alternative: book chapter)\nBusiness Challenges: Abowd’s Issues Encountered Deploying Differential Privacy (article: 4p)\nTechnical Implementation: Chetty and Friedman’s A Practical Method to Reduce Privacy Loss when Disclosing Statistics based on Small Samples (article: 7p)\n\n\n\n\nreadings.png\n\n\nPlease see the Literature Review section (link in toolbar) for additional information."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#implications-for-ehealth-program",
    "href": "Privacy Models for EHealth - Intro and Scope.html#implications-for-ehealth-program",
    "title": "Privacy Models for E-Health",
    "section": "Implications for EHealth Program",
    "text": "Implications for EHealth Program\n\nDifferential privacy is a framework, not a mathematical formula that can be universally applied. In fact, there generally is no universally optimal method for non-count queries. The specifics of implementation need to depend on the data and on user requirements.\nDifferentially private methods now exist for most basic statistical functions (from counts and CDF to regression and machine learning models), but efficiency can vary dramatically, and certain advanced research techniques simply do not (currently) offer support for DP.\nLots of opportunities to be clever—“What you want is not always what you want to add noise to”: sometimes, it makes sense to aggregate data in a counterintuitive way and then running calculations on the noisy results to obtain a metric of interest in order to optimize the privacy-accuracy tradeoff (ex sum of noisy ages over sum of noisy population is more efficient than adding noise directly to the mean of ages).\nIn short, the complexities of differential privacy cannot be fully abstracted or automated away. Instead, staff will need to be educated, work from agreed upon guidelines, and apply their own creativity to make full use of differential privacy."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#open-questions-and-constraints",
    "href": "Privacy Models for EHealth - Intro and Scope.html#open-questions-and-constraints",
    "title": "Privacy Models for E-Health",
    "section": "Open Questions and Constraints",
    "text": "Open Questions and Constraints\nWhile the literature review was enlightnening, the team continued to navigate a number of open questions, constraints, and unsettled debates, including:\n\n\nWhat is an appropriate epsilon (i.e. metric describing the risk of privacy loss, balanced against the trade-off of accuracy)?\nHow can we best manage multiple (i.e. different kinds of) data releases against a single privacy budget?\nHow do we employ a framework that allows for cyclical releases over time (i.e. not just protecting physicians in 2018 but across time)?\nWhat types of analysis can be supported by synthetic data (i.e. what features of the data are and are not preserved)?\nHow can we best measure the accuracy of any analysis conducted using synthetic data?\nHow do we best implement a dual guarantee for businesses and individuals?\nHow do we report on business measures that amplify and rely heavily on a small number of (large) firms (ex: measures of market concentration for markets dominated by 2-3 firms)?"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#environment-for-experimentation",
    "href": "Privacy Models for EHealth - Intro and Scope.html#environment-for-experimentation",
    "title": "Privacy Models for E-Health",
    "section": "Environment for Experimentation",
    "text": "Environment for Experimentation\nTo avoid the complexities and limitations of working with protected data, the project instead turned to large public data, primarily from CMS Medicare, to construct a proxy of the protected dataset. The specific datasets used were as follows:\n\nProxy for physician and establishment-level data: CMS Care Compare National Downloadable File of December 2018.\nProxy for physician numerical data: CMS Medicare Program Use and Payments Data - by Provider dataset (MPUPD) of 2018.\nProxy for firm and EIN-level data: AHRQ 2018 Compendium of US Health Systems, including the systems and group practice linkage file.\n\nSupport files used include (a) Census mappings of zip codes to counties and counties to core based statistical areas and (b) a mapping of PECOS specialties to more rolled up specialty groups based on the method used by the CMS MD-PPAS dataset.\n\n\n\nproxydata.png\n\n\nLinking this data together provided a sufficiently rich, large, and parallel dataset that captures many of the same kinds of relationships and characteristics as our protected linked data. This data covered (rounded to 2-significant digits):\n\n560,000 unique physicians\n160,000 unique addresses – proxy for establishments, incl. self-employment\n39,000 unique named practices – proxy for EINs\n640 systems – proxy for (large) firms"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#scope",
    "href": "Privacy Models for EHealth - Intro and Scope.html#scope",
    "title": "Privacy Models for E-Health",
    "section": "Scope",
    "text": "Scope\nThe goal of the second deliverable is to learn by doing by conducting pilot implementations of competing privacy models on the public proxy data, with a focus on methods utilizing differential privacy. Where feasible, this work will ideally seed generalizable methods for implementing privacy models within the scope of the physician-business linkage project. This work will be guided by the following guidelines:\n\nKeep it simple and flexible: Most privacy models can be enhanced through extensive optimization and by incorporating more sophisticated techniques. However, this work by definition needs to be tailored tothe particular data, question, and statistical functions or models used. Since we do not yet have a final design for specific data products – and since the EHealth Team lacks mathematical statisticians – this initial research will instead focus on the straightforward and mostly generalizable techniques with the fewest moving pieces.\nFocus on basic descriptive statistics: While our data may well provide a basis for models (from basic regressions to complex machine learning techniques), we do not currently have the specifics to define clear use cases using such methods. Since differential privacy and similar approaches often require specific implementations for specific types of modeling, this project will instead focus primarily on basic descriptive statistics like counts, ratios, and distributions, in part to comply with guideline 1 of keeping things simple.\nLeverage pragmatic solutions and existing capabilities: Given the small size of our team, the EHealth Program is best served by solutions that are easy to implement (and maintain) on existing, standard-issue infrastructure. Accordingly, we seek to adopt the following preferences:\n\nA bias for existing/emerging solutions already offered, maintained, and/or hosted by the Census Bureau – or, failing that, reputable and well maintained open source solutions;\nA bias for techniques that have a track record of success within the Bureau, including past disclosure avoidance review approvals;\nAn openness to techniques that may lack a formal privacy guarantee provided they offer credible techniques for privacy preservation that will likely be endorsed by the disclosure avoidance review process.\n\n\nThe above guidelines have led, for instance, to a preference for the CenSyn tool – an emerging, general-purpose tool developed, supported, and endorsed by other parts of the Bureau (incl. as a possible tool for the ACS) – should this prove roughly competitive with synthetic data techniques that offer a more formal privacy guarantee."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#project-pipelines",
    "href": "Privacy Models for EHealth - Intro and Scope.html#project-pipelines",
    "title": "Privacy Models for E-Health",
    "section": "Project Pipelines",
    "text": "Project Pipelines\nThe pilot implementations will primarily be structured around exploring options for differentiated pipelines, specifically:\n\nPipeline 0: True actuals (no processing and no protection) (no actions needed)\nPipeline 1: Traditional disclosure rules / rounding and suppression (out of scope: to be implemented by EHealth staff rather than CDF fellow)\nPipeline 2: Differentially private aggregate tables\nPipeline 3: Synthetic data: CenSyn (nonDP) and DP alternative\nPipeline X: Differentially private model-based estimates (regression, machine learning, etc.) (out of scope: cursory summary of tools and approaches only, due to diffusion of options depending on model-approach used; example tools incl. OpenDP / SmartNoise / PyDP / r-Diffpriv)\n\n\n\n\nmodels.png"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#pipeline-knowledge-capture",
    "href": "Privacy Models for EHealth - Intro and Scope.html#pipeline-knowledge-capture",
    "title": "Privacy Models for E-Health",
    "section": "Pipeline Knowledge Capture",
    "text": "Pipeline Knowledge Capture\nFor each pipeline, we seek to learn the following:\n\nGeneral approach: High-level implementation steps/sequence for processing the data and the methods used.\nScope of application: Conditions under which this pipeline makes a good fit and should be considered.\nPros / Opportunities: Strengths of this approach, including unique opportunities.\nCons / Limitations: Weaknesses and limitations of this approach.\nLevel of Effort: Short discussion of the difficulty and complexity of utilizing this pipeline.\nTools: Tools that need to or can be used to assist with the implementation of this pipeline.\nDecision Points: 1-4 key decisions that need to be made in either selecting or calibrating this method and the consequences that result."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#research-questions-to-guide-pilots",
    "href": "Privacy Models for EHealth - Intro and Scope.html#research-questions-to-guide-pilots",
    "title": "Privacy Models for E-Health",
    "section": "Research Questions to Guide Pilots",
    "text": "Research Questions to Guide Pilots\nIn order to provide specific use cases, the project will compile data around the following questions. The resulting data will then be passed through the privacy models being piloted:\n\nRatio of physicians (by specialty) to population\nDistribution of physicians by organization type\nCommunity representativeness - Comparison of physician vs. population demographics (age/gender) (as stand-in for race and other characteristics)\nFirm market concentration by geography (as measured in terms of employed physicians)\nCommon business ownership/integration across geographies (ex: state pairings)\nSecondary sources of work for physicians (i.e. business characteristics of secondary employers)\nDeterminants/correlates of Medicare charges (as stand-in for income)\nCare coordination - Number and specialty diversity of colleagues by organization type\n\nThe reporting for the questions above is typically broken down by state and by core based statistical area (with a focus on larger metropolitan statistical areas).\nNote: while these pilots uses real (public) data, this data is only really intended to serve as a proxy for the protected data. These research questions are therefore intended to be fairly nominal, and the project is not directly focused on providing the most elegant or correct answer to these questions. Rather, they exist to generate a variety of disparate tables against which to apply and evaluate privacy models."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#decision-tree-selecting-the-right-pipeline-to-use",
    "href": "Privacy Models for EHealth - Intro and Scope.html#decision-tree-selecting-the-right-pipeline-to-use",
    "title": "Privacy Models for E-Health",
    "section": "Decision Tree: Selecting the Right Pipeline to Use",
    "text": "Decision Tree: Selecting the Right Pipeline to Use\nThe graphic below depicts a series of decisions that users can use to determine which pipeline best suits their project needs:\n\n\n\ndecision_tree.png"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#pipeline-2-dp-tables",
    "href": "Privacy Models for EHealth - Intro and Scope.html#pipeline-2-dp-tables",
    "title": "Privacy Models for E-Health",
    "section": "Pipeline 2 (DP Tables)",
    "text": "Pipeline 2 (DP Tables)\n\nGeneral approach: Run traditional analyses, then inject noise\nScope of application: This approach is advantageous when a specific question or set of questions is known about in advance. It could also be useful if a centralized approach to DP is chosen, where partners send queries and we send back noisy results.\nPros / Opportunities: The main advantage is a more precise application of the privacy budget. We have control over how and how often the data is accessed, so we can allocate the privacy budget efficiently.\nCons / Limitations: Lack of flexibility. Researchers or partners with questions we hadn’t considered won’t be able to get answers from the data if there isn’t any more room in the privacy budget.\nLevel of Effort: The biggest challenge will be deciding on how the privacy budget is allocated. Since budget is allocated per query, we need to prioritize the queries we run.\nTools: The E-Health DP Guide has guidance implementations. Tools like OpenDP or PyDP could also be useful.\nDecision Points: 1) Which queries are most important? 2) How many questions can be reasonably answered with the given privacy budget?"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#pipeline-3-synthetic-data",
    "href": "Privacy Models for EHealth - Intro and Scope.html#pipeline-3-synthetic-data",
    "title": "Privacy Models for E-Health",
    "section": "Pipeline 3 (Synthetic Data)",
    "text": "Pipeline 3 (Synthetic Data)\n\nGeneral approach: Build a synthetic dataset based on the original dataset. Release the synthetic dataset for partners/researchers to run queries on.\nScope of application: We want to maintain as much flexibility as possible in terms of running queries. This approach could also be structured in a way that partners get access to the synthetic data to run tests on, and then they send their finalized queries back to us to run on the original data.\nPros / Opportunities: We’d be giving partners much greater latitude to poke and prod the dataset. They would have the freedom to explore the data in a way that precomputed tables doesn’t allow.\nCons / Limitations: Synthetic datasets only support certain kinds of analyses (counting queries). They also sacrifice a bit of accuracy when comparing single queries from Pipeline 2.\nLevel of Effort: There needs to be clear communication with partners/researchers about the limitations of the data. Synthetic datasets are a very powerful tool, but they’re not a silver bullet.\nTools: MST, CenSyn (also part of E-Health DP Guide)\nDecision Points: 1) What’s the ratio of columns to rows? A dataset with too few rows per cell could be problematic when building a synthetic dataset. 2) Are there really important, single questions that you want from the dataset? If not, synthetic data might be your best bet."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#pipeline-x-dp-complex-models",
    "href": "Privacy Models for EHealth - Intro and Scope.html#pipeline-x-dp-complex-models",
    "title": "Privacy Models for E-Health",
    "section": "Pipeline X (DP Complex Models)",
    "text": "Pipeline X (DP Complex Models)\n\nGeneral approach: Applying concepts of DP to more advanced analyses (regressions, neural nets, clustering, etc.)\nScope of application: Partners/researchers with more complex questions they want to address with the data.\nPros / Opportunities: The opportunity to do a deeper dive into the data and find connections or information that isn’t readily available with counting queries.\nCons / Limitations: More effort. There will likely need to be a closer partnership with the stakeholder, and new tools will need to be explored.\nLevel of Effort: High, but not prohibitive. There are off-the-shelf tools that can do many/most of the analysis a partner might be looking for. It’ll just be a matter of finding and getting a grasp of them.\nTools: List from NIST: https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools\nDecision Points: 1) Can the question be answered with a counting query? If not, you’ll need Pipeline X."
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#areas-for-further-continued-study",
    "href": "Privacy Models for EHealth - Intro and Scope.html#areas-for-further-continued-study",
    "title": "Privacy Models for E-Health",
    "section": "Areas for Further / Continued Study",
    "text": "Areas for Further / Continued Study\n\nPre-built privacy tools\nIncremental data release\nTuning CenSyn\nCentralized query management (partners send us queries to run on the data)\nProtecting multiple entities (physicians and businesses)\nOptimization (ex: Select-Measure-Reconstruct framework)\nAlternative models (see Pufferfish framework)"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#lessons-learned",
    "href": "Privacy Models for EHealth - Intro and Scope.html#lessons-learned",
    "title": "Privacy Models for E-Health",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nNo one method will be able to do everything.\nThere’s an inherent sacrifice to accuracy that needs to be made to retain privacy.\nThere isn’t a single right answer when choosing epsilon. The “best” epsilon is all about balancing priorities.\n\nThere’s a lot of art - as opposed to science - in DP\n\nYou can do a lot with “simple” analyses. Most projects that will want to use the data will only need to able able to answer counting queries.\nNo matter how many times you test it, fully trusting the accuracy of a synthetic dataset is difficult and terrifying.\n\nThat said, synthetic datasets are really cool"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#recommendations",
    "href": "Privacy Models for EHealth - Intro and Scope.html#recommendations",
    "title": "Privacy Models for E-Health",
    "section": "Recommendations",
    "text": "Recommendations\n\nMaintain a smaller privacy budget than Google and Microsoft\nI might be blinded by the shiny newness of the synthetic data technologies, but go with synthetic data unless there’s a very compelling case to release specific analyses\nMaintain open dialogue with the CenSyn team\n\n\n\n\nbrainstorm.png"
  },
  {
    "objectID": "Privacy Models for EHealth - Intro and Scope.html#next-steps",
    "href": "Privacy Models for EHealth - Intro and Scope.html#next-steps",
    "title": "Privacy Models for E-Health",
    "section": "Next Steps",
    "text": "Next Steps\nThis technical review of the opportunities and limitations presented by modern privacy preservation techniques offers the team with new insights to guide the selection and development of data products. These considerations will be weighed against the demand and interest demonstrated by stakeholders for specific data products and focus areas through a series of expert stakeholder consultations. This combined assessment of demand opportunities vs. supply limitations will guide the final priorities for the data products to be released through the physician-business linkage project.\nNext, the team will design data products that best balance demand and technical limitations. Once these products have been fully scoped, the team will determine which of the identified privacy models pipelines best suits product design and implement the needed infrastructure.\nIn addition, given the sensitivity of the data we work with, the EHealth Team will continue to build knowledge and capacity on privacy model implementations, including for potential application within projects working with sensitive patient records."
  },
  {
    "objectID": "Literature Review.html",
    "href": "Literature Review.html",
    "title": "Privacy Models for E-Health",
    "section": "",
    "text": "On August 3, 2022, the EHealth Team convened a knowledge sharing session to review and discuss introductory videos and readings on the topic of differential privacy, including:\n\nLay Audience: Minute Physics’ Protecting Privacy with Math (video: 10m)\n\nThis short but excellent video produced in partnership with the US Census Bureau provides a high-level overview of the promise of Differential Privacy, intended for a general audience.\n\nNon-Technical Overview: MIT Press’ Designing Access with Differential Privacy (webinar video: 25m) (alternative: book chapter)\n\nIn this video, Alexandra Wood of Harvard University and the Berkman Center presents her chapter from the new Handbook on Using Administrative Data for Research and Evidence-Based Policy from MIT Press. She explains how administrative data containing personal information can be collected, analyzed, and published in a way that ensures the individuals in the data will be afforded the strong protections of differential privacy.\nFor those interested, the Handbook is a fantastic resource that is well worth a bookmark. Each chapter is available as a webpage, PDF, and short video webinar. If you prefer reading over viewing videos, you can browse her chapter for free. The appendix includes useful additional content, including links to tools and guidelines for operationalizing differential privacy.\n\nBusiness Challenges: Abowd’s Issues Encountered Deploying Differential Privacy (article: 4p)\n\nIn this short article, Abowd and team discuss the unanticipated challenges encountered in implementing differential privacy in practice within a statistics agency. This included challenges with: obtaining qualified personnel and computing infrastructure, accounting for and balancing a diverse range of users and needs, generating micro data to meet established expectations, and determining a proper value for the privacy-loss parameter. For those interested in more, Michael Hawes describes 7 lessons learned in Harvard Data Science Review.\n\nTechnical Implementation: Chetty and Friedman’s A Practical Method to Reduce Privacy Loss when Disclosing Statistics based on Small Samples (article: 7p)\n\nIn this paper, Raj Chetty (Harvard Univ.) and John Friedman (Brown Univ.) discuss their implementation of a differential privacy-inspired mechanism for releasing protected statistics at small geographic levels for the groundbreaking and influential Opportunity Atlas that uses Census data to identify the social mobility of all neighborhoods in the United States. Not only did this mechanism make it possible to release such granular statistics, it also reduced bias relative to traditional disclosure methods.\nFor those interested, a longer version of the paper with more detail on their Opportunity Atlas use case and implementation is available from NBER.\n\n\n\n\n\nreadings.png\n\n\nPlease see the summary sections below for the main ideas discussed and taken from these readings.\n\n\n\nThe readings below proved helpful for this project.\n\n\n\nUS Census: Differential Privacy 101 Webinar. (first 20min). https://youtu.be/Hsh1R1WA0kU\nWood et al. (2020). Differential Privacy: A Primer for Non-Technical Audiences. https://scholarship.law.vanderbilt.edu/jetlaw/vol21/iss1/4/\nMcKay Bowen, C. (Aug 2021). Personal Privacy and the Public Good. Urban Institute Research Report. https://www.urban.org/sites/default/files/publication/104694/privacy-and-the-public-good_0_0.pdf\nDrechsler, J. (Feb 2021). Differential privacy for government agencies: Are we there yet? https://privacytools.seas.harvard.edu/files/privacytools/files/are_we_there_yet_-_2102.08847.pdf\nScience (Jan 2019). Can a set of equations keep US census data private? (magazine article). https://www.science.org/content/article/can-set-equations-keep-us-census-data-private\nNew York Times (April 2022). The 2020 Census suggests that people live underwater. There’s a reason. (newspaper article). https://www.nytimes.com/2022/04/21/us/census-data-privacy-concerns.html\n\n\n\n\n\nNBER (2020). Summer Institute Methods Lectures: Differential Privacy for Economists. https://www.nber.org/lecture/summer-institute-2020-methods-lectures-differential-privacy-economists\nDesfontaines, D. of Google (2020). Lowering the cost of anonymization (thesis and blog). http://desfontain.es/thesis/index.html\nFoote et al. (Apr 2019). Releasing Earnings Distributions using Differential Privacy: Disclosure Avoidance System for Post-Secondary Employment Outcomes (PSEO). https://www2.census.gov/ces/wp/2019/CES-WP-19-13.pdf\nUS Census (2021). 2020 Decennial Census: Disclosure Avoidance Modernization. (website). https://www.census.gov/programs-surveys/decennial-census/decade/2020/planning-management/process/disclosure-avoidance.html\n\nGithub repo with implementation information: https://github.com/uscensusbureau/DAS_2020_Redistricting_Production_Code/wiki/Background\n\nUS Census (May 2021). Differential Privacy 201 and the TopDown Algorithm Webinar. https://youtu.be/bRIoE0rqwAw\n\n\n\n\n\nAlgorithm that won NIST’s synthetic data competition: https://arxiv.org/pdf/2108.04978.pdf\n\nMechanism behind that algorithm: https://arxiv.org/pdf/1901.09136.pdf\n\nMWEM, a popular approach for synthetic data: https://www.cs.huji.ac.il/~katrina//papers/mwem-nips.pdf\nChoosing epsilon: https://par.nsf.gov/servlets/purl/10217360\nImpact of DP on redistricting: https://www.science.org/doi/10.1126/sciadv.abk3283\nContinuous Data Release: https://arxiv.org/pdf/1711.11436.pdf\nHDMM, an algorithm used in the 2020 Census: http://www.vldb.org/pvldb/vol11/p1206-mckenna.pdf\nAn extension of k-anonymity: l-diversity: https://personal.utdallas.edu/~mxk055100/courses/privacy08f_files/ldiversity.pdf"
  },
  {
    "objectID": "Literature Review.html#summary-1a-main-concepts---disclosure-avoidance",
    "href": "Literature Review.html#summary-1a-main-concepts---disclosure-avoidance",
    "title": "Privacy Models for E-Health",
    "section": "Summary 1a: Main Concepts - Disclosure Avoidance",
    "text": "Summary 1a: Main Concepts - Disclosure Avoidance\n\nBackground: Disclosure Avoidance\n\nThe purpose of employing disclosure avoidance methods is to prevent, or at least make it more difficult, to reconstruct the underlying data and re-identify the people or businesses described by the released data products.\nThis is done by:\n\nReducing precision (ex: rounding to 2 significant digits)\nRemoving vulnerable records (ex: only 2 families with 8+ members in a tract)\nAdding uncertainty (ex: noise injection)\n\n\n\n\nBackground: Traditional Techniques\nTraditional disclosure avoidance techniques often have the benefit of being:\n\nSimple to understand\nUniversally applicable\n\nBut less appealing features include that they…\n\nCan be blunt instruments;\nTraditionally lack the ability to clearly estimate the privacy risk / level of protection offered beyond basic assumptions and rules of thumb;\nOften require secrecy of the precise methods/calibration used and its consequences which can impact research by preventing researchers from counteracting or managing the unknown biases and uncertainty introduced.\n\nExamples of traditional techniques include:\n\nRounding\nSuppression rules\nCoarsening and top/bottom coding\nRecord swapping\nSampling\nNoise infusion\nSynthetic Data and multiple imputation\n\n\n\nBackground: Data Disclosure Risks\n\nIdentity: Linking a particular record to an individual\nAttribute: The data reveals an attribute of an individual\nInferential: The data reveals an attribute with high certainty\n\n(Note: Non-sensitive attributes can be used to identify records, too)\n\n\n\nlr_table1.png\n\n\nTraditional techniques include:\n\nDe-identification: remove direct identifiers.\n\n\n\n\nlr_table2.png\n\n\n\nSuppression: remove values that hava high risk for disclosure.\n\n\n\n\nlr_table3.png\n\n\n\nCoursening: grouping values into categories or top/bottom coding.\n\n\n\n\nlr_table4.png"
  },
  {
    "objectID": "Literature Review.html#summary-1b-main-concepts---differential-privacy",
    "href": "Literature Review.html#summary-1b-main-concepts---differential-privacy",
    "title": "Privacy Models for E-Health",
    "section": "Summary 1b: Main Concepts - Differential Privacy",
    "text": "Summary 1b: Main Concepts - Differential Privacy\n\nDifferential Privacy: What is it?\n\nOver the past two decades, we have come to see that traditional de-anonymization techniques often fail to protect the privacy of individuals in sensitive datasets.\nTo address this problem, computer scientists introduced differential privacy, a strong statistical notion of privacy that bounds the amount of information a statistical release leaks about any individual.\nDifferential Privacy is a formal mathematical framework for quantifying and managing privacy risks. Its key feature is that it provides a quantifiable guarantee of privacy.\nNote that this is not a substitute for disclosure avoidance algorithms and techniques (like noise infusion, data swapping, etc.); instead, it offers a method for measuring and thereby controlling risk.\n\n\n\nDifferential Privacy: Real-world use\n\nUsed widely by tech companies, including Google, Apple, Facebook, and Microsoft to protect the data they collect from users – everything from emoji usage to self-reported personal health metrics. (Typically using the “local” model where the device applies differential privacy before collection by the tech company).\nMore recently, considered an emerging option for releasing statistical products and microdata using the “curator” model where a trusted party like the US Census collects data and then applies differential privacy in a secure environment before releasing the differentially private dataset or statistics.\nFirst use at Census was the LODES On The Map application in 2009. Most prominent use is the 2020 Census. Experimental products also often use DP, including the post secondary education outcomes products.\nMIT Technology Review named DP as one of the top ten technologies expected to have “widespread consequences for human life.”\n\n\n\nDifferential Privacy: How it works\n\nAn analysis satisfied differential privacy if the information it yields about someone doesn’t depend on whether that person is in the database. Put another way, “even if the participant removed her data from the dataset, no outputs … would become significantly more or less likely.”\n\n\n\n\ndp_flow.png\n\n\n\nThis definition has the advantage that formalization yields a metric summarizing an algorithm’s level of “privacy” in a single number (ϵ). Data owners can finely “tune” this to meet precise confidentiality (or accuracy) needs\n\nWhat is the maximum privacy loss that is acceptable and what is the minimum utility required? These considerations can be tricky to balance since they are not in the same unit—and there is not yet agreement on what makes for a “proper” epsilon.\n\n\n\n\nPrivacy-Accuracy Tradeoff\nChoosing an appropriate epsilon is about finding a balance between accuracy and privacy. Probably the most common method of finding that balance is running a repeated calculation with varying epsilon values and measuring the resulting error. The following graph is one such example. In it, epsilon is incremented by 0.1 between a starting value of 0.1 and a final value of 2, and each corresponding error value is plotted.\n\n\n\ntradeoff.png\n\n\nThe “optimal” epsilon can then be chosen in several ways. One would be to choose epsilon based on a maximum allowable error, either in absolute or percentage terms. In this method, you would choose the smallest epsilon that still falls below the maximum allowed error. Another method would be by choosing an epsilon at the “elbow” of the epsilon/error graph. The idea behind this method is to choose an epsilon just before you start seeing the improvement in marginal accuracy start to level off.\n\n\nOther Key Concepts Governing DP\nComposition:\n\nDP’s rule of composition means that the ε applied by different applications can simply be summed to obtain the collective risk.\n\nPrivacy Budget:\n\nDP moves away from the binary notion of “is the individual’s private information exposed or not” to a matter of accumulative risk.\nPrivacy loss that accumulates over multiple computations must therefore be “calculated, tracked, and limited” by an established “privacy budget.”\n\nJust as with a household budget, it is “easy to waste privacy loss budget without ‘financial’ planning.” One common practice is to preserve some privacy budget for unanticipated and future uses.\n\nSensitivity:\n\nAccuracy is a function of ϵ and the sensitivity of the data (i.e. how much impact a single record can have when added or removed).\nExample: Adding a billionaire to a Census tract has a sensitivity of +/- 1 in terms of population count, but will have a much more dramatic impact on mean income.\n\nTransparency:\n\nDP allows for full transparency of methods, code, and calibration (just need to protect randomization). This allows researchers to potentially counteract resulting biases and better manage the uncertainty introduced into the data.\nMoving away from “security through obscurity.”\n\n\n\nTwo approaches to DP: Noisy statistics vs. Synthetic data\nThere are two main flavors of differential privacy. The first is the application of random noise to pre-calculated statistics. In this version of differential privacy, an analysis is run on the original dataset and then some form of 0-centered noise (Laplace, Gaussian, etc.) is added to create a differentially private analysis.\nThe second form of differential privacy is synthetic data generation. In this form, you create a model of the original dataset, use that model to create a new dataset, and then run your analysis on the new dataset. If your new dataset was generated correctly, it won’t include records from the original dataset, but an analysis run on both datasets should give you the same answer, with some margin of error.\n\n\n\nmodels.png"
  },
  {
    "objectID": "Literature Review.html#summary-2-technical-and-business-challenges-in-census-2020-implementation",
    "href": "Literature Review.html#summary-2-technical-and-business-challenges-in-census-2020-implementation",
    "title": "Privacy Models for E-Health",
    "section": "Summary 2: Technical and Business Challenges in Census 2020 Implementation",
    "text": "Summary 2: Technical and Business Challenges in Census 2020 Implementation\n\nScientific Issues Encountered Deploying Differential Privacy\n\nInvariants: characteristics that cannot be changed or for which no noise can be introduced (ex: maintaining the actual state-level population counts for Census 2020)\nMetrics: quantifying the tradeoff between privacy and accuracy\nEquity: how is the privacy budget allocated?\nChoosing Epsilon: policy decision based on societal values\n\nMaintaining flexibility for future data\n\nIterative data releases and correlative statistics\n\n\n\nOperational Issues Encountered Deploying Differential Privacy\n\nTools and personnel\nChoosing low-sensitivity queries: Children per household vs Total children\nStructural vs Sampling Zero: is it 0 because it can’t exist or because it doesn’t exist in this context?\nComputational Power\nFinal Specifications: how is the data going to be used?\nUser Expectations: requirements for internal consistency and non-negative counts (correcting for this can introduce more error than DP itself)\n\n\n\n“Devil’s Advocate”\n\nA recent critique in Communications of the ACM argues that “fundamental misunderstandings and blatantly flawed implementations pervade the application of DP”.\nCritics argue that the ε <= 1 recommended by the founders of DP as necessary to “obtain a meaningful privacy guarantee” often leads to “very poor” analytical utility—and, as a result, most real-world implementations use an “unreasonably large ε.” (Apple iOS=14, Google=9, Census person file=17.14). Such high values “are pointless in terms of privacy”, as the privacy guarantee “completely fades away.”\nAlso problematic: too often, data collectors and providers do not release the value of ε. Others employ sophisticated relaxations of DP to allow for a nominally lower value but without necessarily a true reduction in risk.\nBig limitation: “it is impossible to collect DP-protected data from a community of respondents an indefinite number of times with a meaningful privacy guarantee” (ex: Apple only protects over a single day)."
  },
  {
    "objectID": "Literature Review.html#summary-3-implementation---opportunity-atlas-example",
    "href": "Literature Review.html#summary-3-implementation---opportunity-atlas-example",
    "title": "Privacy Models for E-Health",
    "section": "Summary 3: Implementation - Opportunity Atlas Example",
    "text": "Summary 3: Implementation - Opportunity Atlas Example\n\nSummary: A Practical Method to Reduce Privacy Loss When Disclosing Statistics Based on Small Samples\nAdvantages:\n\nUnusually rich detail on individual outcomes at a very high geographic resolution.\nAlmost certainly would not be publishable under conventional disclosure rules—and actually improves over cell suppression and other traditional techniques in terms of preserving accuracy.\n\nRequired a few pragmatic compromises:\n\nAdopted slight relaxations that technically break differential privacy, but which abide by traditional assumptions of disclosure avoidance. (Note: newer methods allow for this while continuing to offer formal privacy guarantee).\nAllowing a relatively high value of ε to ensure the data meets public policy needs: i.e. that housing authorities could use the data to identify tracts in the top and bottom tail of the distribution (“moving to opportunity”).\n\n\n\n\nopportunityatlas.png\n\n\n\n\nKey Concepts from Paper and Method\nMarginals:\n\nDistinct sets of attributes\n\nCells:\n\nStratifying attributes (Ac) and metric attributes (Am) \nEach unique combination of stratifying attributes is called a cell \n\n\n\n\nchetty_sample.png\n\n\nQuery sensitivity:\n\nThe maximum difference between outcome of a marginal query when run on D and D’ (databases differing by one record)\nMore sensitivity calls for more noise\n\nLocal/global sensitivity:\n\nGlobal – sensitivity across the entire dataset\nLocal – sensitivity within a cell\n\nMaximum observed sensitivity:\n\nDisclosing sensitivity within each cell releases information\nUse the largest local sensitivity for every cell’s noise infusion"
  },
  {
    "objectID": "E-Health DP Guide.html",
    "href": "E-Health DP Guide.html",
    "title": "Privacy Models for E-Health",
    "section": "",
    "text": "*ish\n\n\nBefore starting on any of the disclosure techniques, create a dataset that can be used as an example\n\n\nCode\n# load libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_dir = \"\"\n\n# read files\nphysicians = pd.read_csv(data_dir + \"physicians.csv\")\nestablishments = pd.read_csv(data_dir + \"establishments.csv\", dtype={'zip': str, 'zip9': str, 'zip3': str})\npractices = pd.read_csv(data_dir + \"practices.csv\")\nfirms = pd.read_csv(data_dir + \"firms.csv\")\nphys2estab = pd.read_csv(data_dir + \"rel_phys2estab.csv\")\n\n\n\n\nOnce the physicians are linked to establishments, practices, and firms, a few fields need to be cleaned to make processing easier down the line.\n\nAges are top-coded to 85\n\nThis is for consistency across datasets\n\nNull ages are set to a default 0 value\nMedicare charges are put into bins\n\nThis is for convenience when we eventually start creating synthetic datasets. We treat all fields as categorical when making synthetic data, and this prevents the number of categories from going too high\nBinning can be removed if we start treating numeric values as numeric during synthetic data generation\nAlso see the PSEO paper (link in the differential privacy section of this guide) for notes on binning incomes without sacrificing privacy and revealing information about the dataset\n\nReplace nulls with a default “nan” value\n\n\n\nCode\n# link physicians to establishments\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# add practices and firms\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\nphys_firm = phys_prac.merge(firms, on=\"health_sys_id\", how=\"left\")\n\n# define the columns we want to keep\ncolumns = [\"NPI\", \"st\", \"gndr\", \"age_imputed\", \"tin_name\", \"practice_type_random\", \"health_sys_state\", \"pri_spec_category\", \"practice_scope\", \"practice_size\", \"medicare_charges\"]\n\n# remove all other columns\ndataset = phys_firm[columns].drop_duplicates()\n\n# clean a few values\ndataset[\"age_imputed\"] = dataset[\"age_imputed\"].fillna(0)\ndataset[\"age_imputed\"] = np.where(dataset[\"age_imputed\"] > 85, 85, dataset[\"age_imputed\"].astype(int))\n\n# bin the medicare charges\nlabels = [\"{0} - {1}\".format(i, i + 999999) for i in range(0, 55000000, 1000000)]\ndataset[\"medicare_charges\"] = dataset[\"medicare_charges\"].astype(int)\ndataset[\"medicare_charges\"] = pd.cut(dataset.medicare_charges, bins=np.arange(0, 56000000, 1000000), right=False, labels=labels)\n\n# get rid of nulls in non-numeric columns\nstring_cols = dataset.select_dtypes(include=[np.object]).columns\nfor col in string_cols:\n    dataset[col] = dataset[col].fillna(\"nan\")\n\n\n\n\n\nA more comprehensive way to organize incomes would be by binning with a lognormal distribution. The main reason you would want to do this is to prevent information leakage. The method I used above (finding the min and max of the dataset and then making evenly spaced bins) reveals information about the dataset, and this weakens privacy.\nTo avoid this leakage, we can use external information about income to create our bins. Incomes tend to follow a lognormal distribution, so we can use the median and standard deviation of incomes in the US to create an accurate enough model to bin our data.\nNote within the note: our bins could be even more accurate if we had estimates of physician incomes instead of just average income in the US, but I couldn’t find any public data for that.\n\n\nCode\n# take mean and standard deviation form the PSEO paper\nmean = 11.003\nstdv = 0.753\n\n# sample a lognormal distribution a few times\nlognorm_dist = np.random.lognormal(mean, stdv, size=1000)\n\n# get thresholds for 20 evenly spaced bins from the lognormal distribution\npercentiles = np.arange(0, 100, 5)\nincome_bins = np.percentile(lognorm_dist, percentiles)\n\n# add a top-code value\nincome_bins = np.append(income_bins, 1000000)\nincome_bins\n\n# The code below is how you would incorporate the lognormal bins into the dataset\n\n\"\"\"\n# link physicians to establishments\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# add practices and firms\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\nphys_firm = phys_prac.merge(firms, on=\"health_sys_id\", how=\"left\")\n\n# define the columns we want to keep\ncolumns = [\"NPI\", \"st\", \"gndr\", \"age_imputed\", \"tin_name\", \"practice_type_random\", \"health_sys_state\", \"pri_spec_category\", \"practice_scope\", \"practice_size\", \"medicare_charges\"]\n\n# remove all other columns\ndataset = phys_firm[columns].drop_duplicates()\n\ndataset[\"medicare_charges\"] = dataset[\"medicare_charges\"].astype(int)\ndataset[\"medicare_charges\"] = np.where(dataset[\"medicare_charges\"] > 999999, 999999, dataset[\"medicare_charges\"])\ndataset[\"medicare_charges\"] = pd.cut(dataset.medicare_charges, bins=income_bins, right=False)\n\ndataset\n\"\"\"\n\n\nNote: I’m making a blanket replacement of nulls with “nan” here. Nulls should be handled based on the context of the column for the real data\n\n\nCode\ndataset\n\n\n\n\n\n\n  \n    \n      \n      NPI\n      st\n      gndr\n      age_imputed\n      tin_name\n      practice_type_random\n      health_sys_state\n      pri_spec_category\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      1003000126\n      VA\n      M\n      54\n      Commonwealth Hospitalist Group Llc\n      type-4\n      TN\n      Primary care\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      1\n      1003000134\n      IL\n      M\n      45\n      Northshore University Healthsystem Faculty Pra...\n      type-2\n      IL\n      Hospital based\n      multi-unit, single-cbsa\n      large (20-99)\n      1000000 - 1999999\n    \n    \n      9\n      1003000142\n      OH\n      M\n      49\n      Mercy Health Physicians - North, Llc.\n      type-2\n      OH\n      Hospital based\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      13\n      1003000407\n      PA\n      M\n      45\n      Brookville Hospital\n      type-3\n      PA\n      Primary care\n      single-unit\n      medium (6-19)\n      0 - 999999\n    \n    \n      14\n      1003000423\n      OH\n      F\n      41\n      Lake Obstetrics & Gynecology, Inc.\n      type-5\n      nan\n      OBGYN\n      multi-unit, single-cbsa\n      medium (6-19)\n      0 - 999999\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1203604\n      1992998736\n      FL\n      M\n      41\n      Premier Family & Sports Medicine, Llc\n      type-9\n      nan\n      Primary care\n      single-unit\n      small (2-5)\n      0 - 999999\n    \n    \n      1203605\n      1992999031\n      CA\n      M\n      42\n      Austin J Ma Md A Professional Corporation\n      type-8\n      nan\n      Medical specialty\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      1203606\n      1992999122\n      FL\n      M\n      65\n      nan\n      nan\n      nan\n      Primary care\n      nan\n      nan\n      0 - 999999\n    \n    \n      1203608\n      1992999551\n      CA\n      F\n      44\n      Graybill Medical Group Inc\n      type-9\n      nan\n      Primary care\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      1203609\n      1992999825\n      WA\n      M\n      43\n      Virginia Mason Medical Center\n      type-9\n      WA\n      Surgery specialty\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n  \n\n672614 rows × 11 columns\n\n\n\nThe final dataset has information about physicians, practices, and health systems. Null values are replaced with placeholder values where necessary.\n\n\n\n\nThe Census DRB defined a set of rounding rules for disclosing aggregate statistics. The rules, in the DRB’s words, are “an interim solution for privacy protection.” They are defined as:\nIf N is less than 15, report N<15\nIf N is between 15 and 99, round to the nearest 10\nIf N is between 100-999, round to the nearest 50\nIf N is between 1000-9999, round to the nearest 100\nIf N is between 10000-99999, round to the nearest 500\nIf N is between 100000-999999, round to the nearest 1,000\nIf N is 1000000 or more, round to four significant digits\nThe following code implements the DRB rounding rules and is taken from this Github repo: https://github.com/uscensusbureau/drb_rounder\n\n\nCode\nimport math\n\ndef nearest(n, number):\n    \"\"\"Returns n to the nearest number\"\"\"\n    return math.floor((n / number) + 0.5) * number\n\n\ndef round_counts(n):\n    \"\"\"\n    Implements the DRB rounding rules for counts. Note that we return a string, \n    so that we can report N < 15\n    \"\"\"\n    n = int(n)\n    assert n == math.floor(n)  # make sure it is an integer; shouldn't be needed with above\n    assert n >= 0\n    if      0 <= n <      15: return 15\n    if     15 <= n <=     99: return nearest( n,   10)\n    if    100 <= n <=    999: return nearest( n,   50)\n    if   1000 <= n <=   9999: return nearest( n,  100)\n    if  10000 <= n <=  99999: return nearest( n,  500)\n    if 100000 <= n <= 999999: return nearest( n, 1000)\n    return round4_decimal(n)\n\n\ndef round4_float(f):\n    \"\"\"\n    Implements the IEEE floating point rounding rules with the \"round half even\"\n    option when rounding. This means that 1000.5 rounds to 1000 while 1001.5 \n    rounds to 1002.\n    \"\"\"\n    from decimal import ROUND_HALF_EVEN, Context\n    prec4_rounder = Context(prec=4, rounding=ROUND_HALF_EVEN)\n    return float(str(prec4_rounder.create_decimal(f)))\n\n\ndef round4_decimal(d):\n    \"\"\"Similarly rounds half even with precision of 4 but for decimals\"\"\"\n    return int(round4_float(d))\n\n\nThe DRB’s code is then used in the following function that can be applied to a dataset.\nThe function takes the raw data as an input, applies whatever grouping you specify, calculates counts, and then applies rounding rules.\n\n\nCode\ndef drb_round(data, grouping_columns, target_column):\n    \"\"\"Apply rounding rules to user-defined grouping\"\"\"\n    # copy input df so we can change it\n    df = data.copy()\n    \n    # get true group counts\n    group_counts = df.groupby(grouping_columns)[target_column].count().reset_index(name=\"true_count\")\n\n    # apply rounding rules\n    group_counts[target_column] = group_counts[\"true_count\"].apply(round_counts)\n    \n    # drop true count from df\n    group_counts.drop(columns=[\"true_count\"], inplace=True)\n\n    return group_counts\n\n\n\n\nCode\n# example usage\n\n# calculate the number of physicians per practice size\nphys_per_prac_size = drb_round(dataset, [\"practice_size\"], \"NPI\")\n\nphys_per_prac_size\n\n\n\n\n\n\n  \n    \n      \n      practice_size\n      NPI\n    \n  \n  \n    \n      0\n      large (20-99)\n      447000\n    \n    \n      1\n      medium (6-19)\n      74000\n    \n    \n      2\n      nan\n      102000\n    \n    \n      3\n      small (2-5)\n      49000\n    \n    \n      4\n      sole practitioner\n      450\n    \n  \n\n\n\n\n\n\n\nThe two main papers I used as influences for my DP research fall into this category. Those papers were:\nA PRACTICAL METHOD TO REDUCE PRIVACY LOSS WHEN DISCLOSING STATISTICS BASED ON SMALL SAMPLES\nRELEASING EARNINGS DISTRIBUTIONS USING DIFFERENTIALPRIVACY: DISCLOSURE AVOIDANCE SYSTEM FOR POST-SECONDARY EMPLOYMENT OUTCOMES (PSEO)\nPractically speaking, this form of DP is implemented similar to the rounding method above. The basic procedure is:\n\nGroup the data based on some predefined marginal (group of variables)\nCalculate the number of rows in each unique combination of variables (the size of each cell)\nApply the disclosure avoidance technique\n\nThe difference is that, instead of rounding based on some predefined rule in step 3, you’re adding noise from a zero-centered random distribtion (a Laplace distribution in our case).\nAlso similar to rounding, there’s a mechanism for controlling how much the protected value can differ from the true value. DP does this with the epsilon parameter, which dictates the magnitude of noise that gets added to a measurement, and thus how “private” the measurement is. A higher epsilon means that less noise is added, and the data is less private. Conversely, a low epsilon means more noise and more privacy protection.\nGenerally, you would want to use a lower epsilon (more noise, more privacy) when the data you’re working with has a high sensitivity. More information on sensitivity can be found in the first paper linked to above.\n\n\n\n\nFunction used in place of groupby + count()\nTakes in the columns you want to group by and the column you want to count and returns a differentially private count based on the given epsilon.\nAlso returns the root-mean-square error (RMSE) between the true counts and the private counts\n\n\n\nCan be used in the agg clause of a function.\nTakes in the epsilon and returns a differentially private count based on the epsilon\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\ndef diff_count(data, grouping_columns, target_column, epsilon):\n    \"\"\"Create differentially private count of cells based on input columns\"\"\"\n    # copy input df so we can change it\n    df = data.copy()\n    # get true group counts\n    group_counts = df.groupby(grouping_columns)[target_column].count().reset_index(name=\"true_count\")\n\n    # add discrete Laplacian noise to group counts to get noisy group counts\n    # noise function from here: https://randorithms.com/2020/10/09/geometric-mechanism.html\n    p = 1 - np.exp(-epsilon)\n    noise = np.random.geometric(p, size=len(group_counts)) - np.random.geometric(p, size=len(group_counts))\n    group_counts[target_column] = group_counts[\"true_count\"] + noise\n    \n    # calculate the error between the true and the noisy counts\n    err = mean_squared_error(group_counts[\"true_count\"], group_counts[target_column])\n    \n    # drop the true count from the df\n    group_counts.drop(columns=[\"true_count\"], inplace=True)\n\n    return group_counts, err\n\ndef dp_agg(data, epsilon):\n    # create noise based on the given epsilon\n    p = 1 - np.exp(-epsilon)\n    noise = np.random.geometric(p) - np.random.geometric(p)\n    \n    # get noisy count by adding noise to true count\n    return len(data) + noise\n\n\n\n\n\nDeciding on an epsilon value is more of a policy decision than a technical one. Its influenced by the tradeoff between privacy and accuracy, and how much of each you’re willing to tolerate.\nTo make an informed decision, the most common approach is to run your calculation with a range of epsilon values and choose a value that has enough privacy protection while also having as little error as possible. (Note: this procedure is why the diff_count function returns RMSE, a common metric used to measure the difference between a true statistic and an estimated statistic). Because of the random nature of adding noise, we also repeat the calculation a few times.\nA very loose “rule of thumb” is that epsilon should be around 1. However, many large scale DP implementations use epsilon values closer to 10 or higher.\nA discussion about choosing epsilon in practice can be found in this paper.\n\n\nCode\n# make a list to keep track of errors\nerrors = []\n\n# the range of epsilon values to try\nbottom_range = 0.1\ntop_range = 2\n\n# how granular the steps should be\nstep = 0.1\n\n# how many times to try each epsilon value\niterations = 5\n\nfor epsilon in np.arange(bottom_range, top_range, step):\n    # repeat the calculation five times\n    for i in range(iterations):\n        # calculate the noisy count\n        phys_by_spec_dp, rmse = diff_count(dataset, [\"pri_spec_category\"], \"NPI\", epsilon)\n        # save the error value\n        errors.append([epsilon, rmse])\n\n# plot error vs. epsilon\ndf = pd.DataFrame(errors, columns=[\"eps\", \"rmse\"])\nplt.scatter(df.eps, df.rmse)\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"RMSE\")\nplt.show()\n\n\n\n\n\n\n\nCode\ndf.groupby(\"eps\").mean()\n\n\n\n\n\n\n  \n    \n      \n      rmse\n    \n    \n      eps\n      \n    \n  \n  \n    \n      0.1\n      387.400000\n    \n    \n      0.2\n      49.600000\n    \n    \n      0.3\n      24.100000\n    \n    \n      0.4\n      13.400000\n    \n    \n      0.5\n      7.366667\n    \n    \n      0.6\n      6.866667\n    \n    \n      0.7\n      1.833333\n    \n    \n      0.8\n      7.100000\n    \n    \n      0.9\n      1.633333\n    \n    \n      1.0\n      1.433333\n    \n    \n      1.1\n      2.100000\n    \n    \n      1.2\n      1.000000\n    \n    \n      1.3\n      3.233333\n    \n    \n      1.4\n      0.666667\n    \n    \n      1.5\n      0.900000\n    \n    \n      1.6\n      0.633333\n    \n    \n      1.7\n      0.500000\n    \n    \n      1.8\n      0.666667\n    \n    \n      1.9\n      0.266667\n    \n  \n\n\n\n\n\n\n\nBased on the PSEO paper, add noise to histograms\n\n\nCode\n# choose bins\nbin_thresh = np.arange(0, 85, 10)\n\n# put ages into the bins\nage_bins = pd.cut(dataset[\"age_imputed\"], bin_thresh, right=False)\n\n# create a new df with state and bins\nhist_ex = dataset[[\"st\", \"NPI\"]].copy()\nhist_ex[\"age_bin\"] = age_bins.astype(str)\n\n# get bin counts per state and age group\nhist_ex, rmse = diff_count(hist_ex, [\"st\", \"age_bin\"], \"NPI\", 1)\n\n# plot histogram for an example state\nhist_ex = hist_ex[hist_ex[\"st\"] == \"VA\"]\nplt.bar(hist_ex[\"age_bin\"], hist_ex[\"NPI\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNow, instead of computing a statistic and then adding noise to protect the results, we create synthetic data that has similar characteristics to the original dataset, which then allows statistics to be computed without the need for additional noise injection. Broadly speaking, synthetic data generation works by modeling the original data and then using that model to create synthetic data with similar data distributions.\nAs an example, say our original dataset has 35 records from Virginia and 40 records from Maryland. The synthetic dataset, which doesn’t contain any of the same records as the original dataset, will also have 35 records from Virginia and 40 records from Maryland (or somewhere close to that).\n\n\nThe fist tool we used for generating synthetic data is CenSyn. CenSyn is a tool developed by the Census in collaboration with Knexus Research Corporation for creating synthetic datasets. The underlying model for CenSyn is based on a series of decision trees. Each tree uses a set of variables to predict another set, which can then be used to predict another set, and so on for all of the variables in the dataset.\nMore about CenSyn are available here\n\n\n\nThe other tool we used is called MST. MST was the winner of the 2018 National Institute of Standards and Technology (NIST) Differential Privacy Synthetic Data Competition. It was developed by McKenna et al. from UMass, Amherst.\nThe two main steps in MST are: 1) take noisy measurements of marginal queries, 2) estimate a data distribution from the noisy measurements and generate a synthetic dataset from the distribution.\n\n\nGiven a set of marginal queries, the first step is to take measurements and add Gaussian noise. Weights can also be assigned in this step if some queries are more important than others.\nThe noisy measurements, the weights, and the magnitude of noise are passed to the next step.\n\n\n\nMST then uses another algorithm written by McKenna, Private-PGM. Private-PGM is designed to infer a data distribution from a set of noisy measurements. It does this by solving an optimization problem comparing the noisy measurements and the estimated distribution, and can be interpreted as a maximum likelihood estimator. Once Private-PGM finds a data distribution, it can then generate a synthetic dataset that closely matches the noisy marginal measurements.\nMore details can be found in this paper about the competition, and this paper about the mechanism that MST relies on.\n\n\n\n\nThis portion of the notebook is meant to be run from your console/command prompt.\n\n\nI had issues with an SSL error the first time I tried installing CenSyn, and I solved that by updating pip:\npython -m pip install --upgrade --force-reinstall pip\n\n\n\nFrom the directory with the CenSyn wheel file, run:\npip install censyn-0.7.0-py3-none-any.whl\n\n\n\n\n\n\nThe core functions used in MST are from a github repo called private-pgm. They can be installed with:\npip install git+https://github.com/ryan112358/private-pgm.git\n\n\n\nMST uses a function called cdp2adp that comes from a github repo published by IBM. You can get them with:\ngit clone https://github.com/IBM/discrete-gaussian-differential-privacy.git\nmv discrete-gaussian-differential-privacy ibm\nThis will put the files needed for the function in a directory called ibm.\nYou’ll need to add this to your python file to actually use cdp2adp:\nimport sys\nimport os\nsys.path.append(os.getcwd() + \"/ibm\")\n\n\n\n\nBoth MST and CenSyn expect the input dataset to have numeric values. We’re going to need map the original dataset to numeric values, and then reverse map the synthetic data to get the original categorical values.\nI used an sklearn LabelEncoder to create this mapping.\n\n\nCode\nfrom sklearn import preprocessing\n\n# not 100% sure how to handle identifiers with MST, so dropping them\ndataset_presyn = dataset.drop(columns=[\"NPI\", \"tin_name\"])\n\n# get a list of the columns in the dataset\ncols = dataset_presyn.columns\n\n# create a dict to store the mappings for later use\nmapping = {}\n\n# make an empty dataframe that will be used for the mapped data\ndf = pd.DataFrame(index=range(len(dataset_presyn)),columns=cols)\n\nfor col in cols:\n    # create a mapping between the dataset's values and numbers\n    le = preprocessing.LabelEncoder()\n    le.fit(dataset_presyn[col])\n    \n    # transform the dataset column into a mapped column\n    df[col] = le.transform(dataset_presyn[col])\n    \n    # keep track of the mapping\n    mapping[col] = [le, le.classes_]\n\ndf\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      age_imputed\n      practice_type_random\n      health_sys_state\n      pri_spec_category\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      48\n      1\n      25\n      5\n      41\n      3\n      2\n      0\n      0\n    \n    \n      1\n      15\n      1\n      16\n      3\n      13\n      0\n      1\n      0\n      1\n    \n    \n      2\n      37\n      1\n      20\n      3\n      34\n      0\n      0\n      0\n      0\n    \n    \n      3\n      40\n      1\n      16\n      4\n      37\n      3\n      4\n      1\n      0\n    \n    \n      4\n      37\n      0\n      12\n      6\n      50\n      2\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      672609\n      9\n      1\n      12\n      10\n      50\n      3\n      4\n      3\n      0\n    \n    \n      672610\n      4\n      1\n      13\n      9\n      50\n      1\n      2\n      0\n      0\n    \n    \n      672611\n      9\n      1\n      36\n      0\n      50\n      3\n      3\n      2\n      0\n    \n    \n      672612\n      4\n      0\n      15\n      10\n      50\n      3\n      2\n      0\n      0\n    \n    \n      672613\n      51\n      1\n      14\n      10\n      46\n      5\n      2\n      0\n      0\n    \n  \n\n672614 rows × 9 columns\n\n\n\n\n\n\n\nCode\ncensyn_location = \"data/data.parquet\"\nmst_location = \"data.csv\"\n\ndf.to_parquet(censyn_location)\ndf.to_csv(mst_location, index=False)a\n\n\n\n\n\n\n\n\nThe only other file that MST needs is a domain file that tells MST how many possible values there are for each column\n\n\nCode\n# dict to keep track of column values\ndomain = {}\n\n# use the mapping created in the encoding step to figure out the number of values per column\nfor key, value in mapping.items():\n    # create an entry for each column\n    domain[key] = len(value[1])\n\ndomain\n\n\n{'st': 55,\n 'gndr': 2,\n 'age_imputed': 57,\n 'practice_type_random': 11,\n 'health_sys_state': 51,\n 'pri_spec_category': 6,\n 'practice_scope': 5,\n 'practice_size': 5,\n 'medicare_charges': 41}\n\n\n\n\nCode\nimport json\n\nmst_domain = \"data-domain.json\"\n\n# write the domain information to a file\nwith open(mst_domain, \"w\") as file:\n     file.write(json.dumps(domain))\n\n\n\n\n\nCenSyn configuration is a little more involved.\nThe first file you need is a feature file that tells CenSyn what each feature is and how it should be modeled. For example:\n\"st\": {\n    \"feature_desc\": \"Two-letter state code\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 2,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n        \"model_name\": \"DecisionTreeModel\",\n        \"model_type\": \"\",\n        \"model_params\": {\n            \"max_depth\": 10,\n            \"criterion\": \"entropy\",\n            \"min_impurity_decrease\": 1e-05\n        }\n    },\n    \"encoder\": {\n        \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n        \"encoder_param\": {\n            \"column\": \"st\",\n            \"indicator\": true,\n            \"inplace\": true\n        }\n    },\n    \"dependencies\": []\nFor this notebook I’m going to treat every column as categorical and use the same decision tree model configuration for each. This can (and probably should) be customized later.\n\n\nCode\n# get the max length of each field for the \"field_length\" field\nmeasurer = np.vectorize(len)\nfeaturelens = dict(zip(df, measurer(df.values.astype(str)).max(axis=0)))\n\nfeaturesjson = {}\nfor key, value in featurelens.items():\n    # fields that need to be updated for each column: field_length, encoder.encoder_param.column\n    base_entry = {\n            \"feature_desc\": \"desc\",\n            \"feature_type\": \"obj\",\n            \"field_length\": 0,\n            \"contains_null\": False,\n            \"field_density\": \"dense\",\n            \"model_info\": {\n                \"model_name\": \"DecisionTreeModel\",\n                \"model_type\": \"\",\n                \"model_params\": {\n                    \"max_depth\": 10,\n                    \"criterion\": \"entropy\",\n                    \"min_impurity_decrease\": 1e-05\n                }\n            },\n            \"encoder\": {\n                \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n                \"encoder_param\": {\n                    \"column\": \"cl\",\n                    \"indicator\": True,\n                    \"inplace\": True\n                }\n            },\n            \"dependencies\": []\n    }\n    base_entry[\"field_length\"] = int(value)\n    base_entry[\"encoder\"][\"encoder_param\"][\"column\"] = key\n    featuresjson[key] = base_entry\n\nprint(json.dumps(featuresjson, indent=2))\n\n\n{\n  \"st\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 2,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"st\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"gndr\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 1,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"gndr\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"age_imputed\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 2,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"age_imputed\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"practice_type_random\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 2,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"practice_type_random\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"health_sys_state\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 2,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"health_sys_state\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"pri_spec_category\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 1,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"pri_spec_category\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"practice_scope\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 1,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"practice_scope\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"practice_size\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 1,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"practice_size\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  },\n  \"medicare_charges\": {\n    \"feature_desc\": \"desc\",\n    \"feature_type\": \"obj\",\n    \"field_length\": 2,\n    \"contains_null\": false,\n    \"field_density\": \"dense\",\n    \"model_info\": {\n      \"model_name\": \"DecisionTreeModel\",\n      \"model_type\": \"\",\n      \"model_params\": {\n        \"max_depth\": 10,\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 1e-05\n      }\n    },\n    \"encoder\": {\n      \"encoder_type\": \"encoder.encoder.IdentityEncoder\",\n      \"encoder_param\": {\n        \"column\": \"medicare_charges\",\n        \"indicator\": true,\n        \"inplace\": true\n      }\n    },\n    \"dependencies\": []\n  }\n}\n\n\nThe second file you need is a feature schema file. I don’t know of anything here that needs to be changed.\n\n\nCode\nfeature_schema = {\n    \"$schema\": \"http://json-schema.org/schema#\",\n    \"$id\": \"censyn_feature_definition_0.1\",\n    \"type\": \"object\",\n    \"patternProperties\": {\n        \".*?\": {\n            \"properties\": {\n                \"feature_desc\": { \"type\": \"string\" },\n                \"feature_type\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                        \"obj\",\n                        \"integer\",\n                        \"floating_point\"\n                    ]\n                },\n                \"field_length\": {\n                    \"type\": \"number\" ,\n                    \"multipleOf\": 1,\n                    \"minimum\": 1\n                },\n                \"contains_null\": { \"type\": \"boolean\" },\n                \"field_density\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                        \"dense\",\n                        \"sparse\"\n                    ]\n                },\n                \"bins\": {\n                    \"type\": \"object\"\n                }\n            },\n            \"constraints\": {\n                \"range\": {\n                    \"type\": \"object\",\n                    \"patternProperties\": {\n                        \"*.^$\": {\n                            \"type\": [\n                                \"number\",\n                                \"array\",\n                                \"string\"\n                            ],\n                            \"items\": {\n                                \"type\": [\n                                    \"number\",\n                                    \"null\"\n                                ],\n                                \"minItems\": 2,\n                                \"maxItems\": 2\n                            }\n                        }\n                    },\n                    \"minItems\": 1\n                }\n            },\n            \"oneOf\": [{\n                \"required\": [\n                    \"feature_desc\",\n                    \"feature_type\",\n                    \"field_length\",\n                    \"contains_null\",\n                    \"field_density\"\n                ]\n            },\n            {\n                \"required\": [\n                    \"feature_desc\",\n                    \"feature_type\",\n                    \"field_length\",\n                    \"contains_null\",\n                    \"field_density\",\n                    \"constraints\"\n                ]\n            }]\n        }\n    }\n}\n\n\nThe last file is the synthesis configuration file. This is the file that tells CenSyn where to find all of the previously built configuration files.\nA couple notes about this file:\n\nI’m not specifying any consistency check. That’s a feature that could be useful though\n\nAn example of a consistency check might be: all synthetic data must have an age between 30 and 85\n\nI’m randomly choosing bootstrap features. It might be useful to choose those more strategically in the future\nI’m specifying a single processor. If using a system that can handle more, that might be something to play around with\n\n\n\nCode\n# specify locations for input files\nfeature_location = \"conf/features.json\"\nschema_location = \"conf/features_schema.json\"\n\n# specify location for output\ncen_output = \"syn_data/syn.parquet\"\n\n# write feature and feature schema files\nwith open(feature_location, \"w\") as file:\n     file.write(json.dumps(featuresjson, indent=2))\n\nwith open(schema_location, \"w\") as file:\n     file.write(json.dumps(feature_schema, indent=2))\n\n# randomly pick two columns to be bootstrap columns\nbootstrap_cols = np.random.choice(cols, size=2, replace=False)\n        \nconfigs = {\n    \"experiment_file\": \"\",\n    \"features_file\": feature_location,\n    \"process_features\": list(cols),\n    \"bootstrap_features\": list(bootstrap_cols),\n    \"post_process_features\": [],\n    \"ignore_features\": [],\n    \"weight_features\": [],\n    \"data_files\": [censyn_location],\n    \"filters\": [],\n    \"external_bootstrap_data_files\": [],\n    \"external_bootstrap_filters\": [],\n    \"synthesize_external_bootstrap_data\": False,\n    \"independent_features_name\": \"\",\n    \"independent_feature_filters\": [],\n    \"synthesize_filters\": [],\n    \"model_file\": \"\",\n    \"output_data_file\": cen_output,\n    \"report\": {\n        \"report_file\": \"output/synthesis_report.txt\",\n        \"report_level\": \"SUMMARY\"\n    },\n    \"processes\": 1,\n    \"logging_level\": \"WARNING\"\n}\n\n# write configuration file\nconfig_location = \"conf/synthesize.cfg\"\nwith open(config_location, \"w\") as file:\n     file.write(json.dumps(configs, indent=2))\n\n\n\n\n\n\n\n\nThis one can be run from a notebook.\nFirst we load all of the functions we’ll need:\n\n\nCode\n# source: https://github.com/ryan112358/private-pgm/blob/master/mechanisms/mst.py\n\n# add folder with cdp2adp to working directory so python can find the import\nimport sys\nimport os\nsys.path.append(os.getcwd() + \"/ibm\")\n\nimport numpy as np\nfrom mbi import FactoredInference, Dataset, Domain\nfrom scipy import sparse\nfrom disjoint_set import DisjointSet\nimport networkx as nx\nimport itertools\nfrom cdp2adp import cdp_rho\nfrom scipy.special import logsumexp\nimport argparse\n\ndef MST(data, epsilon, delta):\n    rho = cdp_rho(epsilon, delta)\n    sigma = np.sqrt(3/(2*rho))\n    cliques = [(col,) for col in data.domain]\n    log1 = measure(data, cliques, sigma)\n    data, log1, undo_compress_fn = compress_domain(data, log1)\n    cliques = select(data, rho/3.0, log1)\n    log2 = measure(data, cliques, sigma)\n    engine = FactoredInference(data.domain, iters=5000)\n    est = engine.estimate(log1+log2)\n    synth = est.synthetic_data()\n    return undo_compress_fn(synth)\n\ndef measure(data, cliques, sigma, weights=None):\n    if weights is None:\n        weights = np.ones(len(cliques))\n    weights = np.array(weights) / np.linalg.norm(weights)\n    measurements = []\n    for proj, wgt in zip(cliques, weights):\n        x = data.project(proj).datavector()\n        y = x + np.random.normal(loc=0, scale=sigma/wgt, size=x.size)\n        Q = sparse.eye(x.size)\n        measurements.append( (Q, y, sigma/wgt, proj) )\n    return measurements\n\ndef compress_domain(data, measurements):\n    supports = {}\n    new_measurements = []\n    for Q, y, sigma, proj in measurements:\n        col = proj[0]\n        sup = y >= 3*sigma\n        supports[col] = sup\n        if supports[col].sum() == y.size:\n            new_measurements.append( (Q, y, sigma, proj) )\n        else: # need to re-express measurement over the new domain\n            y2 = np.append(y[sup], y[~sup].sum())\n            I2 = np.ones(y2.size)\n            I2[-1] = 1.0 / np.sqrt(y.size - y2.size + 1.0)\n            y2[-1] /= np.sqrt(y.size - y2.size + 1.0)\n            I2 = sparse.diags(I2)\n            new_measurements.append( (I2, y2, sigma, proj) )\n    undo_compress_fn = lambda data: reverse_data(data, supports)\n    return transform_data(data, supports), new_measurements, undo_compress_fn\n\ndef exponential_mechanism(q, eps, sensitivity, prng=np.random, monotonic=False):\n    coef = 1.0 if monotonic else 0.5\n    scores = coef*eps/sensitivity*q\n    probas = np.exp(scores - logsumexp(scores))\n    return prng.choice(q.size, p=probas)\n\ndef select(data, rho, measurement_log, cliques=[]):\n    engine = FactoredInference(data.domain, iters=1000)\n    est = engine.estimate(measurement_log)\n\n    weights = {}\n    candidates = list(itertools.combinations(data.domain.attrs, 2))\n    for a, b in candidates:\n        xhat = est.project([a, b]).datavector()\n        x = data.project([a, b]).datavector()\n        weights[a,b] = np.linalg.norm(x - xhat, 1)\n\n    T = nx.Graph()\n    T.add_nodes_from(data.domain.attrs)\n    ds = DisjointSet()\n\n    for e in cliques:\n        T.add_edge(*e)\n        ds.union(*e)\n\n    r = len(list(nx.connected_components(T)))\n    epsilon = np.sqrt(8*rho/(r-1))\n    for i in range(r-1):\n        candidates = [e for e in candidates if not ds.connected(*e)]\n        wgts = np.array([weights[e] for e in candidates])\n        idx = exponential_mechanism(wgts, epsilon, sensitivity=1.0)\n        e = candidates[idx]\n        T.add_edge(*e)\n        ds.union(*e)\n\n    return list(T.edges)\n\ndef transform_data(data, supports):\n    df = data.df.copy()\n    newdom = {}\n    for col in data.domain:\n        support = supports[col]\n        size = support.sum()\n        newdom[col] = int(size)\n        if size < support.size:\n            newdom[col] += 1\n        mapping = {}\n        idx = 0\n        for i in range(support.size):\n            mapping[i] = size\n            if support[i]:\n                mapping[i] = idx\n                idx += 1\n        assert idx == size\n        df[col] = df[col].map(mapping)\n    newdom = Domain.fromdict(newdom)\n    return Dataset(df, newdom)\n\ndef reverse_data(data, supports):\n    df = data.df.copy()\n    newdom = {}\n    for col in data.domain:\n        support = supports[col]\n        mx = support.sum()\n        newdom[col] = int(support.size)\n        idx, extra = np.where(support)[0], np.where(~support)[0]\n        mask = df[col] == mx\n        if extra.size == 0:\n            pass\n        else:\n            df.loc[mask, col] = np.random.choice(extra, mask.sum())\n        df.loc[~mask, col] = idx[df.loc[~mask, col]]\n    newdom = Domain.fromdict(newdom)\n    return Dataset(df, newdom)\n\n\nC:\\Anaconda3 2020.07\\lib\\site-packages\\mbi\\__init__.py:15: UserWarning: MixtureInference disabled, please install jax and jaxlib\n  warnings.warn('MixtureInference disabled, please install jax and jaxlib')\n\n\nThen we feed in the data.\nSome notes about the parameters we’re using:\n\nDelta allows for a slightly looser definition of differential privacy. It’s defined the probability that there’s a privacy leakage in the output data\nThe degree is the type of marginals that are used to train MST. We use 3-marginals in this case\nIf computational load is a concern, num_marginals can be used to select only a subset of all possible variable combinations during training. By setting it to None, we’re telling MST to use all possible combinations\n\n\n\nCode\n# set parameters\nepsilon = 1.0\ndelta = 1e-9\ndegree = 3\nnum_marginals = None\nmax_cells = 10000\nsave = \"synth-data.csv\"\n\n# create a Dataset object with our input data\ndata = Dataset.load(mst_location, mst_domain)\n\n# create workloads based on the degree of marginals we want\nworkload = list(itertools.combinations(data.domain, degree))\n# workload = [cl for cl in workload if data.domain.size(cl) <= max_cells]\nif num_marginals is not None:\n    workload = [workload[i] for i in prng.choice(len(workload), num_marginals, replace=False)]\n\n# run the synthesis\nsynth = MST(data, epsilon, delta)\n\n# write out output data to a csv file\nif save is not None:\n    synth.df.to_csv(save, index=False)\n\n# error evaluation\nerrors = []\nfor proj in workload:\n    X = data.project(proj).datavector()\n    Y = synth.project(proj).datavector()\n    e = 0.5*np.linalg.norm(X/X.sum() - Y/Y.sum(), 1)\n    errors.append(e)\nprint('Average Error: ', np.mean(errors)) \n\n\nAverage Error:  0.08556736815034863\n\n\n\n\n\nThis one happens outside of the notebook.\nFrom the current directory, run the following command:\ncensyn --synthesize_config_file <config_location>\nReplacing <config_location> with whatever location you used in your code.\n\n\n\n\nWe now have two synthetic datasets saved as files. The next step is to read them back into pandas and use the mapping from the encoding step to decode the data back into a readable format.\n\n\nCode\n# read MST data from csv\nmst_synth = pd.read_csv(save)\n\n# read CenSyn data from parquet\ncen_synth = pd.read_parquet(cen_output)\n\n# create empty DFs for decoded data\nsynth_df_mst = pd.DataFrame(index=range(len(mst_synth)), columns=cols)\nsynth_df_cen = pd.DataFrame(index=range(len(cen_synth)), columns=cols)\n\n# decode MST data using mappings\nfor key, value in mapping.items():\n    synth_df_mst[key] = value[0].inverse_transform(mst_synth[key].astype(int))\n\n# decode CenSyn data with the mappings\nfor key, value in mapping.items():\n    synth_df_cen[key] = value[0].inverse_transform(cen_synth[key].astype(int))\n\n\n\n\nCode\nsynth_df_mst\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      age_imputed\n      practice_type_random\n      health_sys_state\n      pri_spec_category\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      FL\n      F\n      36\n      type-2\n      FL\n      Primary care\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      1\n      TX\n      M\n      36\n      type-3\n      nan\n      Surgery specialty\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      2\n      NJ\n      F\n      38\n      type-9\n      nan\n      OBGYN\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      3\n      NY\n      M\n      58\n      type-8\n      NY\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      4\n      DC\n      F\n      51\n      type-4\n      nan\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      672629\n      TX\n      M\n      63\n      type-5\n      nan\n      Medical specialty\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      672630\n      NJ\n      M\n      55\n      type-2\n      nan\n      Hospital based\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      672631\n      CT\n      M\n      40\n      type-5\n      nan\n      Medical specialty\n      multi-unit, single-cbsa\n      medium (6-19)\n      0 - 999999\n    \n    \n      672632\n      IN\n      F\n      45\n      type-0\n      IN\n      Primary care\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      672633\n      LA\n      M\n      48\n      type-1\n      nan\n      Medical specialty\n      single-unit\n      large (20-99)\n      0 - 999999\n    \n  \n\n672634 rows × 9 columns\n\n\n\n\n\nCode\nsynth_df_cen\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      age_imputed\n      practice_type_random\n      health_sys_state\n      pri_spec_category\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      NY\n      M\n      60\n      type-0\n      nan\n      Medical specialty\n      single-unit\n      medium (6-19)\n      0 - 999999\n    \n    \n      1\n      TX\n      M\n      85\n      type-7\n      TX\n      Medical specialty\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      2\n      GA\n      M\n      38\n      type-6\n      GA\n      Hospital based\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      3\n      NY\n      M\n      56\n      type-4\n      NY\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      4\n      NY\n      F\n      44\n      type-8\n      MI\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      672609\n      AK\n      M\n      70\n      type-3\n      nan\n      Surgery specialty\n      multi-unit, single-state\n      small (2-5)\n      1000000 - 1999999\n    \n    \n      672610\n      PA\n      M\n      58\n      type-5\n      PA\n      Medical specialty\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      672611\n      PR\n      M\n      69\n      nan\n      nan\n      Primary care\n      nan\n      nan\n      0 - 999999\n    \n    \n      672612\n      SC\n      M\n      58\n      type-8\n      SC\n      Surgery specialty\n      single-unit\n      large (20-99)\n      0 - 999999\n    \n    \n      672613\n      MA\n      M\n      42\n      type-2\n      MA\n      Hospital based\n      multi-unit, single-cbsa\n      medium (6-19)\n      0 - 999999\n    \n  \n\n672614 rows × 9 columns\n\n\n\n\n\n\nCenSyn has a tool for comparing counting queries across two datasets. We’re going to use it to compare the two synthetic datasets to the original dataset. The metrics returned by CenSyn show the differences between distributions in the two datsets over the same sets of features. The metrics are:\n\nThe marginal metric score (value between 0 and 2):\n\n0: Perfectly matching density distributions (for the marginals used in the comparison).\n2: No overlap whatsoever (for the marginals used in the comparison).\n\nThe NIST score (value between 0 and 1000):\n\n0: No overlap whatsoever (for the marginals used in the comparison).\n960-980: Sampling error baseline scores for ACS datasets tend to fall in this range. The sampling error baseline is computed as the distance between two 50 percent subsamples of the real data. Synthetic data scoring around 970 can generally be considered good quality.\n1000: Perfectly matching density distributions (for the marginals used in the comparison, according to the specified binning).\n\n\n\n\nJust like with synthesis, the evaluation tool needs a configuration file to run.\nA few notes on the configuration file:\n\nI’m using the full datasets in this configuration. You can use sampling by changing the filters_a.attributes.proportion feature (same for filters_b)\nMarginalMetric creates the marginal metric score and DensityDistributionMetric creates the NIST score\nIn MarginalMetric, you can change sample_ratio to choose more or fewer marginal queries to evaluate\n\nThe current setting, 0.3, randomly evaluates 30% of all possible marginals\n\nmarginal_dimensionality changes the marginal degree when calculating MarginalMetric\n\nMarginal degree is not configurable for DensityDistributionMetric (I think it defaults to 3)\n\n\nConfiguring the Eval tool for the CenSyn dataset:\n\n\nCode\n# specify location for eval report\ncen_eval_output = \"output/eval_cen.txt\"\n\n# define configurations\neval_config_cen = {\n    \"features_file\": feature_location,\n    \"process_features\":  list(cols),\n    \"ignore_features\": [],\n    \"weight_features\": [],\n    \"data_files_a\": [censyn_location],\n    \"data_files_b\": [cen_output],\n    \"filters_a\": [\n        {\n            \"class\": \"RandomFilter\",\n            \"attributes\": {\n                \"proportion\": 1.0\n            }\n        }\n    ],\n    \"filters_b\": [\n        {\n            \"class\": \"RandomFilter\",\n            \"attributes\": {\n                \"proportion\": 1.0\n            }\n        }\n    ],\n    \"metrics\": [\n        {\n            \"class\": \"MarginalMetric\",\n            \"use_bins\": True,\n            \"use_weights\": True,\n            \"attributes\": {\n                \"marginal_dimensionality\": 3,\n                \"picking_strategy\": \"random\",\n                \"sample_ratio\": 0.3,\n                \"maximum_marginals\": 0,\n                \"stable_features\": [],\n                \"logging_level\": \"DEBUG\",\n                \"name\": \"marginal_metric\"\n            }\n        },\n        {\n            \"class\": \"DensityDistributionMetric\",\n            \"use_bins\": True,\n            \"use_weights\": True,\n            \"attributes\": {\n                \"name\": \"density_distribution_metric\"\n            }\n        }\n    ],\n    \"report\" : {\n        \"report_file\": cen_eval_output,\n        \"report_level\": \"SUMMARY\",\n        \"bins_readable_values\": True,\n        \"stable_feature_display_number_lines\": 0,\n        \"stable_feature_minimum_data_count\": 0\n    },\n    \"processes\": 1,\n    \"logging_level\": \"INFO\"\n}\n\n# write configuration file\neval_config_location = \"conf/eval_cen.cfg\"\nwith open(eval_config_location, \"w\") as file:\n     file.write(json.dumps(eval_config_cen, indent=2))\n\n\nConfiguring the Eval tool for the MST dataset\n\n\nCode\n# specify location for eval report\nmst_eval_output = \"output/eval_cen.txt\"\n\n# write MST syn dataset in parquet format so CenSyn can read it\nmst_parquet = \"syn_data/syn_mst.parquet\"\nmst_synth.to_parquet(mst_parquet)\n\n# define configurations\neval_config_mst = {\n    \"features_file\": feature_location,\n    \"process_features\":  list(cols),\n    \"ignore_features\": [],\n    \"weight_features\": [],\n    \"data_files_a\": [censyn_location],\n    \"data_files_b\": [mst_parquet],\n    \"filters_a\": [\n        {\n            \"class\": \"RandomFilter\",\n            \"attributes\": {\n                \"proportion\": 1.0\n            }\n        }\n    ],\n    \"filters_b\": [\n        {\n            \"class\": \"RandomFilter\",\n            \"attributes\": {\n                \"proportion\": 1.0\n            }\n        }\n    ],\n    \"metrics\": [\n        {\n            \"class\": \"MarginalMetric\",\n            \"use_bins\": True,\n            \"use_weights\": True,\n            \"attributes\": {\n                \"marginal_dimensionality\": 3,\n                \"picking_strategy\": \"random\",\n                \"sample_ratio\": 0.3,\n                \"maximum_marginals\": 0,\n                \"stable_features\": [],\n                \"logging_level\": \"DEBUG\",\n                \"name\": \"marginal_metric\"\n            }\n        },\n        {\n            \"class\": \"DensityDistributionMetric\",\n            \"use_bins\": True,\n            \"use_weights\": True,\n            \"attributes\": {\n                \"name\": \"density_distribution_metric\"\n            }\n        }\n    ],\n    \"report\" : {\n        \"report_file\": mst_eval_output,\n        \"report_level\": \"SUMMARY\",\n        \"bins_readable_values\": True,\n        \"stable_feature_display_number_lines\": 0,\n        \"stable_feature_minimum_data_count\": 0\n    },\n    \"processes\": 1,\n    \"logging_level\": \"INFO\"\n}\n\n# write configuration file\neval_config_location_mst = \"conf/eval_mst.cfg\"\nwith open(eval_config_location_mst, \"w\") as file:\n     file.write(json.dumps(eval_config_mst, indent=2))\n\n\n\n\n\nFrom the current directory, run the two configuration files, replacing the placeholders below with the actual config filenames.\ncensyn --eval_config_file <eval_config_location>\n\ncensyn --eval_config_file <eval_config_location_mst>\nYou’ll find the reports at the locations specified by cen_eval_output and mst_eval_output.\nThe report will begin with something like this:\nMetric: transform_features-time, : 1.069183588027954\n\nMetric: density_distribution_metric, summary of results: \nRaw Marginal Score: 0.006563742257557543\nCalculated Marginal Score: 996.7181288712212\n\nMetric: density_distribution_metric, Feature error scores: \nst                      0.017582837051594392\npractice_size           0.012053905590662278\nhealth_sys_state        0.008716864433821301\npractice_scope          0.006686010968398048\ngndr                    0.0053082513486835126\npractice_type_random    0.00337282712098938\nmedicare_charges        0.0028539306408371236\nage_imputed             0.001273383890948221\npri_spec_category       0.001225669272083632\n\nMetric: density_distribution_metric, Score for medicare_charges: \nRaw Error Score: 0.0028539306408371236\nCalculated NIST Score: 998.5730346795815\n\nMetric: density_distribution_metric, Density Distribution for medicare_charges: \n+-------------------------------------------------------------+\n|         medicare_charges Density Distribution Values        |\n+------+--------------+--------------+-----------+------------+\n| Bins | Data_a Count | Data_b Count | Raw score | NIST score |\n+------+--------------+--------------+-----------+------------+\n| 0    |       595885 |       595058 |    0.0014 |    999.306 |\n| 1    |        50651 |        51297 |    0.0127 |    993.663 |\n| 10   |           31 |           36 |    0.1493 |    925.373 |\n| 11   |           40 |           45 |    0.1176 |    941.176 |\n.\n.\n.\n.\n\n\n\n\nAnother way to evaluate the two datasets is by looking directly at a desired query. For example, we can look at how the synthetic datasets compare to the true dataset when looking at physicians per specialty\n\n\nCode\n# get relevant columns from original and synth datasets\nphys_estab_state = dataset[['NPI', 'st', 'pri_spec_category']]\nphys_estab_state_mst = synth_df_mst[['st', 'pri_spec_category']]\nphys_estab_state_cen = synth_df_cen[['st', 'pri_spec_category']]\n\n# phys per specialty\nphys_by_spec = phys_estab_state.groupby(\"pri_spec_category\").NPI.count()\n\n# phys per specialty from synth data\nphys_by_spec_mst = phys_estab_state_mst.groupby(\"pri_spec_category\").size()\nphys_by_spec_cen = phys_estab_state_cen.groupby(\"pri_spec_category\").size()\n\n# merge query results into a single dataframe\nmerged = pd.concat([phys_by_spec, phys_by_spec_mst, phys_by_spec_cen], axis=1)\nmerged.columns = [\"true\", \"mst\", \"cen\"]\nmerged\n\n\n\n\n\n\n  \n    \n      \n      true\n      mst\n      cen\n    \n    \n      pri_spec_category\n      \n      \n      \n    \n  \n  \n    \n      Hospital based\n      162826\n      162856\n      165826\n    \n    \n      Medical specialty\n      140489\n      140469\n      139313\n    \n    \n      OBGYN\n      29471\n      29497\n      29540\n    \n    \n      Primary care\n      202206\n      202194\n      201603\n    \n    \n      Psychiatry\n      23062\n      23037\n      23072\n    \n    \n      Surgery specialty\n      114560\n      114581\n      113260\n    \n  \n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt \n\nx_len = np.arange(len(merged))\nplt.bar(x_len - 0.2, merged[\"true\"], 0.2, label = \"True Count\")\nplt.bar(x_len, merged[\"mst\"], 0.2, label = \"MST Count\")\nplt.bar(x_len + 0.2, merged[\"cen\"], 0.2, label = \"CenSyn Count\")\n  \nplt.xticks(x_len, list(merged.index))\nplt.xlabel(\"Specialties\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\nrmse = mean_squared_error(merged[\"true\"], merged[\"mst\"])\nprint(\"MST RMSE: \", rmse)\n\nrmse = mean_squared_error(merged[\"true\"], merged[\"cen\"])\nprint(\"CenSyn RMSE: \", rmse)\n\n\nMST RMSE:  531.0\nCenSyn RMSE:  2073574.3333333333"
  },
  {
    "objectID": "Examples.html",
    "href": "Examples.html",
    "title": "Privacy Models for E-Health",
    "section": "",
    "text": "CDF Summer Project 2022\n\n\n\n\nCode\n# load data science libraries\nimport pandas as pd\nimport numpy as np\n\ndata_dir = \"\"\n\n\n\n\nCode\nphysicians = pd.read_csv(data_dir + \"physicians.csv\")\nestablishments = pd.read_csv(data_dir + \"establishments.csv\", dtype={'zip': str, 'zip9': str, 'zip3': str})\npractices = pd.read_csv(data_dir + \"practices.csv\")\nfirms = pd.read_csv(data_dir + \"firms.csv\")\nphys2estab = pd.read_csv(data_dir + \"rel_phys2estab.csv\")\nphys2prac = pd.read_csv(data_dir + \"rel_phys2prac.csv\")\ncbsa = pd.read_csv(data_dir + \"geo_cbsas.csv\")\nstates = pd.read_csv(data_dir + \"geo_state.csv\")\nstate_agesex = pd.read_csv(data_dir + \"geo_state_agesex.csv\")\nstate_abr = pd.read_csv(data_dir + \"state_abr.csv\", names=[\"name\", \"abr\"])\n\n\n\n\nCode\n# read from raw agesex data\nstate_agesex_raw = pd.read_csv(data_dir + \"state_agesex_raw.csv\")\n\n# filter out national counts and total counts\nstate_agesex_raw_filtered = state_agesex_raw[(state_agesex_raw[\"STATE\"] > 0) \n                                             & (state_agesex_raw[\"AGE\"] != 999)\n                                             & (state_agesex_raw[\"SEX\"] != 0)][[\"NAME\", \"SEX\", \"AGE\", \"POPEST2019_CIV\"]]\n\n# update M/F to match physician dataset\nstate_agesex_raw_filtered[\"SEX\"] = np.where(state_agesex_raw_filtered[\"SEX\"] == 1, \"M\", \"F\")\n\n# put ages into bins of 10 years\nage_bins = np.arange(0, 110, 10)\nidxs = np.digitize(state_agesex_raw_filtered[\"AGE\"], age_bins)\n\n# add bin labels to dataset\nstate_agesex_raw_filtered[\"bin\"] = idxs\n\n# group by bins and get total populations\nstate_agesex_binned = state_agesex_raw_filtered.groupby([\"NAME\", \"SEX\", \"bin\"]).agg({\"POPEST2019_CIV\": \"sum\", \"AGE\": [\"min\", \"max\"]})\n\n# format column names\nstate_agesex_binned.columns = state_agesex_binned.columns.map('_'.join)\nstate_agesex_f = state_agesex_binned.reset_index()\nstate_agesex_f.drop(columns=[\"bin\"], inplace=True)\nstate_agesex_f.columns = [\"st\", \"sex\", \"pop\", \"bin_min\", \"bin_max\"]\n\n# update state names to match physician files\nstate_agesex_st_nms = pd.merge(state_agesex_f, state_abr, how=\"left\", left_on=\"st\", right_on=\"name\")[[\"sex\", \"pop\", \"bin_min\", \"bin_max\", \"abr\"]]\n\n\n# save to csv\nstate_agesex_st_nms.to_csv(data_dir + \"state_agesex.csv\", index=False)\n\n\n\n\nCode\nstate_agesex = pd.read_csv(data_dir + \"state_agesex.csv\")\n\n\n\n\n\nWhile this research question can be answered using existing public data (including Census ACS), it’s included here as an example of a simple aggregate table that can be generated at different geographic levels. Note that the data above already loads population counts for cbsa and state-level statistics.\nBreakdown by Characteristics: Specialty Category\nBreakdown by Geography: Nation, State, CBSA\n\n\nCode\n# Calculate the number of physicians per 10,000 residents in the United States\nphysicians.NPI.count() / (states['pop'].sum() / 10000)\n\n\n17.00683953715448\n\n\n\n\nCode\n# Calculate the number of physicians per 10,000 residents in the United States for each specialty category\nphys_by_spec = physicians.pri_spec_category.value_counts().reset_index()\nphys_by_spec['per 10k'] = phys_by_spec['pri_spec_category'] / (states['pop'].sum() / 10000)\nphys_by_spec\n\n\n\n\n\n\n  \n    \n      \n      index\n      pri_spec_category\n      per 10k\n    \n  \n  \n    \n      0\n      Primary care\n      176865\n      5.362390\n    \n    \n      1\n      Hospital based\n      123518\n      3.744956\n    \n    \n      2\n      Medical specialty\n      115389\n      3.498492\n    \n    \n      3\n      Surgery specialty\n      98614\n      2.989889\n    \n    \n      4\n      OBGYN\n      26941\n      0.816827\n    \n    \n      5\n      Psychiatry\n      19601\n      0.594285\n    \n  \n\n\n\n\n\n\nCode\n# Calculate the number of physicians per 10,000 residents for each US state\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# use drop duplicates to ensure we only count a physician once per state\nphys_estab_state = phys_estab[['NPI', 'st', 'pri_spec_category']].copy().drop_duplicates()\nphys_by_state = phys_estab_state.groupby('st').NPI.count().reset_index()\nphys_by_state_spec = pd.crosstab(phys_estab_state.st, phys_estab_state.pri_spec_category)\n\nstates.rename(columns={'state': 'st'}, inplace=True)\nphys_by_state_spec = pd.merge(states[['st', 'pop']], phys_by_state_spec, on='st', how='left')\nphys_by_state_spec = pd.merge(phys_by_state_spec, phys_by_state, on='st', how='left')\n\nfor col in phys_by_state_spec.columns[~phys_by_state_spec.columns.isin(states.columns)]:\n    phys_by_state_spec[col + \" per 10k\"] = (phys_by_state_spec[col] / (phys_by_state_spec['pop'] / 10000)).round()\n\nphys_by_state_spec.sort_values(by='NPI per 10k', ascending=False, inplace=True)\n\n\n\n\nCode\nphys_by_state_spec\n\n\n\n\n\n\n  \n    \n      \n      st\n      pop\n      Hospital based\n      Medical specialty\n      OBGYN\n      Primary care\n      Psychiatry\n      Surgery specialty\n      NPI\n      Hospital based per 10k\n      Medical specialty per 10k\n      OBGYN per 10k\n      Primary care per 10k\n      Psychiatry per 10k\n      Surgery specialty per 10k\n      NPI per 10k\n    \n  \n  \n    \n      8\n      DC\n      701974\n      664.0\n      711.0\n      191.0\n      806.0\n      106.0\n      516.0\n      2994.0\n      9.0\n      10.0\n      3.0\n      11.0\n      2.0\n      7.0\n      43.0\n    \n    \n      21\n      MA\n      6873003\n      4356.0\n      4290.0\n      826.0\n      5592.0\n      1044.0\n      2865.0\n      18973.0\n      6.0\n      6.0\n      1.0\n      8.0\n      2.0\n      4.0\n      28.0\n    \n    \n      45\n      VT\n      624340\n      328.0\n      236.0\n      71.0\n      551.0\n      107.0\n      265.0\n      1558.0\n      5.0\n      4.0\n      1.0\n      9.0\n      2.0\n      4.0\n      25.0\n    \n    \n      39\n      RI\n      1057798\n      508.0\n      501.0\n      133.0\n      860.0\n      158.0\n      407.0\n      2567.0\n      5.0\n      5.0\n      1.0\n      8.0\n      1.0\n      4.0\n      24.0\n    \n    \n      6\n      CT\n      3570549\n      1773.0\n      1903.0\n      496.0\n      2343.0\n      459.0\n      1487.0\n      8461.0\n      5.0\n      5.0\n      1.0\n      7.0\n      1.0\n      4.0\n      24.0\n    \n    \n      19\n      ME\n      1340825\n      694.0\n      526.0\n      119.0\n      1112.0\n      136.0\n      561.0\n      3148.0\n      5.0\n      4.0\n      1.0\n      8.0\n      1.0\n      4.0\n      23.0\n    \n    \n      23\n      MN\n      5600166\n      2590.0\n      2637.0\n      503.0\n      4446.0\n      442.0\n      2049.0\n      12667.0\n      5.0\n      5.0\n      1.0\n      8.0\n      1.0\n      4.0\n      23.0\n    \n    \n      20\n      MD\n      6037624\n      2976.0\n      2917.0\n      621.0\n      4035.0\n      573.0\n      2461.0\n      13583.0\n      5.0\n      5.0\n      1.0\n      7.0\n      1.0\n      4.0\n      22.0\n    \n    \n      30\n      NJ\n      8885418\n      4304.0\n      4475.0\n      965.0\n      5176.0\n      705.0\n      3481.0\n      19106.0\n      5.0\n      5.0\n      1.0\n      6.0\n      1.0\n      4.0\n      22.0\n    \n    \n      38\n      PA\n      12794885\n      6453.0\n      6143.0\n      1359.0\n      8421.0\n      1074.0\n      5092.0\n      28542.0\n      5.0\n      5.0\n      1.0\n      7.0\n      1.0\n      4.0\n      22.0\n    \n    \n      32\n      NY\n      19514849\n      9979.0\n      10052.0\n      1990.0\n      11500.0\n      1991.0\n      7507.0\n      43019.0\n      5.0\n      5.0\n      1.0\n      6.0\n      1.0\n      4.0\n      22.0\n    \n    \n      34\n      ND\n      760394\n      367.0\n      316.0\n      71.0\n      467.0\n      68.0\n      329.0\n      1618.0\n      5.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      21.0\n    \n    \n      7\n      DE\n      967679\n      415.0\n      405.0\n      113.0\n      680.0\n      62.0\n      374.0\n      2049.0\n      4.0\n      4.0\n      1.0\n      7.0\n      1.0\n      4.0\n      21.0\n    \n    \n      1\n      AK\n      736990\n      440.0\n      200.0\n      67.0\n      533.0\n      50.0\n      280.0\n      1570.0\n      6.0\n      3.0\n      1.0\n      7.0\n      1.0\n      4.0\n      21.0\n    \n    \n      49\n      WI\n      5806975\n      3164.0\n      2239.0\n      526.0\n      3827.0\n      426.0\n      2095.0\n      12277.0\n      5.0\n      4.0\n      1.0\n      7.0\n      1.0\n      4.0\n      21.0\n    \n    \n      50\n      WY\n      581348\n      317.0\n      216.0\n      52.0\n      328.0\n      22.0\n      257.0\n      1192.0\n      5.0\n      4.0\n      1.0\n      6.0\n      0.0\n      4.0\n      21.0\n    \n    \n      22\n      MI\n      9973907\n      4143.0\n      3702.0\n      1032.0\n      6552.0\n      707.0\n      3356.0\n      19492.0\n      4.0\n      4.0\n      1.0\n      7.0\n      1.0\n      3.0\n      20.0\n    \n    \n      41\n      SD\n      879336\n      373.0\n      320.0\n      88.0\n      574.0\n      45.0\n      366.0\n      1766.0\n      4.0\n      4.0\n      1.0\n      7.0\n      1.0\n      4.0\n      20.0\n    \n    \n      26\n      MT\n      1061705\n      564.0\n      357.0\n      77.0\n      660.0\n      53.0\n      410.0\n      2121.0\n      5.0\n      3.0\n      1.0\n      6.0\n      0.0\n      4.0\n      20.0\n    \n    \n      48\n      WV\n      1807426\n      755.0\n      627.0\n      152.0\n      1188.0\n      122.0\n      694.0\n      3538.0\n      4.0\n      3.0\n      1.0\n      7.0\n      1.0\n      4.0\n      20.0\n    \n    \n      35\n      OH\n      11675275\n      4996.0\n      4609.0\n      1157.0\n      7041.0\n      744.0\n      4106.0\n      22653.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      19.0\n    \n    \n      37\n      OR\n      4176346\n      1813.0\n      1420.0\n      332.0\n      2570.0\n      206.0\n      1397.0\n      7738.0\n      4.0\n      3.0\n      1.0\n      6.0\n      0.0\n      3.0\n      19.0\n    \n    \n      25\n      MO\n      6124160\n      2713.0\n      2464.0\n      577.0\n      3225.0\n      440.0\n      2146.0\n      11565.0\n      4.0\n      4.0\n      1.0\n      5.0\n      1.0\n      4.0\n      19.0\n    \n    \n      15\n      IA\n      3150011\n      1408.0\n      1138.0\n      249.0\n      1766.0\n      193.0\n      1105.0\n      5859.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      19.0\n    \n    \n      13\n      IL\n      12716164\n      5324.0\n      4935.0\n      1197.0\n      7517.0\n      759.0\n      3936.0\n      23668.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      3.0\n      19.0\n    \n    \n      17\n      KY\n      4461952\n      1751.0\n      1639.0\n      413.0\n      2408.0\n      231.0\n      1525.0\n      7967.0\n      4.0\n      4.0\n      1.0\n      5.0\n      1.0\n      3.0\n      18.0\n    \n    \n      16\n      KS\n      2912619\n      1213.0\n      1062.0\n      242.0\n      1659.0\n      158.0\n      1032.0\n      5366.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      18.0\n    \n    \n      27\n      NE\n      1923826\n      870.0\n      681.0\n      171.0\n      1021.0\n      91.0\n      716.0\n      3550.0\n      5.0\n      4.0\n      1.0\n      5.0\n      0.0\n      4.0\n      18.0\n    \n    \n      14\n      IN\n      6696893\n      2951.0\n      2326.0\n      599.0\n      3721.0\n      349.0\n      2157.0\n      12103.0\n      4.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      18.0\n    \n    \n      11\n      HI\n      1420074\n      688.0\n      407.0\n      118.0\n      917.0\n      78.0\n      396.0\n      2604.0\n      5.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      18.0\n    \n    \n      47\n      WA\n      7512465\n      3290.0\n      2672.0\n      474.0\n      4588.0\n      339.0\n      2268.0\n      13631.0\n      4.0\n      4.0\n      1.0\n      6.0\n      0.0\n      3.0\n      18.0\n    \n    \n      42\n      TN\n      6772268\n      2354.0\n      2371.0\n      619.0\n      3581.0\n      271.0\n      2123.0\n      11319.0\n      3.0\n      4.0\n      1.0\n      5.0\n      0.0\n      3.0\n      17.0\n    \n    \n      33\n      NC\n      10386227\n      3938.0\n      3642.0\n      984.0\n      5268.0\n      679.0\n      3194.0\n      17705.0\n      4.0\n      4.0\n      1.0\n      5.0\n      1.0\n      3.0\n      17.0\n    \n    \n      46\n      VA\n      8509358\n      3036.0\n      2919.0\n      740.0\n      4646.0\n      532.0\n      2544.0\n      14417.0\n      4.0\n      3.0\n      1.0\n      5.0\n      1.0\n      3.0\n      17.0\n    \n    \n      9\n      FL\n      21216924\n      7438.0\n      7974.0\n      1537.0\n      11185.0\n      980.0\n      6283.0\n      35397.0\n      4.0\n      4.0\n      1.0\n      5.0\n      0.0\n      3.0\n      17.0\n    \n    \n      40\n      SC\n      5091517\n      1763.0\n      1527.0\n      483.0\n      2842.0\n      281.0\n      1595.0\n      8491.0\n      3.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      17.0\n    \n    \n      18\n      LA\n      4664616\n      1635.0\n      1636.0\n      497.0\n      2297.0\n      213.0\n      1620.0\n      7898.0\n      4.0\n      4.0\n      1.0\n      5.0\n      0.0\n      3.0\n      17.0\n    \n    \n      0\n      AL\n      4893186\n      1456.0\n      1523.0\n      423.0\n      2521.0\n      213.0\n      1488.0\n      7624.0\n      3.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      16.0\n    \n    \n      31\n      NM\n      2097021\n      827.0\n      558.0\n      166.0\n      1203.0\n      114.0\n      549.0\n      3417.0\n      4.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      16.0\n    \n    \n      5\n      CO\n      5684926\n      2412.0\n      1783.0\n      386.0\n      2668.0\n      234.0\n      1687.0\n      9170.0\n      4.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      16.0\n    \n    \n      3\n      AR\n      3011873\n      999.0\n      823.0\n      262.0\n      1712.0\n      135.0\n      812.0\n      4743.0\n      3.0\n      3.0\n      1.0\n      6.0\n      0.0\n      3.0\n      16.0\n    \n    \n      2\n      AZ\n      7174064\n      2695.0\n      2337.0\n      400.0\n      3460.0\n      332.0\n      1905.0\n      11129.0\n      4.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      16.0\n    \n    \n      36\n      OK\n      3949342\n      1322.0\n      1062.0\n      315.0\n      2034.0\n      188.0\n      1070.0\n      5991.0\n      3.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      15.0\n    \n    \n      24\n      MS\n      2981835\n      1094.0\n      912.0\n      272.0\n      1293.0\n      103.0\n      944.0\n      4618.0\n      4.0\n      3.0\n      1.0\n      4.0\n      0.0\n      3.0\n      15.0\n    \n    \n      12\n      ID\n      1754367\n      529.0\n      462.0\n      125.0\n      908.0\n      69.0\n      548.0\n      2641.0\n      3.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      15.0\n    \n    \n      44\n      UT\n      3151239\n      1211.0\n      815.0\n      187.0\n      1312.0\n      122.0\n      891.0\n      4538.0\n      4.0\n      3.0\n      1.0\n      4.0\n      0.0\n      3.0\n      14.0\n    \n    \n      10\n      GA\n      10516579\n      3278.0\n      3042.0\n      756.0\n      4609.0\n      415.0\n      2854.0\n      14954.0\n      3.0\n      3.0\n      1.0\n      4.0\n      0.0\n      3.0\n      14.0\n    \n    \n      4\n      CA\n      39346023\n      13534.0\n      11304.0\n      1955.0\n      18319.0\n      1850.0\n      9537.0\n      56499.0\n      3.0\n      3.0\n      0.0\n      5.0\n      0.0\n      2.0\n      14.0\n    \n    \n      43\n      TX\n      28635442\n      8845.0\n      8004.0\n      1902.0\n      11883.0\n      980.0\n      6915.0\n      38529.0\n      3.0\n      3.0\n      1.0\n      4.0\n      0.0\n      2.0\n      13.0\n    \n    \n      51\n      PR\n      3255642\n      531.0\n      948.0\n      152.0\n      1370.0\n      110.0\n      596.0\n      3707.0\n      2.0\n      3.0\n      0.0\n      4.0\n      0.0\n      2.0\n      11.0\n    \n    \n      28\n      MV\n      3030281\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      29\n      MH\n      1355244\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nCode\n# Calculate the number of physicians per 10,000 residents for each CBSA\n\n# use drop duplicates to ensure we only count a physician once per cbsa\nphys_estab_cbsa = phys_estab[['NPI', 'cbsa', 'pri_spec_category']].copy().drop_duplicates()\nphys_by_cbsa = phys_estab_cbsa.groupby('cbsa').NPI.count().reset_index()\nphys_by_cbsa_spec = pd.crosstab(phys_estab_cbsa.cbsa, phys_estab_cbsa.pri_spec_category)\nphys_by_cbsa_spec = pd.merge(cbsa, phys_by_cbsa_spec, on='cbsa', how='left')\nphys_by_cbsa_spec = pd.merge(phys_by_cbsa_spec, phys_by_cbsa, on='cbsa', how='left')\n\nfor col in phys_by_cbsa_spec.columns[~phys_by_cbsa_spec.columns.str.startswith('cbsa')]:\n    phys_by_cbsa_spec[col + \" per 10k\"] = (phys_by_cbsa_spec[col] / (phys_by_cbsa_spec['cbsa_pop'] / 10000)).round()\n\nphys_by_cbsa_spec.sort_values(by='NPI per 10k', ascending=False, inplace=True)\n\n\n\n\nCode\nphys_by_cbsa_spec.head(10)\n\n\n\n\n\n\n  \n    \n      \n      cbsa\n      cbsa_name\n      cbsa_pop\n      Hospital based\n      Medical specialty\n      OBGYN\n      Primary care\n      Psychiatry\n      Surgery specialty\n      NPI\n      Hospital based per 10k\n      Medical specialty per 10k\n      OBGYN per 10k\n      Primary care per 10k\n      Psychiatry per 10k\n      Surgery specialty per 10k\n      NPI per 10k\n    \n  \n  \n    \n      697\n      32860\n      Menomonie, WI\n      45452\n      266.0\n      195.0\n      35.0\n      213.0\n      18.0\n      168.0\n      895.0\n      59.0\n      43.0\n      8.0\n      47.0\n      4.0\n      37.0\n      197.0\n    \n    \n      455\n      15140\n      Brownsville, TN\n      17002\n      50.0\n      46.0\n      12.0\n      89.0\n      2.0\n      32.0\n      231.0\n      29.0\n      27.0\n      7.0\n      52.0\n      1.0\n      19.0\n      136.0\n    \n    \n      863\n      44900\n      Summerville, GA\n      24843\n      62.0\n      49.0\n      9.0\n      156.0\n      9.0\n      46.0\n      331.0\n      25.0\n      20.0\n      4.0\n      63.0\n      4.0\n      19.0\n      133.0\n    \n    \n      293\n      40340\n      Rochester, MN\n      223062\n      712.0\n      912.0\n      31.0\n      411.0\n      69.0\n      345.0\n      2480.0\n      32.0\n      41.0\n      1.0\n      18.0\n      3.0\n      15.0\n      111.0\n    \n    \n      40\n      14100\n      Bloomsburg-Berwick, PA\n      82884\n      239.0\n      170.0\n      30.0\n      249.0\n      19.0\n      164.0\n      871.0\n      29.0\n      21.0\n      4.0\n      30.0\n      2.0\n      20.0\n      105.0\n    \n    \n      390\n      10660\n      Albert Lea, MN\n      30364\n      82.0\n      69.0\n      16.0\n      96.0\n      10.0\n      43.0\n      316.0\n      27.0\n      23.0\n      5.0\n      32.0\n      3.0\n      14.0\n      104.0\n    \n    \n      526\n      20660\n      Easton, MD\n      36972\n      62.0\n      65.0\n      19.0\n      99.0\n      15.0\n      69.0\n      329.0\n      17.0\n      18.0\n      5.0\n      27.0\n      4.0\n      19.0\n      89.0\n    \n    \n      675\n      31680\n      Malvern, AR\n      33787\n      65.0\n      58.0\n      8.0\n      84.0\n      9.0\n      41.0\n      265.0\n      19.0\n      17.0\n      2.0\n      25.0\n      3.0\n      12.0\n      78.0\n    \n    \n      405\n      11740\n      Ashland, OH\n      53362\n      219.0\n      59.0\n      10.0\n      71.0\n      4.0\n      55.0\n      418.0\n      41.0\n      11.0\n      2.0\n      13.0\n      1.0\n      10.0\n      78.0\n    \n    \n      406\n      11780\n      Ashtabula, OH\n      96513\n      313.0\n      90.0\n      31.0\n      242.0\n      15.0\n      66.0\n      757.0\n      32.0\n      9.0\n      3.0\n      25.0\n      2.0\n      7.0\n      78.0\n    \n  \n\n\n\n\n\n\n\nOur work will provide insights on the institutional structures through which physicians provide care. These structures have changed dramatically over time as a consequence of intensive vertical integration in the healthcare industry.\nThis research question is mostly centered on creating counts or percent breakdowns by state and/or metropolitan area.\nThe main task is to translate business characteristics into distinct “types” by which to group physicians.\nFollow-up question of interest: Rural/urban split (note: need to obtain data)\n\n\nCode\n#TODO\n# link physicians to establishments\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# get establishment_type column, location, and identifier\nphys_estab_state = phys_estab[['NPI', 'st', 'establishment_type']].copy().drop_duplicates()\n\n# find doctor's per state\nphys_by_state = phys_estab_state.groupby('st').NPI.count().reset_index()\n\n# find doctor's per establishment type\nest_type_crosstab = pd.crosstab(phys_estab_state.st, phys_estab_state.establishment_type)\n\n# add state count to establishment type count\nphys_by_est = pd.merge(est_type_crosstab, phys_by_state, how=\"left\", on=\"st\")\n\n# get list of establishment types\nests = phys_estab_state[\"establishment_type\"].unique()\nfor est in ests:\n    # find the fraction of all physicians who are part of each type of establishment\n    phys_by_est[est + \"_fraction\"] = phys_by_est[est] / phys_by_est.NPI\n\nphys_by_est\n\n\n\n\n\n\n\n  \n    \n      \n      st\n      multi-unit\n      single-unit\n      NPI\n      multi-unit_fraction\n      single-unit_fraction\n    \n  \n  \n    \n      0\n      AK\n      949\n      689\n      1638\n      0.579365\n      0.420635\n    \n    \n      1\n      AL\n      4966\n      3059\n      8025\n      0.618816\n      0.381184\n    \n    \n      2\n      AR\n      3456\n      1489\n      4945\n      0.698888\n      0.301112\n    \n    \n      3\n      AZ\n      8097\n      3551\n      11648\n      0.695141\n      0.304859\n    \n    \n      4\n      CA\n      39501\n      19905\n      59406\n      0.664933\n      0.335067\n    \n    \n      5\n      CO\n      6960\n      2612\n      9572\n      0.727121\n      0.272879\n    \n    \n      6\n      CT\n      6905\n      1946\n      8851\n      0.780138\n      0.219862\n    \n    \n      7\n      DC\n      2611\n      423\n      3034\n      0.860580\n      0.139420\n    \n    \n      8\n      DE\n      1603\n      555\n      2158\n      0.742817\n      0.257183\n    \n    \n      9\n      FL\n      25439\n      11570\n      37009\n      0.687373\n      0.312627\n    \n    \n      10\n      GA\n      9973\n      5950\n      15923\n      0.626327\n      0.373673\n    \n    \n      11\n      GU\n      22\n      187\n      209\n      0.105263\n      0.894737\n    \n    \n      12\n      HI\n      1654\n      1014\n      2668\n      0.619940\n      0.380060\n    \n    \n      13\n      IA\n      5121\n      932\n      6053\n      0.846027\n      0.153973\n    \n    \n      14\n      ID\n      2048\n      726\n      2774\n      0.738284\n      0.261716\n    \n    \n      15\n      IL\n      19368\n      5400\n      24768\n      0.781977\n      0.218023\n    \n    \n      16\n      IN\n      10454\n      2010\n      12464\n      0.838736\n      0.161264\n    \n    \n      17\n      KS\n      4378\n      1182\n      5560\n      0.787410\n      0.212590\n    \n    \n      18\n      KY\n      6670\n      1495\n      8165\n      0.816901\n      0.183099\n    \n    \n      19\n      LA\n      5352\n      3082\n      8434\n      0.634574\n      0.365426\n    \n    \n      20\n      MA\n      15509\n      4635\n      20144\n      0.769907\n      0.230093\n    \n    \n      21\n      MD\n      10810\n      3356\n      14166\n      0.763095\n      0.236905\n    \n    \n      22\n      ME\n      2650\n      585\n      3235\n      0.819165\n      0.180835\n    \n    \n      23\n      MI\n      15094\n      5518\n      20612\n      0.732292\n      0.267708\n    \n    \n      24\n      MN\n      12093\n      1235\n      13328\n      0.907338\n      0.092662\n    \n    \n      25\n      MO\n      9919\n      2132\n      12051\n      0.823085\n      0.176915\n    \n    \n      26\n      MP\n      2\n      31\n      33\n      0.060606\n      0.939394\n    \n    \n      27\n      MS\n      3211\n      1662\n      4873\n      0.658937\n      0.341063\n    \n    \n      28\n      MT\n      1712\n      483\n      2195\n      0.779954\n      0.220046\n    \n    \n      29\n      NC\n      14980\n      3397\n      18377\n      0.815149\n      0.184851\n    \n    \n      30\n      ND\n      1456\n      207\n      1663\n      0.875526\n      0.124474\n    \n    \n      31\n      NE\n      2795\n      869\n      3664\n      0.762828\n      0.237172\n    \n    \n      32\n      NH\n      2897\n      537\n      3434\n      0.843623\n      0.156377\n    \n    \n      33\n      NJ\n      12525\n      7932\n      20457\n      0.612260\n      0.387740\n    \n    \n      34\n      NM\n      2548\n      971\n      3519\n      0.724069\n      0.275931\n    \n    \n      35\n      NV\n      2678\n      1527\n      4205\n      0.636861\n      0.363139\n    \n    \n      36\n      NY\n      32045\n      13544\n      45589\n      0.702911\n      0.297089\n    \n    \n      37\n      OH\n      19348\n      4038\n      23386\n      0.827333\n      0.172667\n    \n    \n      38\n      OK\n      4267\n      2074\n      6341\n      0.672922\n      0.327078\n    \n    \n      39\n      OR\n      6204\n      1862\n      8066\n      0.769154\n      0.230846\n    \n    \n      40\n      PA\n      23908\n      5577\n      29485\n      0.810853\n      0.189147\n    \n    \n      41\n      PR\n      719\n      3216\n      3935\n      0.182719\n      0.817281\n    \n    \n      42\n      RI\n      1795\n      880\n      2675\n      0.671028\n      0.328972\n    \n    \n      43\n      SC\n      7009\n      1784\n      8793\n      0.797111\n      0.202889\n    \n    \n      44\n      SD\n      1493\n      297\n      1790\n      0.834078\n      0.165922\n    \n    \n      45\n      TN\n      8844\n      3021\n      11865\n      0.745386\n      0.254614\n    \n    \n      46\n      TX\n      27330\n      12889\n      40219\n      0.679530\n      0.320470\n    \n    \n      47\n      UT\n      3843\n      822\n      4665\n      0.823794\n      0.176206\n    \n    \n      48\n      VA\n      11752\n      3213\n      14965\n      0.785299\n      0.214701\n    \n    \n      49\n      VI\n      32\n      73\n      105\n      0.304762\n      0.695238\n    \n    \n      50\n      VT\n      1284\n      300\n      1584\n      0.810606\n      0.189394\n    \n    \n      51\n      WA\n      11825\n      2223\n      14048\n      0.841757\n      0.158243\n    \n    \n      52\n      WI\n      11337\n      1426\n      12763\n      0.888271\n      0.111729\n    \n    \n      53\n      WV\n      2873\n      817\n      3690\n      0.778591\n      0.221409\n    \n    \n      54\n      WY\n      870\n      358\n      1228\n      0.708469\n      0.291531\n    \n  \n\n\n\n\n\n\nCode\n# could also add per capita stats\n\n\n\n\n\nWith the growing interest in social determinants of health and community representation, our project has an opportunity to provide unique insights on how and whether physicians reflect the communities they serve. While our research will most likely focus on place of birth and race/ethnicity, we will instead use age and gender as a stand-in since these can be readily obtained from public data.\nBreakdown by Characteristics: Specialty Category, Organization Type\nBreakdown by Geography: Nation, State, CBSA\n\n\nCode\n#TODO\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# use drop duplicates to ensure we only count a physician once per state\nphys_estab_state = phys_estab[['NPI', 'st', 'gndr', 'age_imputed']].copy().drop_duplicates()\n\n# put ages into bins of 10 years\nage_bins = np.arange(0, 110, 10)\nidxs = np.digitize(phys_estab_state[\"age_imputed\"], age_bins)\n\n# add bin labels to dataset\nphys_estab_state[\"bin\"] = idxs\n\n# group by state and bins and get total populations\nphys_age_sex = phys_estab_state.groupby([\"st\", \"gndr\", \"bin\"]).agg({\"NPI\": \"count\", \"age_imputed\": \"min\"}).reset_index()\nphys_age_sex = phys_age_sex.drop(columns=[\"bin\"])\n\n# add state data\nphys_age_sex_pop = pd.merge(phys_age_sex, state_agesex, how=\"left\", left_on=[\"st\", \"gndr\", \"age_imputed\"], right_on=[\"abr\", \"sex\", \"bin_min\"])\n\n# filter fields\nphys_age_sex_pop = phys_age_sex_pop[[\"st\", \"gndr\", \"bin_min\", \"NPI\", \"pop\"]]\n\n# compute per 10k stat\nphys_age_sex_pop[\"phy_per_10k\"] = phys_age_sex_pop[\"NPI\"] / (phys_age_sex_pop[\"pop\"]/10000)\n\n\n\n\nCode\nphys_age_sex_pop\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      bin_min\n      NPI\n      pop\n      phy_per_10k\n    \n  \n  \n    \n      0\n      AK\n      F\n      30.0\n      145\n      51272.0\n      28.280543\n    \n    \n      1\n      AK\n      F\n      40.0\n      168\n      40674.0\n      41.304027\n    \n    \n      2\n      AK\n      F\n      50.0\n      130\n      43451.0\n      29.918759\n    \n    \n      3\n      AK\n      F\n      60.0\n      88\n      39819.0\n      22.100003\n    \n    \n      4\n      AK\n      F\n      70.0\n      17\n      19719.0\n      8.621127\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      813\n      WY\n      M\n      50.0\n      250\n      35040.0\n      71.347032\n    \n    \n      814\n      WY\n      M\n      60.0\n      199\n      37838.0\n      52.592632\n    \n    \n      815\n      WY\n      M\n      70.0\n      72\n      21138.0\n      34.061879\n    \n    \n      816\n      WY\n      M\n      80.0\n      8\n      9006.0\n      8.882967\n    \n    \n      817\n      WY\n      M\n      NaN\n      3\n      NaN\n      NaN\n    \n  \n\n818 rows × 6 columns\n\n\n\n\n\nCode\n\"\"\"\npermutation tests\nhttps://www.cs.cornell.edu/courses/cs1380/2018sp/textbook/chapters/16/1/two-categorical-distributions.html\n\"\"\"\n# function to get proportion of a class in the data\ndef proportions(array):\n    return array/np.sum(array)\n\n# total variation distance to measure how different categorical distributions are\ndef tvd(dist1, dist2):\n    return 0.5*(np.sum(np.abs(dist1 - dist2)))\n\n# get the data associated with physicians\nphys = phys_age_sex_pop[[\"st\", \"gndr\", \"bin_min\", \"NPI\"]].copy()\n\n# get the data associated with the general public\npopulation = phys_age_sex_pop[[\"st\", \"gndr\", \"bin_min\", \"pop\"]].copy()\n\n# add a column to flag whether the data is for phys or not\nphys[\"phy\"] = \"1\"\npopulation[\"phy\"] = \"0\"\n\n# merge the two datasets\nmerged = pd.DataFrame(np.vstack([phys, population]), columns=[\"st\", \"gndr\", \"bin\", \"pop\", \"phy\"]).fillna(0)\n\n# combine all categories into a single column\nmerged[\"cat\"] = merged[\"gndr\"] + merged[\"bin\"].astype(str)\nmerged.drop(columns=[\"gndr\", \"bin\"], inplace=True)\n\n# get a subset for testing\nsubset = merged[merged[\"st\"] == \"AK\"]\n\nsubset\n\n\n\n\n\n\n  \n    \n      \n      st\n      pop\n      phy\n      cat\n    \n  \n  \n    \n      0\n      AK\n      145.0\n      1\n      F30.0\n    \n    \n      1\n      AK\n      168.0\n      1\n      F40.0\n    \n    \n      2\n      AK\n      130.0\n      1\n      F50.0\n    \n    \n      3\n      AK\n      88.0\n      1\n      F60.0\n    \n    \n      4\n      AK\n      17.0\n      1\n      F70.0\n    \n    \n      5\n      AK\n      2.0\n      1\n      F0.0\n    \n    \n      6\n      AK\n      1.0\n      1\n      F0.0\n    \n    \n      7\n      AK\n      139.0\n      1\n      M30.0\n    \n    \n      8\n      AK\n      273.0\n      1\n      M40.0\n    \n    \n      9\n      AK\n      292.0\n      1\n      M50.0\n    \n    \n      10\n      AK\n      196.0\n      1\n      M60.0\n    \n    \n      11\n      AK\n      95.0\n      1\n      M70.0\n    \n    \n      12\n      AK\n      16.0\n      1\n      M80.0\n    \n    \n      13\n      AK\n      2.0\n      1\n      M0.0\n    \n    \n      14\n      AK\n      6.0\n      1\n      M0.0\n    \n    \n      818\n      AK\n      51272.0\n      0\n      F30.0\n    \n    \n      819\n      AK\n      40674.0\n      0\n      F40.0\n    \n    \n      820\n      AK\n      43451.0\n      0\n      F50.0\n    \n    \n      821\n      AK\n      39819.0\n      0\n      F60.0\n    \n    \n      822\n      AK\n      19719.0\n      0\n      F70.0\n    \n    \n      823\n      AK\n      0.0\n      0\n      F0.0\n    \n    \n      824\n      AK\n      0.0\n      0\n      F0.0\n    \n    \n      825\n      AK\n      52633.0\n      0\n      M30.0\n    \n    \n      826\n      AK\n      43363.0\n      0\n      M40.0\n    \n    \n      827\n      AK\n      47407.0\n      0\n      M50.0\n    \n    \n      828\n      AK\n      41558.0\n      0\n      M60.0\n    \n    \n      829\n      AK\n      20398.0\n      0\n      M70.0\n    \n    \n      830\n      AK\n      6421.0\n      0\n      M80.0\n    \n    \n      831\n      AK\n      0.0\n      0\n      M0.0\n    \n    \n      832\n      AK\n      0.0\n      0\n      M0.0\n    \n  \n\n\n\n\n\n\nCode\ndef permutation_test(df, category, classes, repetitions):\n\n    # create a pivot table finding the number of phys/not in each category\n    counts = pd.pivot_table(df, columns=[classes], index=category, aggfunc=np.size)\n    \n    # stop here if data is malformed\n    if \"0\" not in counts.columns or \"1\" not in counts.columns:\n        return (0,0)\n\n    # calculate tvd\n    observed_tvd = tvd(proportions(counts[\"0\"]), proportions(counts[\"1\"]))\n\n    tvds = []\n    # shuffle data x times based on input\n    for i in range(repetitions):\n        # shuffle phys/not label\n        shuffled_var = df[[classes]].sample(frac=1).to_numpy()\n        # add new label to df\n        df[\"shuffled\"] = shuffled_var\n\n        # recalculate pivot based on shuffled labels\n        shuffled_counts = pd.pivot_table(df[[category, \"shuffled\"]], columns=[\"shuffled\"], index=category, aggfunc=np.size)\n\n        # calculate tvd based on shuffled counts\n        new_tvd = tvd(proportions(shuffled_counts[\"0\"]), proportions(shuffled_counts[\"1\"]))\n        \n        # keep track of calculated values\n        tvds.append(new_tvd)\n\n    # calculate p-value\n    tvds = np.array(tvds)\n    emp_p = np.count_nonzero(tvds >= observed_tvd)/repetitions\n\n    return (observed_tvd, emp_p)\n\n\n\n\nCode\n# CAREFUL: takes forever (~hour for all 50 states)\nresults = []\nstates = merged[\"st\"].unique()\nfor name, group in merged.groupby(\"st\"):\n    # format the data so it is one row per person\n    group = group.loc[group.index.repeat(group[\"pop\"])]\n    # run function with relevant columns\n    vals = permutation_test(group[[\"phy\", \"cat\"]], \"cat\", \"phy\", 50)\n    results.append([name, vals[0], vals[1]])\n    print([name, vals[0], vals[1]])\n\nprint(results)\n\n\n/usr/local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._set_item(key, value)\n\n\n['AK', 0.18021522997958397, 0.0]\n['AL', 0.2932136955124203, 0.0]\n['AR', 0.2937022718135871, 0.0]\n['AZ', 0.2606198561672938, 0.0]\n['CA', 0.2347407528633154, 0.0]\n['CO', 0.2288251123980192, 0.0]\n['CT', 0.23527136107987995, 0.0]\n['DC', 0, 0]\n['DE', 0.25029933860194475, 0.0]\n['FL', 0.3029553778100085, 0.0]\n['GA', 0.27281678989061126, 0.0]\n['GU', 0, 0]\n['HI', 0.19974944091922722, 0.0]\n['IA', 0.23998560753313636, 0.0]\n['ID', 0.24630874345728387, 0.0]\n['IL', 0.23441423101332284, 0.0]\n['IN', 0.2697486816663744, 0.0]\n['KS', 0.26336729446962004, 0.0]\n['KY', 0.2722768267897496, 0.0]\n['LA', 0.27347980078737105, 0.0]\n['MA', 0.1978436360546689, 0.0]\n['MD', 0.2289315610239334, 0.0]\n['ME', 0.21749939235094626, 0.0]\n['MI', 0.21914869692714858, 0.0]\n['MN', 0.21875506804397793, 0.0]\n['MO', 0.2580233284315929, 0.0]\n['MP', 0, 0]\n['MS', 0.3189530580657671, 0.0]\n['MT', 0.2821870597873727, 0.0]\n['NC', 0.25963133381598574, 0.0]\n['ND', 0.21149760129583914, 0.0]\n['NE', 0.2530966429205866, 0.0]\n['NH', 0.21119041454015075, 0.0]\n['NJ', 0.24876949889183236, 0.0]\n['NM', 0.2186982696570639, 0.0]\n['NV', 0.2855833315796024, 0.0]\n['NY', 0.24237199347297012, 0.0]\n['OH', 0.2520410830889507, 0.0]\n['OK', 0.2840563221741394, 0.0]\n['OR', 0.24334216904386435, 0.0]\n['PA', 0.25059056617991216, 0.0]\n['PR', 0, 0]\n['RI', 0.18583370830677687, 0.0]\n['SC', 0.27903244780381925, 0.0]\n['SD', 0.23387317242589084, 0.0]\n['TN', 0.2920888997854446, 0.0]\n['TX', 0.2648182719557806, 0.0]\n['UT', 0.30165251189624265, 0.0]\n['VA', 0.25963604454610856, 0.0]\n['VI', 0, 0]\n['VT', 0.17382601049143895, 0.0]\n['WA', 0.2198138521463306, 0.0]\n['WI', 0.2408151134735927, 0.0]\n['WV', 0.25620053560926886, 0.0]\n['WY', 0.29211924877081863, 0.0]\n[['AK', 0.18021522997958397, 0.0], ['AL', 0.2932136955124203, 0.0], ['AR', 0.2937022718135871, 0.0], ['AZ', 0.2606198561672938, 0.0], ['CA', 0.2347407528633154, 0.0], ['CO', 0.2288251123980192, 0.0], ['CT', 0.23527136107987995, 0.0], ['DC', 0, 0], ['DE', 0.25029933860194475, 0.0], ['FL', 0.3029553778100085, 0.0], ['GA', 0.27281678989061126, 0.0], ['GU', 0, 0], ['HI', 0.19974944091922722, 0.0], ['IA', 0.23998560753313636, 0.0], ['ID', 0.24630874345728387, 0.0], ['IL', 0.23441423101332284, 0.0], ['IN', 0.2697486816663744, 0.0], ['KS', 0.26336729446962004, 0.0], ['KY', 0.2722768267897496, 0.0], ['LA', 0.27347980078737105, 0.0], ['MA', 0.1978436360546689, 0.0], ['MD', 0.2289315610239334, 0.0], ['ME', 0.21749939235094626, 0.0], ['MI', 0.21914869692714858, 0.0], ['MN', 0.21875506804397793, 0.0], ['MO', 0.2580233284315929, 0.0], ['MP', 0, 0], ['MS', 0.3189530580657671, 0.0], ['MT', 0.2821870597873727, 0.0], ['NC', 0.25963133381598574, 0.0], ['ND', 0.21149760129583914, 0.0], ['NE', 0.2530966429205866, 0.0], ['NH', 0.21119041454015075, 0.0], ['NJ', 0.24876949889183236, 0.0], ['NM', 0.2186982696570639, 0.0], ['NV', 0.2855833315796024, 0.0], ['NY', 0.24237199347297012, 0.0], ['OH', 0.2520410830889507, 0.0], ['OK', 0.2840563221741394, 0.0], ['OR', 0.24334216904386435, 0.0], ['PA', 0.25059056617991216, 0.0], ['PR', 0, 0], ['RI', 0.18583370830677687, 0.0], ['SC', 0.27903244780381925, 0.0], ['SD', 0.23387317242589084, 0.0], ['TN', 0.2920888997854446, 0.0], ['TX', 0.2648182719557806, 0.0], ['UT', 0.30165251189624265, 0.0], ['VA', 0.25963604454610856, 0.0], ['VI', 0, 0], ['VT', 0.17382601049143895, 0.0], ['WA', 0.2198138521463306, 0.0], ['WI', 0.2408151134735927, 0.0], ['WV', 0.25620053560926886, 0.0], ['WY', 0.29211924877081863, 0.0]]\n\n\n\n\nCode\n# load results into a pandas df\ntvd = pd.DataFrame(results, columns = [\"st\", \"tvd\", \"pval\"])\n\n# add it to the physician data by state\nage_tvd = pd.merge(phys_age_sex_pop, tvd, on=\"st\", how=\"left\")\n\nage_tvd\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      bin_min\n      NPI\n      pop\n      phy_per_10k\n      tvd\n      pval\n    \n  \n  \n    \n      0\n      AK\n      F\n      30.0\n      145\n      51272.0\n      28.280543\n      0.180215\n      0.0\n    \n    \n      1\n      AK\n      F\n      40.0\n      168\n      40674.0\n      41.304027\n      0.180215\n      0.0\n    \n    \n      2\n      AK\n      F\n      50.0\n      130\n      43451.0\n      29.918759\n      0.180215\n      0.0\n    \n    \n      3\n      AK\n      F\n      60.0\n      88\n      39819.0\n      22.100003\n      0.180215\n      0.0\n    \n    \n      4\n      AK\n      F\n      70.0\n      17\n      19719.0\n      8.621127\n      0.180215\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      813\n      WY\n      M\n      50.0\n      250\n      35040.0\n      71.347032\n      0.292119\n      0.0\n    \n    \n      814\n      WY\n      M\n      60.0\n      199\n      37838.0\n      52.592632\n      0.292119\n      0.0\n    \n    \n      815\n      WY\n      M\n      70.0\n      72\n      21138.0\n      34.061879\n      0.292119\n      0.0\n    \n    \n      816\n      WY\n      M\n      80.0\n      8\n      9006.0\n      8.882967\n      0.292119\n      0.0\n    \n    \n      817\n      WY\n      M\n      NaN\n      3\n      NaN\n      NaN\n      0.292119\n      0.0\n    \n  \n\n818 rows × 8 columns\n\n\n\n\n\n\nA key advantage of our data is that we can explicitly identify common ownership and individual places of work. Using data on common ownership – what are called firms, which are roughly equivalent to the concept of “health systems” in the health care arena – we can calculate the level of market concentration for specific geographies.\nCalculation: HHI (or consider CRn), using employed physicians as the measure of market power\nBreakdown by Geography: State, CBSA\nFollow-up question of interest: Market concentration of employeed physicians by certain specialties\n\n\nCode\n# merge physician data with establishment data\nphys_estab = physicians.merge(phys2estab[[\"NPI\", \"establishment_id\"]], on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')[[\"NPI\", \"st\", \"org_pac_id\"]]\n\n# add practice and then firm data\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\nphys_firm = phys_prac.merge(firms, on=\"health_sys_id\", how=\"left\")\n\n# get relevant columns for measuring market concentration\nphys_firm = phys_firm[[\"NPI\", \"health_sys_name\", \"health_sys_state\"]]\n\n\n\n\nCode\ndef sdi(data):\n    def p(n, N):\n        \"\"\" Relative abundance \"\"\"\n        if n == 0:\n            return 0\n        else:\n            return float(n)/N\n    N = sum(data)\n    return 1 - (sum(p(n, N)**2 for n in data if n != 0))\n\n# get count of physicians per health system and state\nphys_firm_cts = phys_firm.groupby([\"health_sys_name\", \"health_sys_state\"]).NPI.count().reset_index()\n\n# calculate diversity of state based on Simpson's Index\nphy_diversity = phys_firm_cts.groupby(\"health_sys_state\")[\"NPI\"].apply(list).apply(sdi).reset_index(name=\"sdi\")\n\n\nphy_diversity\n\n\n\n\n\n\nOur data positions us to contribute new insights on industry consolidation. In this research question, we look at whether there are patterns in business consolidation across states and whether there are certain pairings of states that seem to be particularly well integrated.\nBreakdown by Geography: State, possibly CBSA\nMain challenge is to identify a proper metric (for instance, having a certain minimum number of physicians in another state before that other state is counted, possibly factoring in state size, etc.). May also want to consider ways to explicitly account for geographic proximity (such as neighboring states), including to assist with data presentation.\n\n\nCode\n#TODO\n# find pairings of states based on whether physicians work in both states\n# wordclouds?\n# correlation?\n\n# merge physician data with establishment data\nphys_estab = physicians.merge(phys2estab[[\"NPI\", \"establishment_id\"]], on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left').drop_duplicates()\n\nphys_sts = phys_estab[[\"NPI\", \"st\"]]\n\n\n\n\nCode\nphys_sts_uniq = phys_sts.groupby(\"NPI\").st.unique().reset_index(name=\"states\")\n\nphys_sts_uniq[\"multi\"] = np.where(len(phys_sts_uniq[\"states\"]) > 1, 1, 0)\n\nphys_sts_uniq[phys_sts_uniq[\"multi\"] > 1]\n\n\n\n\n\n\n  \n    \n      \n      NPI\n      states\n      multi\n    \n  \n  \n  \n\n\n\n\n\n\n\nOur data provides unique insights on the multiple sources of income received by physicians. From the public data, we do not have access to earnings information, to information on business activities outside of clinical care, and are not able to break down any metrics of business activity (like Medicare charges) by source of work. Instead, we will need to focus on the mix of organization types with whom the physician is affiliated.\nIn effect, this becomes a table showing the share of physicians that work for one type of organization that also work for another type of organization.\nBreakdown by Characteristics: Age (young/old), Specialty Category\nBreakdown by Geography: Nation, State, CBSA\n\n\nCode\n# merge physician data with establishment data\nphys_estab = physicians.merge(phys2estab[[\"NPI\", \"establishment_id\"]], on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left').drop_duplicates()\n\n# add practice information\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\n\n# get relevant columns\nphys_base = phys_prac[[\"NPI\", \"st\", \"org_pac_id\", \"practice_scope\", \"health_sys_id\"]].copy().drop_duplicates()\n\n# make a column for whether practice is linked to health system\nphys_base[\"health_sys\"] = np.where(phys_base[\"health_sys_id\"].isnull(), 0, 1)\n\nphys_base\n\n\n\n\n\n\n  \n    \n      \n      NPI\n      st\n      org_pac_id\n      practice_scope\n      health_sys_id\n      health_sys\n    \n  \n  \n    \n      0\n      1003000126\n      VA\n      4587979323\n      multi-unit, single-state\n      HSI00000492\n      1\n    \n    \n      1\n      1003000134\n      IL\n      2163334699\n      multi-unit, single-cbsa\n      HSI00000756\n      1\n    \n    \n      9\n      1003000142\n      OH\n      2668522400\n      multi-unit, multi-state\n      HSI00000661\n      1\n    \n    \n      13\n      1003000407\n      PA\n      3072579598\n      single-unit\n      HSI00000819\n      1\n    \n    \n      14\n      1003000423\n      OH\n      5799794350\n      multi-unit, single-cbsa\n      NaN\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1203604\n      1992998736\n      FL\n      9032361274\n      single-unit\n      NaN\n      0\n    \n    \n      1203605\n      1992999031\n      CA\n      8325934482\n      multi-unit, single-state\n      NaN\n      0\n    \n    \n      1203606\n      1992999122\n      FL\n      91992999122\n      NaN\n      NaN\n      0\n    \n    \n      1203608\n      1992999551\n      CA\n      9335050517\n      multi-unit, single-state\n      NaN\n      0\n    \n    \n      1203609\n      1992999825\n      WA\n      9830002617\n      multi-unit, single-state\n      HSI00001232\n      1\n    \n  \n\n678976 rows × 6 columns\n\n\n\n\n\nCode\ngrp_df = phys_base.groupby([\"NPI\", \"st\"]).agg(\n    orgs_uniq = (\"org_pac_id\", \"nunique\"),\n    scopes_uniq = (\"practice_scope\", \"nunique\"),\n    health_sys_uniq = (\"health_sys_id\", \"nunique\")).reset_index()\n\n\nphys_char = pd.merge(phys_estab[[\"NPI\", \"st\", \"age_imputed\"]].drop_duplicates(), grp_df, on=[\"NPI\", \"st\"], how=\"left\")\n\n# make a column for old/young\n# cutoff based on average age: https://www.definitivehc.com/resources/healthcare-insights/average-provider-age-medical-specialty\nphys_char[\"old\"] = np.where(phys_char[\"age_imputed\"] > 53, 1, 0)\n\n# topcode num orgs and scopes\nphys_char[\"scopes_uniq_top\"] = np.where(phys_char[\"scopes_uniq\"] > 3, 3, phys_char[\"scopes_uniq\"])\nphys_char[\"orgs_uniq_top\"] = np.where(phys_char[\"orgs_uniq\"] > 3, 3, phys_char[\"orgs_uniq\"])\n\nphys_char = phys_char[[\"NPI\", \"st\", \"orgs_uniq_top\", \"scopes_uniq_top\", \"health_sys_uniq\", \"old\"]]\nphys_char\n\n\n\n\n\n\n  \n    \n      \n      NPI\n      st\n      orgs_uniq_top\n      scopes_uniq_top\n      health_sys_uniq\n      old\n    \n  \n  \n    \n      0\n      1003000126\n      VA\n      1\n      1\n      1\n      1\n    \n    \n      1\n      1003000134\n      IL\n      1\n      1\n      1\n      0\n    \n    \n      2\n      1003000142\n      OH\n      1\n      1\n      1\n      0\n    \n    \n      3\n      1003000407\n      PA\n      1\n      1\n      1\n      0\n    \n    \n      4\n      1003000423\n      OH\n      1\n      1\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      587391\n      1992998736\n      FL\n      1\n      1\n      0\n      0\n    \n    \n      587392\n      1992999031\n      CA\n      1\n      1\n      0\n      0\n    \n    \n      587393\n      1992999122\n      FL\n      1\n      0\n      0\n      1\n    \n    \n      587394\n      1992999551\n      CA\n      1\n      1\n      0\n      0\n    \n    \n      587395\n      1992999825\n      WA\n      1\n      1\n      1\n      0\n    \n  \n\n587396 rows × 6 columns\n\n\n\n\n\nCode\n# for each state, look at how many physicians work 1, 2, or 3+ work scopes, broken down by age\npd.pivot_table(phys_char, \n               index=[\"st\", \"old\"], \n               columns=[\"scopes_uniq_top\"], \n               values=\"NPI\", \n               aggfunc=np.size).apply(lambda r: r/r.sum(), axis=1)\n\n\n\n\n\n\n  \n    \n      \n      scopes_uniq_top\n      0\n      1\n      2\n      3\n    \n    \n      st\n      old\n      \n      \n      \n      \n    \n  \n  \n    \n      AK\n      0\n      0.072043\n      0.874194\n      0.050538\n      0.003226\n    \n    \n      1\n      0.206250\n      0.746875\n      0.040625\n      0.006250\n    \n    \n      AL\n      0\n      0.106004\n      0.837141\n      0.053932\n      0.002922\n    \n    \n      1\n      0.228238\n      0.733161\n      0.037047\n      0.001554\n    \n    \n      AR\n      0\n      0.083493\n      0.851015\n      0.062428\n      0.003064\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      WI\n      1\n      0.053662\n      0.851252\n      0.086989\n      0.008096\n    \n    \n      WV\n      0\n      0.075717\n      0.869659\n      0.047593\n      0.007031\n    \n    \n      1\n      0.230314\n      0.728242\n      0.034932\n      0.006513\n    \n    \n      WY\n      0\n      0.076023\n      0.875731\n      0.048246\n      NaN\n    \n    \n      1\n      0.183071\n      0.791339\n      0.025591\n      NaN\n    \n  \n\n110 rows × 4 columns\n\n\n\n\n\nCode\n# do the same but for orgs\npd.pivot_table(phys_char, \n               index=[\"st\", \"old\"], \n               columns=[\"orgs_uniq_top\"], \n               values=\"NPI\", \n               aggfunc=np.size).apply(lambda r: r/r.sum(), axis=1)\n\n\n\n\n\n\n  \n    \n      \n      orgs_uniq_top\n      1\n      2\n      3\n    \n    \n      st\n      old\n      \n      \n      \n    \n  \n  \n    \n      AK\n      0\n      0.880645\n      0.096774\n      0.022581\n    \n    \n      1\n      0.882812\n      0.087500\n      0.029687\n    \n    \n      AL\n      0\n      0.875399\n      0.100425\n      0.024176\n    \n    \n      1\n      0.888860\n      0.091969\n      0.019171\n    \n    \n      AR\n      0\n      0.893910\n      0.098430\n      0.007660\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      WI\n      1\n      0.811523\n      0.148936\n      0.039541\n    \n    \n      WV\n      0\n      0.909681\n      0.077880\n      0.012439\n    \n    \n      1\n      0.907046\n      0.079337\n      0.013618\n    \n    \n      WY\n      0\n      0.907895\n      0.084795\n      0.007310\n    \n    \n      1\n      0.925197\n      0.061024\n      0.013780\n    \n  \n\n110 rows × 3 columns\n\n\n\n\n\n\nOur data provides unique insights on the incomes (and sources of income) of physicians. While we cannot measure income in public datasets, we can use Medicare Charges as a stand-in. The goal of this research question is to show which factors are associated with higher or lower incomes.\nBreakdown by Characteristics: Age, Specialty Category, Gender, Organization Type, Geography(State)\n\n\nCode\n# merge physician data with establishment data\nphys_estab = physicians.merge(phys2estab[[\"NPI\", \"establishment_id\"]], on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')[[\"NPI\", \"st\", \"gndr\", \"pri_spec_category\", \"age_imputed\", \"medicare_charges\", \"org_pac_id\"]]\n\n# add practice and then firm data\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\n\nphys_inc = phys_prac[[\"NPI\", \"st\", \"gndr\", \"pri_spec_category\", \"age_imputed\", \"practice_scope\", \"practice_size\", \"medicare_charges\"]].drop_duplicates()\n\nphys_inc\n\n\n\n\n\n\n  \n    \n      \n      NPI\n      st\n      gndr\n      pri_spec_category\n      age_imputed\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      1003000126\n      VA\n      M\n      Primary care\n      54.0\n      multi-unit, single-state\n      large (20-99)\n      692640.00\n    \n    \n      1\n      1003000134\n      IL\n      M\n      Hospital based\n      45.0\n      multi-unit, single-cbsa\n      large (20-99)\n      1143597.00\n    \n    \n      9\n      1003000142\n      OH\n      M\n      Hospital based\n      49.0\n      multi-unit, multi-state\n      large (20-99)\n      261064.00\n    \n    \n      13\n      1003000407\n      PA\n      M\n      Primary care\n      45.0\n      single-unit\n      medium (6-19)\n      91196.00\n    \n    \n      14\n      1003000423\n      OH\n      F\n      OBGYN\n      41.0\n      multi-unit, single-cbsa\n      medium (6-19)\n      16773.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1203604\n      1992998736\n      FL\n      M\n      Primary care\n      41.0\n      single-unit\n      small (2-5)\n      124013.80\n    \n    \n      1203605\n      1992999031\n      CA\n      M\n      Medical specialty\n      42.0\n      multi-unit, single-state\n      large (20-99)\n      38362.21\n    \n    \n      1203606\n      1992999122\n      FL\n      M\n      Primary care\n      65.0\n      NaN\n      NaN\n      853391.00\n    \n    \n      1203608\n      1992999551\n      CA\n      F\n      Primary care\n      44.0\n      multi-unit, single-state\n      large (20-99)\n      181414.40\n    \n    \n      1203609\n      1992999825\n      WA\n      M\n      Surgery specialty\n      43.0\n      multi-unit, single-state\n      large (20-99)\n      310475.50\n    \n  \n\n650181 rows × 8 columns\n\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFECV\nX = phys_inc[[\"st\", \"gndr\", \"pri_spec_category\", \"age_imputed\", \"practice_scope\", \"practice_size\"]]\ny = phys_inc[\"medicare_charges\"]\n\nX = pd.get_dummies(X, columns=[\"st\", \"gndr\", \"pri_spec_category\", \"practice_scope\", \"practice_size\"]).fillna(0)\n\nreg = LinearRegression()\nselector = RFECV(reg, step=1, cv=5)\nselector = selector.fit(X, y)\n\nselector.score(X, y)\n\n\n/usr/local/lib/python3.9/site-packages/sklearn/base.py:445: UserWarning: X does not have valid feature names, but RFECV was fitted with feature names\n  warnings.warn(\n\n\n0.07898071714042043\n\n\n\n\nCode\nfrom sklearn.feature_selection import f_regression\nfstat, pval = f_regression(X, y)\nidxs = np.argsort(fstat)\n\ntop_preds = X.columns[idxs]\ntop_preds\n\n\nIndex(['practice_scope_multi-unit, single-cbsa', 'st_NC', 'st_IN',\n       'practice_size_sole practitioner', 'st_CA', 'st_AK', 'st_NH', 'st_NY',\n       'st_IA', 'st_VI', 'st_KY', 'st_NE', 'st_LA', 'st_MP', 'st_OK', 'st_AL',\n       'st_SD', 'st_WY', 'st_MO', 'st_WI', 'st_GU', 'st_WV', 'st_MD',\n       'practice_scope_multi-unit, multi-state', 'st_NM', 'st_ND', 'st_MT',\n       'st_KS', 'st_DC', 'st_CT', 'st_TN', 'st_VA', 'st_ID', 'st_AZ', 'st_GA',\n       'st_VT', 'st_IL', 'st_DE', 'st_AR', 'st_RI', 'st_CO',\n       'practice_scope_single-unit', 'st_UT', 'st_ME', 'st_HI', 'st_OR',\n       'st_NV', 'age_imputed', 'st_SC', 'st_MA',\n       'practice_scope_multi-unit, single-state', 'st_MS', 'st_WA', 'st_TX',\n       'st_OH', 'st_PA', 'st_MI', 'practice_size_small (2-5)', 'st_NJ',\n       'st_PR', 'st_MN', 'st_FL', 'pri_spec_category_Hospital based',\n       'practice_size_medium (6-19)', 'practice_size_large (20-99)',\n       'pri_spec_category_Psychiatry', 'pri_spec_category_OBGYN',\n       'pri_spec_category_Surgery specialty', 'gndr_F', 'gndr_M',\n       'pri_spec_category_Medical specialty',\n       'pri_spec_category_Primary care'],\n      dtype='object')\n\n\n\n\n\nProponents of business consolidation often argue that larger firms can provide greater care coordination. While care coordination is difficult to measure directly, we can look at simple proxies like the number of colleagues a physician has and the diversity of the specialties held by those colleagues.\nBreakdown by Characteristics: Specialty Category, Firm vs. Practice\nBreakdown by Geography: Nation, State, CBSA\n\n\nCode\n# merge physician data with establishment data\nphys_estab = physicians.merge(phys2estab[[\"NPI\", \"establishment_id\"]], on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')[[\"NPI\", \"st\", \"gndr\", \"pri_spec_category\", \"org_pac_id\"]]\n\n# collect physicians who work for each org\nphys_org = phys_estab.groupby([\"org_pac_id\", \"st\"]).agg(\n    phys = (\"NPI\", \"nunique\"),\n    spec_ct = (\"pri_spec_category\", \"nunique\"),\n    specs = (\"pri_spec_category\", \"unique\")).reset_index()\n\n# # self join to find physicians who work with eachother\n# phys_pairs = pd.merge(phys_estab, phys_org, on=\"org_pac_id\", suffixes=(\"_prim\", \"_sec\"))\n\n\n\n\nCode\n# bin the number of physicians at each org\nphys_org[\"phys_bins\"] = pd.cut(phys_org[\"phys\"], [1, 2, 3, 4, 5, 10, float(\"inf\")], right=False)\n\n# find the number of organizations with a certain number of physicians in each state\npd.crosstab(phys_org.st, phys_org.phys_bins).apply(lambda r: r/r.sum(), axis=1)\n\n\n\n\n\n\n  \n    \n      phys_bins\n      [1.0, 2.0)\n      [2.0, 3.0)\n      [3.0, 4.0)\n      [4.0, 5.0)\n      [5.0, 10.0)\n      [10.0, inf)\n    \n    \n      st\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      AK\n      0.583569\n      0.101983\n      0.079320\n      0.050992\n      0.087819\n      0.096317\n    \n    \n      AL\n      0.586597\n      0.144038\n      0.063969\n      0.052654\n      0.092689\n      0.060052\n    \n    \n      AR\n      0.622782\n      0.113912\n      0.046685\n      0.042017\n      0.096172\n      0.078431\n    \n    \n      AZ\n      0.667422\n      0.105700\n      0.048698\n      0.040015\n      0.058890\n      0.079275\n    \n    \n      CA\n      0.765214\n      0.083118\n      0.038153\n      0.024532\n      0.045154\n      0.043829\n    \n    \n      CO\n      0.559729\n      0.118842\n      0.068966\n      0.044335\n      0.096059\n      0.112069\n    \n    \n      CT\n      0.634097\n      0.111860\n      0.047170\n      0.047844\n      0.077493\n      0.081536\n    \n    \n      DC\n      0.662162\n      0.102703\n      0.056757\n      0.027027\n      0.048649\n      0.102703\n    \n    \n      DE\n      0.623853\n      0.148624\n      0.044037\n      0.027523\n      0.097248\n      0.058716\n    \n    \n      FL\n      0.694813\n      0.109497\n      0.045729\n      0.028764\n      0.060940\n      0.060257\n    \n    \n      GA\n      0.607378\n      0.121247\n      0.064055\n      0.044038\n      0.087790\n      0.075493\n    \n    \n      GU\n      0.673469\n      0.122449\n      0.020408\n      0.020408\n      0.040816\n      0.122449\n    \n    \n      HI\n      0.779167\n      0.084375\n      0.040625\n      0.019792\n      0.035417\n      0.040625\n    \n    \n      IA\n      0.440459\n      0.120516\n      0.064562\n      0.050215\n      0.149211\n      0.175036\n    \n    \n      ID\n      0.588737\n      0.122867\n      0.063140\n      0.037543\n      0.104096\n      0.083618\n    \n    \n      IL\n      0.637460\n      0.116469\n      0.054898\n      0.041981\n      0.070183\n      0.079010\n    \n    \n      IN\n      0.592801\n      0.097300\n      0.066367\n      0.030934\n      0.085489\n      0.127109\n    \n    \n      KS\n      0.534286\n      0.133333\n      0.063810\n      0.059048\n      0.100952\n      0.108571\n    \n    \n      KY\n      0.594558\n      0.125850\n      0.068027\n      0.036054\n      0.081633\n      0.093878\n    \n    \n      LA\n      0.644749\n      0.110502\n      0.052511\n      0.042466\n      0.089041\n      0.060731\n    \n    \n      MA\n      0.621420\n      0.094155\n      0.053747\n      0.036877\n      0.096116\n      0.097685\n    \n    \n      MD\n      0.665023\n      0.117647\n      0.052836\n      0.034167\n      0.058471\n      0.071856\n    \n    \n      ME\n      0.591837\n      0.104592\n      0.061224\n      0.028061\n      0.076531\n      0.137755\n    \n    \n      MI\n      0.635350\n      0.120564\n      0.059600\n      0.035032\n      0.076206\n      0.073248\n    \n    \n      MN\n      0.409091\n      0.126959\n      0.065831\n      0.048589\n      0.122257\n      0.227273\n    \n    \n      MO\n      0.569022\n      0.107065\n      0.061413\n      0.047826\n      0.104891\n      0.109783\n    \n    \n      MP\n      0.428571\n      0.285714\n      0.142857\n      0.000000\n      0.000000\n      0.142857\n    \n    \n      MS\n      0.556424\n      0.138889\n      0.074653\n      0.049479\n      0.093750\n      0.086806\n    \n    \n      MT\n      0.573620\n      0.107362\n      0.030675\n      0.027607\n      0.113497\n      0.147239\n    \n    \n      NC\n      0.586871\n      0.138365\n      0.059355\n      0.036164\n      0.077437\n      0.101808\n    \n    \n      ND\n      0.482993\n      0.176871\n      0.081633\n      0.061224\n      0.108844\n      0.088435\n    \n    \n      NE\n      0.460133\n      0.171096\n      0.064784\n      0.073090\n      0.129568\n      0.101329\n    \n    \n      NH\n      0.473973\n      0.104110\n      0.054795\n      0.060274\n      0.131507\n      0.175342\n    \n    \n      NJ\n      0.680589\n      0.113660\n      0.052208\n      0.030469\n      0.065046\n      0.058028\n    \n    \n      NM\n      0.601942\n      0.129450\n      0.055016\n      0.048544\n      0.076052\n      0.088997\n    \n    \n      NV\n      0.672844\n      0.114424\n      0.049960\n      0.029815\n      0.066076\n      0.066882\n    \n    \n      NY\n      0.726885\n      0.091162\n      0.044607\n      0.028656\n      0.053510\n      0.055179\n    \n    \n      OH\n      0.594944\n      0.121693\n      0.064668\n      0.036449\n      0.089359\n      0.092887\n    \n    \n      OK\n      0.674691\n      0.097662\n      0.051582\n      0.026823\n      0.074278\n      0.074966\n    \n    \n      OR\n      0.555352\n      0.114364\n      0.060384\n      0.050320\n      0.089661\n      0.129918\n    \n    \n      PA\n      0.610251\n      0.118679\n      0.064692\n      0.044191\n      0.087472\n      0.074715\n    \n    \n      PR\n      0.928856\n      0.032199\n      0.012879\n      0.008280\n      0.013186\n      0.004600\n    \n    \n      RI\n      0.660118\n      0.090373\n      0.051081\n      0.035363\n      0.072692\n      0.090373\n    \n    \n      SC\n      0.572656\n      0.109520\n      0.068003\n      0.050823\n      0.095920\n      0.103078\n    \n    \n      SD\n      0.476684\n      0.124352\n      0.051813\n      0.067358\n      0.088083\n      0.191710\n    \n    \n      TN\n      0.639421\n      0.110260\n      0.050660\n      0.038314\n      0.068114\n      0.093231\n    \n    \n      TX\n      0.716031\n      0.101145\n      0.042939\n      0.029103\n      0.059160\n      0.051622\n    \n    \n      UT\n      0.594761\n      0.141757\n      0.063174\n      0.046225\n      0.070878\n      0.083205\n    \n    \n      VA\n      0.597701\n      0.118391\n      0.061303\n      0.043295\n      0.088506\n      0.090805\n    \n    \n      VI\n      0.813333\n      0.080000\n      0.026667\n      0.040000\n      0.040000\n      0.000000\n    \n    \n      VT\n      0.628821\n      0.096070\n      0.052402\n      0.052402\n      0.078603\n      0.091703\n    \n    \n      WA\n      0.588682\n      0.102830\n      0.060732\n      0.039337\n      0.084196\n      0.124224\n    \n    \n      WI\n      0.471316\n      0.113611\n      0.055118\n      0.065242\n      0.107987\n      0.186727\n    \n    \n      WV\n      0.685012\n      0.103044\n      0.051522\n      0.025761\n      0.063232\n      0.071429\n    \n    \n      WY\n      0.544554\n      0.135314\n      0.069307\n      0.049505\n      0.108911\n      0.092409\n    \n  \n\n\n\n\n\n\nCode\n# bin the number of specialties\n# probably unnecessary since there are only 6 possibilities\nphys_org[\"spec_bins\"] = pd.cut(phys_org[\"spec_ct\"], [1, 2, 3, 4, 5, 6], right=False)\n\n# find the number of organizations with each number of specialties\npd.crosstab(phys_org.st, phys_org.spec_bins).apply(lambda r: r/r.sum(), axis=1)\n\n\n\n\n\n\n  \n    \n      spec_bins\n      [1, 2)\n      [2, 3)\n      [3, 4)\n      [4, 5)\n      [5, 6)\n    \n    \n      st\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      AK\n      0.860000\n      0.085714\n      0.025714\n      0.008571\n      0.020000\n    \n    \n      AL\n      0.871895\n      0.088017\n      0.021351\n      0.014379\n      0.004357\n    \n    \n      AR\n      0.881020\n      0.074599\n      0.024551\n      0.011331\n      0.008499\n    \n    \n      AZ\n      0.908849\n      0.061527\n      0.017850\n      0.006457\n      0.005317\n    \n    \n      CA\n      0.935370\n      0.047366\n      0.010940\n      0.003352\n      0.002972\n    \n    \n      CO\n      0.890199\n      0.071960\n      0.022953\n      0.006824\n      0.008065\n    \n    \n      CT\n      0.905506\n      0.065942\n      0.014956\n      0.006118\n      0.007478\n    \n    \n      DC\n      0.901099\n      0.057692\n      0.013736\n      0.016484\n      0.010989\n    \n    \n      DE\n      0.924494\n      0.049724\n      0.018416\n      0.003683\n      0.003683\n    \n    \n      FL\n      0.909899\n      0.067038\n      0.013193\n      0.006450\n      0.003420\n    \n    \n      GA\n      0.883467\n      0.086682\n      0.018083\n      0.006602\n      0.005166\n    \n    \n      GU\n      0.836735\n      0.061224\n      0.040816\n      0.020408\n      0.040816\n    \n    \n      HI\n      0.952929\n      0.031381\n      0.007322\n      0.005230\n      0.003138\n    \n    \n      IA\n      0.780523\n      0.100291\n      0.049419\n      0.045058\n      0.024709\n    \n    \n      ID\n      0.879518\n      0.068847\n      0.022375\n      0.020654\n      0.008606\n    \n    \n      IL\n      0.894051\n      0.067955\n      0.021059\n      0.009553\n      0.007382\n    \n    \n      IN\n      0.867689\n      0.070415\n      0.030097\n      0.015900\n      0.015900\n    \n    \n      KS\n      0.862069\n      0.088123\n      0.020115\n      0.015326\n      0.014368\n    \n    \n      KY\n      0.871742\n      0.086420\n      0.016461\n      0.015089\n      0.010288\n    \n    \n      LA\n      0.889093\n      0.070119\n      0.021998\n      0.011457\n      0.007333\n    \n    \n      MA\n      0.885408\n      0.071372\n      0.022205\n      0.009913\n      0.011102\n    \n    \n      MD\n      0.902086\n      0.065394\n      0.018735\n      0.004949\n      0.008837\n    \n    \n      ME\n      0.859008\n      0.060052\n      0.033943\n      0.023499\n      0.023499\n    \n    \n      MI\n      0.888253\n      0.070758\n      0.023357\n      0.009847\n      0.007786\n    \n    \n      MN\n      0.772876\n      0.122549\n      0.044118\n      0.034314\n      0.026144\n    \n    \n      MO\n      0.857930\n      0.082048\n      0.029185\n      0.015969\n      0.014868\n    \n    \n      MP\n      0.714286\n      0.142857\n      0.000000\n      0.000000\n      0.142857\n    \n    \n      MS\n      0.860017\n      0.089239\n      0.029746\n      0.013123\n      0.007874\n    \n    \n      MT\n      0.853125\n      0.062500\n      0.043750\n      0.021875\n      0.018750\n    \n    \n      NC\n      0.884478\n      0.071854\n      0.022628\n      0.011116\n      0.009925\n    \n    \n      ND\n      0.751773\n      0.163121\n      0.049645\n      0.014184\n      0.021277\n    \n    \n      NE\n      0.855946\n      0.090452\n      0.030151\n      0.011725\n      0.011725\n    \n    \n      NH\n      0.845506\n      0.075843\n      0.036517\n      0.030899\n      0.011236\n    \n    \n      NJ\n      0.900429\n      0.077597\n      0.012876\n      0.004807\n      0.004292\n    \n    \n      NM\n      0.844517\n      0.103110\n      0.022913\n      0.018003\n      0.011457\n    \n    \n      NV\n      0.909605\n      0.062954\n      0.016949\n      0.009685\n      0.000807\n    \n    \n      NY\n      0.901690\n      0.066194\n      0.017832\n      0.007376\n      0.006909\n    \n    \n      OH\n      0.873850\n      0.077471\n      0.025824\n      0.011873\n      0.010982\n    \n    \n      OK\n      0.886806\n      0.070139\n      0.021528\n      0.008333\n      0.013194\n    \n    \n      OR\n      0.865438\n      0.061751\n      0.034101\n      0.023963\n      0.014747\n    \n    \n      PA\n      0.897218\n      0.070591\n      0.017015\n      0.008048\n      0.007128\n    \n    \n      PR\n      0.981288\n      0.014724\n      0.001840\n      0.001227\n      0.000920\n    \n    \n      RI\n      0.906375\n      0.049801\n      0.031873\n      0.009960\n      0.001992\n    \n    \n      SC\n      0.862518\n      0.087554\n      0.031114\n      0.013025\n      0.005789\n    \n    \n      SD\n      0.835979\n      0.068783\n      0.037037\n      0.026455\n      0.031746\n    \n    \n      TN\n      0.886510\n      0.074946\n      0.022270\n      0.008565\n      0.007709\n    \n    \n      TX\n      0.925653\n      0.051574\n      0.013683\n      0.004880\n      0.004210\n    \n    \n      UT\n      0.879257\n      0.082043\n      0.021672\n      0.006192\n      0.010836\n    \n    \n      VA\n      0.894981\n      0.075676\n      0.017375\n      0.005405\n      0.006564\n    \n    \n      VI\n      0.946667\n      0.026667\n      0.013333\n      0.013333\n      0.000000\n    \n    \n      VT\n      0.858407\n      0.070796\n      0.026549\n      0.017699\n      0.026549\n    \n    \n      WA\n      0.896697\n      0.057625\n      0.021082\n      0.017569\n      0.007027\n    \n    \n      WI\n      0.832558\n      0.082558\n      0.038372\n      0.017442\n      0.029070\n    \n    \n      WV\n      0.863905\n      0.086391\n      0.026036\n      0.011834\n      0.011834\n    \n    \n      WY\n      0.834437\n      0.076159\n      0.039735\n      0.033113\n      0.016556\n    \n  \n\n\n\n\n\n\nCode\n# filter for orgs with more than one specialty\nspec_groups = phys_org[phys_org[\"specs\"].str.len() > 1]\n\n# get each combination of specialties seen in the data\nimport itertools\ncombs = spec_groups[\"specs\"].apply(lambda x: list(itertools.combinations(sorted(x), 2)))\n\n# find the combinations that occur most often\ncombs.explode().value_counts()\n\n\n(Medical specialty, Primary care)         6271\n(Hospital based, Primary care)            5601\n(Hospital based, Medical specialty)       4440\n(Primary care, Surgery specialty)         3954\n(Hospital based, Surgery specialty)       3812\n(Medical specialty, Surgery specialty)    3546\n(OBGYN, Primary care)                     2319\n(OBGYN, Surgery specialty)                1929\n(Medical specialty, OBGYN)                1885\n(Primary care, Psychiatry)                1744\n(Hospital based, OBGYN)                   1705\n(Medical specialty, Psychiatry)           1373\n(Hospital based, Psychiatry)              1228\n(Psychiatry, Surgery specialty)           1180\n(OBGYN, Psychiatry)                        933\nName: specs, dtype: int64"
  },
  {
    "objectID": "Examples - DP.html",
    "href": "Examples - DP.html",
    "title": "Privacy Models for E-Health",
    "section": "",
    "text": "CDF Summer Project 2022\n\n\n\n\nCode\n# load data science libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_dir = \"\"\n\n\n\n\nCode\nphysicians = pd.read_csv(data_dir + \"physicians.csv\")\nestablishments = pd.read_csv(data_dir + \"establishments.csv\", dtype={'zip': str, 'zip9': str, 'zip3': str})\npractices = pd.read_csv(data_dir + \"practices.csv\")\nfirms = pd.read_csv(data_dir + \"firms.csv\")\nphys2estab = pd.read_csv(data_dir + \"rel_phys2estab.csv\")\nphys2prac = pd.read_csv(data_dir + \"rel_phys2prac.csv\")\ncbsa = pd.read_csv(data_dir + \"geo_cbsas.csv\")\nstates = pd.read_csv(data_dir + \"geo_state.csv\")\nstate_agesex = pd.read_csv(data_dir + \"geo_state_agesex.csv\")\nstate_abr = pd.read_csv(data_dir + \"state_abr.csv\", names=[\"name\", \"abr\"])\n\n\n\n\nCode\nstate_agesex = pd.read_csv(data_dir + \"state_agesex.csv\")\n\n\n\n\n\nSome notation and ideas needed for understanding the DP implementation, most of which come from this paper:\nD is a database with k variables (A1, …, Ak)\nD’ is database D that differs from D by one record\nqv(D) is a marginal query that has the attributes v (certain values of A1, …, Ak)\nEach unique combination of attributes is called a cell\nThe sensitivity of a query (∆q) is the difference between q(D) and q(D’). The sensitivity of count queries is 1.\nParallel composition: for a set of disjoint subsets of D, if each query on the subsets are ε-differentially private, then the results of the queries are also e-differentially private\nSequential composition: Let M and B be ε1- and ε2-differentially private algorithms. Releasing the outputs of M(D) and B(D) on the same database D results in (ε1 + ε2)-differential privacy\nRobustness to post-processing: For any deterministic or randomized function F defined over the image of the mechanism A, if A satisfies ε-differential privacy, so does F(A).\n\n\nq(D) is a query on a database D\nη ∼ Geo(X, p) − Geo(Y, p) is a random variable drawn from the distribution generated from the difference of two random variables (X, Y)\n(X, Y) are distributed according to the geometric distribution, where p = 1 − e-ε.\nq*(D) = q(D) + η satisfies ε-differential privacy, where ηd is a vector of d independently drawn Geometric random variables\n\n\n\nLocal sensitivity = sensitivity of a single cell\nGlobal sensitivity = sensitivity from any theoretical change to the entire dataset\nMaximum observed sensitivity (MOS) = largest local sensitivity across all cells\nUsing the local sensitvity of each cell in a DP calculation would be a privacy issue because the local sensitivity reveals information about the data contained in the cell\nInstead, every cell injects noise proportional to the MOS (as a parameter for a Laplace distribution)\n\n\n\n\n\n\nFunction used in place of groupby + count()\nTakes in the columns you want to group by and the column you want to count and returns a differentially private count based on the given epsilon.\nAlso returns the RMSE between the true counts and the private counts\n\n\n\nProbably redundant\nCan be used in the agg clause of a function.\nTakes in the epsilon and returns a differentially private count based on the epsilon\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\ndef diff_count(data, grouping_columns, target_column, epsilon):\n    \"\"\"Create differentially private count of cells based on input columns\"\"\"\n    # copy input df so we can change it\n    df = data.copy()\n    # get true group counts\n    group_counts = df.groupby(grouping_columns)[target_column].count().reset_index(name=\"true_count\")\n\n    # add discrete Laplacian noise to group counts to get noisy group counts\n    # noise function from here: https://randorithms.com/2020/10/09/geometric-mechanism.html\n    p = 1 - np.exp(-epsilon)\n    noise = np.random.geometric(p, size=len(group_counts)) - np.random.geometric(p, size=len(group_counts))\n    group_counts[target_column] = group_counts[\"true_count\"] + noise\n    \n    err = mean_squared_error(group_counts[\"true_count\"], group_counts[target_column])\n    \n    group_counts.drop(columns=[\"true_count\"], inplace=True)\n\n    return group_counts, err\n\ndef dp_agg(data, epsilon):\n    # add discrete Laplacian noise to group counts to get noisy group counts\n    # noise function from here: https://randorithms.com/2020/10/09/geometric-mechanism.html\n    p = 1 - np.exp(-epsilon)\n    noise = np.random.geometric(p) - np.random.geometric(p)\n    \n    return len(data) + noise\n\n\n\n\n\n\nWhile this research question can be answered using existing public data (including Census ACS), it’s included here as an example of a simple aggregate table that can be generated at different geographic levels. Note that the data above already loads population counts for cbsa and state-level statistics.\nBreakdown by Characteristics: Specialty Category\nBreakdown by Geography: Nation, State, CBSA\n\n\nCode\n# Calculate the number of physicians per 10,000 residents in the United States\nphysicians.NPI.count() / (states['pop'].sum() / 10000)\n\n\n17.00683953715448\n\n\nIterate through different epsilon values and measure error to gauge what the tradeoff is between privacy and accuracy\n\n\nCode\n# Make a list to keep track of errors\nerrors = []\n\n# iterate between 0.1 and 2\nfor epsilon in np.arange(0.1, 2, 0.1):\n    # repeat the calculation five times\n    for i in range(5):\n        # calculate the noisy count\n        phys_by_spec_dp, rmse = diff_count(physicians, [\"pri_spec_category\"], \"NPI\", epsilon)\n        # save the error value\n        errors.append([epsilon, rmse])\n\n# plot error vs. epsilon\ndf = pd.DataFrame(errors, columns=[\"eps\", \"rmse\"])\nplt.scatter(df.eps, df.rmse)\nplt.show()\n\n\n\n\n\n\nRepeat the procedure, but this time breaking the data down by state and specialty\n\n\nCode\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# use drop duplicates to ensure we only count a physician once per state\nphys_estab_state = phys_estab[['NPI', 'st', 'pri_spec_category']].copy().drop_duplicates()\n\n# lists to keep track of state and specialty/state calculations\nstate_errors = []\nspecialty_errors = []\nfor epsilon in np.arange(0.1, 2, 0.1):\n    for i in range(5):\n        # calculate physicians per state with noise\n        phys_by_state_dp, rmse = diff_count(phys_estab_state, [\"st\"], \"NPI\", epsilon)\n\n        # calculate physicians per specialty per state with and without noise\n        # had to use groupby instead of cross_tab so I could pass the epsilon parameter to the agg function\n        # this calculation could be done with diff_count\n        phys_by_state_spec = phys_estab_state.groupby([\"st\", \"pri_spec_category\"]).count()\n        phys_by_state_spec_dp = phys_estab_state.groupby([\"st\", \"pri_spec_category\"]).agg({\n            \"NPI\": lambda x: dp_agg(x, epsilon)})\n\n        # find the difference between the true and the noisy counts\n        err = mean_squared_error(phys_by_state_spec.to_numpy(), phys_by_state_spec_dp.to_numpy())\n\n        state_errors.append([epsilon, rmse])\n        specialty_errors.append([epsilon, err])\n\n# plot state error\ndf = pd.DataFrame(state_errors, columns=[\"eps\", \"rmse\"])\nplt.scatter(df.eps, df.rmse)\nplt.show()\n\n# plot specialty error\ndf = pd.DataFrame(specialty_errors, columns=[\"eps\", \"rmse\"])\nplt.scatter(df.eps, df.rmse)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\nphys_by_state_spec_dp = phys_estab_state.groupby([\"st\", \"pri_spec_category\"]).agg({\n            \"NPI\": lambda x: dp_agg(x, epsilon)})\n\n\n\n\n\nCode\n# merge the two\nstates.rename(columns={'state': 'st'}, inplace=True)\n\n# reformat the specialty/state table so it conforms to what it looks like in the original notebook\nphys_by_state_spec_dp = phys_by_state_spec_dp.unstack(\"pri_spec_category\")\nphys_by_state_spec_dp.columns = phys_by_state_spec_dp.columns.droplevel(0)\n\nphys_by_state_spec_dp = pd.merge(states[['st', 'pop']], phys_by_state_spec_dp, on='st', how='left')\nphys_by_state_spec_dp = pd.merge(phys_by_state_spec_dp, phys_by_state_dp, on='st', how='left')\n\n# add \"per 10k\" columns\nfor col in phys_by_state_spec_dp.columns[~phys_by_state_spec_dp.columns.isin(states.columns)]:\n    phys_by_state_spec_dp[col + \" per 10k\"] = (phys_by_state_spec_dp[col] / (phys_by_state_spec_dp['pop'] / 10000)).round()\n\n# sort\nphys_by_state_spec_dp.sort_values(by='NPI per 10k', ascending=False, inplace=True)\nphys_by_state_spec_dp\n\n\n\n\n\n\n  \n    \n      \n      st\n      pop\n      Hospital based\n      Medical specialty\n      OBGYN\n      Primary care\n      Psychiatry\n      Surgery specialty\n      NPI\n      Hospital based per 10k\n      Medical specialty per 10k\n      OBGYN per 10k\n      Primary care per 10k\n      Psychiatry per 10k\n      Surgery specialty per 10k\n      NPI per 10k\n    \n  \n  \n    \n      8\n      DC\n      701974\n      663.0\n      711.0\n      191.0\n      805.0\n      106.0\n      516.0\n      2994.0\n      9.0\n      10.0\n      3.0\n      11.0\n      2.0\n      7.0\n      43.0\n    \n    \n      21\n      MA\n      6873003\n      4356.0\n      4290.0\n      825.0\n      5592.0\n      1045.0\n      2865.0\n      18973.0\n      6.0\n      6.0\n      1.0\n      8.0\n      2.0\n      4.0\n      28.0\n    \n    \n      45\n      VT\n      624340\n      328.0\n      236.0\n      70.0\n      552.0\n      107.0\n      266.0\n      1558.0\n      5.0\n      4.0\n      1.0\n      9.0\n      2.0\n      4.0\n      25.0\n    \n    \n      39\n      RI\n      1057798\n      508.0\n      502.0\n      133.0\n      860.0\n      158.0\n      406.0\n      2567.0\n      5.0\n      5.0\n      1.0\n      8.0\n      1.0\n      4.0\n      24.0\n    \n    \n      6\n      CT\n      3570549\n      1773.0\n      1903.0\n      496.0\n      2344.0\n      459.0\n      1487.0\n      8459.0\n      5.0\n      5.0\n      1.0\n      7.0\n      1.0\n      4.0\n      24.0\n    \n    \n      19\n      ME\n      1340825\n      694.0\n      527.0\n      120.0\n      1113.0\n      136.0\n      562.0\n      3148.0\n      5.0\n      4.0\n      1.0\n      8.0\n      1.0\n      4.0\n      23.0\n    \n    \n      23\n      MN\n      5600166\n      2590.0\n      2637.0\n      501.0\n      4447.0\n      442.0\n      2049.0\n      12667.0\n      5.0\n      5.0\n      1.0\n      8.0\n      1.0\n      4.0\n      23.0\n    \n    \n      20\n      MD\n      6037624\n      2976.0\n      2917.0\n      621.0\n      4035.0\n      573.0\n      2459.0\n      13581.0\n      5.0\n      5.0\n      1.0\n      7.0\n      1.0\n      4.0\n      22.0\n    \n    \n      30\n      NJ\n      8885418\n      4303.0\n      4474.0\n      965.0\n      5176.0\n      705.0\n      3481.0\n      19107.0\n      5.0\n      5.0\n      1.0\n      6.0\n      1.0\n      4.0\n      22.0\n    \n    \n      38\n      PA\n      12794885\n      6453.0\n      6143.0\n      1359.0\n      8421.0\n      1074.0\n      5092.0\n      28542.0\n      5.0\n      5.0\n      1.0\n      7.0\n      1.0\n      4.0\n      22.0\n    \n    \n      32\n      NY\n      19514849\n      9980.0\n      10052.0\n      1990.0\n      11500.0\n      1991.0\n      7507.0\n      43019.0\n      5.0\n      5.0\n      1.0\n      6.0\n      1.0\n      4.0\n      22.0\n    \n    \n      34\n      ND\n      760394\n      366.0\n      316.0\n      71.0\n      467.0\n      67.0\n      329.0\n      1617.0\n      5.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      21.0\n    \n    \n      7\n      DE\n      967679\n      415.0\n      405.0\n      114.0\n      680.0\n      62.0\n      372.0\n      2049.0\n      4.0\n      4.0\n      1.0\n      7.0\n      1.0\n      4.0\n      21.0\n    \n    \n      1\n      AK\n      736990\n      440.0\n      201.0\n      67.0\n      533.0\n      50.0\n      280.0\n      1570.0\n      6.0\n      3.0\n      1.0\n      7.0\n      1.0\n      4.0\n      21.0\n    \n    \n      49\n      WI\n      5806975\n      3164.0\n      2240.0\n      526.0\n      3826.0\n      425.0\n      2095.0\n      12277.0\n      5.0\n      4.0\n      1.0\n      7.0\n      1.0\n      4.0\n      21.0\n    \n    \n      50\n      WY\n      581348\n      317.0\n      216.0\n      52.0\n      328.0\n      22.0\n      257.0\n      1192.0\n      5.0\n      4.0\n      1.0\n      6.0\n      0.0\n      4.0\n      21.0\n    \n    \n      22\n      MI\n      9973907\n      4143.0\n      3702.0\n      1032.0\n      6552.0\n      706.0\n      3356.0\n      19492.0\n      4.0\n      4.0\n      1.0\n      7.0\n      1.0\n      3.0\n      20.0\n    \n    \n      41\n      SD\n      879336\n      374.0\n      319.0\n      88.0\n      574.0\n      45.0\n      366.0\n      1765.0\n      4.0\n      4.0\n      1.0\n      7.0\n      1.0\n      4.0\n      20.0\n    \n    \n      26\n      MT\n      1061705\n      564.0\n      357.0\n      77.0\n      660.0\n      53.0\n      411.0\n      2121.0\n      5.0\n      3.0\n      1.0\n      6.0\n      0.0\n      4.0\n      20.0\n    \n    \n      48\n      WV\n      1807426\n      755.0\n      628.0\n      153.0\n      1188.0\n      122.0\n      694.0\n      3538.0\n      4.0\n      3.0\n      1.0\n      7.0\n      1.0\n      4.0\n      20.0\n    \n    \n      35\n      OH\n      11675275\n      4999.0\n      4609.0\n      1157.0\n      7041.0\n      743.0\n      4107.0\n      22653.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      19.0\n    \n    \n      37\n      OR\n      4176346\n      1813.0\n      1419.0\n      332.0\n      2570.0\n      207.0\n      1397.0\n      7738.0\n      4.0\n      3.0\n      1.0\n      6.0\n      0.0\n      3.0\n      19.0\n    \n    \n      25\n      MO\n      6124160\n      2712.0\n      2464.0\n      578.0\n      3225.0\n      440.0\n      2145.0\n      11565.0\n      4.0\n      4.0\n      1.0\n      5.0\n      1.0\n      4.0\n      19.0\n    \n    \n      15\n      IA\n      3150011\n      1409.0\n      1138.0\n      249.0\n      1766.0\n      193.0\n      1105.0\n      5859.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      19.0\n    \n    \n      13\n      IL\n      12716164\n      5324.0\n      4936.0\n      1197.0\n      7517.0\n      759.0\n      3935.0\n      23668.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      3.0\n      19.0\n    \n    \n      17\n      KY\n      4461952\n      1752.0\n      1639.0\n      413.0\n      2408.0\n      231.0\n      1525.0\n      7967.0\n      4.0\n      4.0\n      1.0\n      5.0\n      1.0\n      3.0\n      18.0\n    \n    \n      16\n      KS\n      2912619\n      1213.0\n      1062.0\n      241.0\n      1659.0\n      158.0\n      1032.0\n      5366.0\n      4.0\n      4.0\n      1.0\n      6.0\n      1.0\n      4.0\n      18.0\n    \n    \n      27\n      NE\n      1923826\n      870.0\n      681.0\n      174.0\n      1021.0\n      91.0\n      716.0\n      3550.0\n      5.0\n      4.0\n      1.0\n      5.0\n      0.0\n      4.0\n      18.0\n    \n    \n      14\n      IN\n      6696893\n      2951.0\n      2327.0\n      599.0\n      3721.0\n      350.0\n      2157.0\n      12102.0\n      4.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      18.0\n    \n    \n      11\n      HI\n      1420074\n      687.0\n      408.0\n      118.0\n      917.0\n      78.0\n      396.0\n      2604.0\n      5.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      18.0\n    \n    \n      47\n      WA\n      7512465\n      3290.0\n      2672.0\n      474.0\n      4586.0\n      338.0\n      2268.0\n      13631.0\n      4.0\n      4.0\n      1.0\n      6.0\n      0.0\n      3.0\n      18.0\n    \n    \n      42\n      TN\n      6772268\n      2354.0\n      2372.0\n      619.0\n      3581.0\n      271.0\n      2123.0\n      11319.0\n      3.0\n      4.0\n      1.0\n      5.0\n      0.0\n      3.0\n      17.0\n    \n    \n      33\n      NC\n      10386227\n      3938.0\n      3642.0\n      983.0\n      5268.0\n      678.0\n      3194.0\n      17706.0\n      4.0\n      4.0\n      1.0\n      5.0\n      1.0\n      3.0\n      17.0\n    \n    \n      46\n      VA\n      8509358\n      3036.0\n      2919.0\n      739.0\n      4646.0\n      532.0\n      2544.0\n      14419.0\n      4.0\n      3.0\n      1.0\n      5.0\n      1.0\n      3.0\n      17.0\n    \n    \n      9\n      FL\n      21216924\n      7438.0\n      7974.0\n      1537.0\n      11185.0\n      980.0\n      6283.0\n      35397.0\n      4.0\n      4.0\n      1.0\n      5.0\n      0.0\n      3.0\n      17.0\n    \n    \n      40\n      SC\n      5091517\n      1763.0\n      1527.0\n      483.0\n      2843.0\n      281.0\n      1595.0\n      8491.0\n      3.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      17.0\n    \n    \n      18\n      LA\n      4664616\n      1635.0\n      1637.0\n      497.0\n      2298.0\n      213.0\n      1620.0\n      7898.0\n      4.0\n      4.0\n      1.0\n      5.0\n      0.0\n      3.0\n      17.0\n    \n    \n      0\n      AL\n      4893186\n      1456.0\n      1523.0\n      424.0\n      2521.0\n      213.0\n      1489.0\n      7625.0\n      3.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      16.0\n    \n    \n      31\n      NM\n      2097021\n      825.0\n      558.0\n      166.0\n      1203.0\n      113.0\n      549.0\n      3417.0\n      4.0\n      3.0\n      1.0\n      6.0\n      1.0\n      3.0\n      16.0\n    \n    \n      5\n      CO\n      5684926\n      2413.0\n      1783.0\n      386.0\n      2668.0\n      234.0\n      1687.0\n      9170.0\n      4.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      16.0\n    \n    \n      3\n      AR\n      3011873\n      999.0\n      822.0\n      262.0\n      1713.0\n      135.0\n      812.0\n      4743.0\n      3.0\n      3.0\n      1.0\n      6.0\n      0.0\n      3.0\n      16.0\n    \n    \n      2\n      AZ\n      7174064\n      2695.0\n      2338.0\n      400.0\n      3461.0\n      332.0\n      1905.0\n      11129.0\n      4.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      16.0\n    \n    \n      36\n      OK\n      3949342\n      1322.0\n      1063.0\n      314.0\n      2034.0\n      187.0\n      1069.0\n      5991.0\n      3.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      15.0\n    \n    \n      24\n      MS\n      2981835\n      1094.0\n      911.0\n      274.0\n      1293.0\n      103.0\n      944.0\n      4618.0\n      4.0\n      3.0\n      1.0\n      4.0\n      0.0\n      3.0\n      15.0\n    \n    \n      12\n      ID\n      1754367\n      529.0\n      462.0\n      125.0\n      908.0\n      68.0\n      549.0\n      2641.0\n      3.0\n      3.0\n      1.0\n      5.0\n      0.0\n      3.0\n      15.0\n    \n    \n      44\n      UT\n      3151239\n      1210.0\n      815.0\n      187.0\n      1311.0\n      123.0\n      892.0\n      4538.0\n      4.0\n      3.0\n      1.0\n      4.0\n      0.0\n      3.0\n      14.0\n    \n    \n      10\n      GA\n      10516579\n      3278.0\n      3042.0\n      756.0\n      4609.0\n      414.0\n      2854.0\n      14954.0\n      3.0\n      3.0\n      1.0\n      4.0\n      0.0\n      3.0\n      14.0\n    \n    \n      4\n      CA\n      39346023\n      13534.0\n      11307.0\n      1955.0\n      18319.0\n      1850.0\n      9537.0\n      56502.0\n      3.0\n      3.0\n      0.0\n      5.0\n      0.0\n      2.0\n      14.0\n    \n    \n      43\n      TX\n      28635442\n      8845.0\n      8004.0\n      1902.0\n      11883.0\n      980.0\n      6915.0\n      38529.0\n      3.0\n      3.0\n      1.0\n      4.0\n      0.0\n      2.0\n      13.0\n    \n    \n      51\n      PR\n      3255642\n      531.0\n      948.0\n      151.0\n      1370.0\n      110.0\n      596.0\n      3707.0\n      2.0\n      3.0\n      0.0\n      4.0\n      0.0\n      2.0\n      11.0\n    \n    \n      28\n      MV\n      3030281\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      29\n      MH\n      1355244\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nCode\n# Calculate the number of physicians per 10,000 residents for each CBSA\n# (this would be done the same way as the state data)\n\n\n\n\n\nOur work will provide insights on the institutional structures through which physicians provide care. These structures have changed dramatically over time as a consequence of intensive vertical integration in the healthcare industry.\nThis research question is mostly centered on creating counts or percent breakdowns by state and/or metropolitan area.\nThe main task is to translate business characteristics into distinct “types” by which to group physicians.\nFollow-up question of interest: Rural/urban split (note: need to obtain data)\n\n\nCode\n# link physicians to establishments\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# add practices\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\n\n# get practice_type column, location, and identifier\nphys_prac_state = phys_prac[['NPI', 'st', 'practice_type_random']].copy().drop_duplicates()\nphys_prac_state = phys_prac_state[phys_prac_state[\"practice_type_random\"].notnull()]\n\nstate_errors = []\npractice_errors = []\nfor epsilon in np.arange(0.1, 2, 0.1):\n    for i in range(5):\n        # find doctor's per state\n        phys_by_state_dp, rmse_s = diff_count(phys_prac_state, [\"st\"], \"NPI\", epsilon)\n\n        # find doctor's per practice type and state\n        est_type_crosstab_dp, rmse_p = diff_count(phys_prac_state, [\"st\", \"practice_type_random\"], \"NPI\", epsilon)\n\n        state_errors.append([epsilon, rmse_s])\n        practice_errors.append([epsilon, rmse_p])\n\n# plot state error\ndf = pd.DataFrame(state_errors, columns=[\"eps\", \"rmse\"])\nplt.scatter(df.eps, df.rmse)\nplt.show()\n\n# plot specialty error\ndf = pd.DataFrame(practice_errors, columns=[\"eps\", \"rmse\"])\nplt.scatter(df.eps, df.rmse)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCode\n# reformat the \"crosstab\" table to actually be in crosstab format\nest_type_pv_dp = est_type_crosstab_dp.pivot(index=\"st\", columns=\"practice_type_random\", values=\"NPI\")\n\n# add state count to establishment type count\nphys_by_est = pd.merge(est_type_pv_dp, phys_by_state_dp, how=\"left\", on=\"st\")\n\n# get list of establishment types\nests = phys_prac_state[\"practice_type_random\"].unique()\n\nfor est in ests:\n    # find the fraction of all physicians who are part of each type of establishment\n    phys_by_est[est + \"_fraction\"] = phys_by_est[est] / phys_by_est.NPI\n\nphys_by_est\n\n\n\n\n\n\n  \n    \n      \n      st\n      type-0\n      type-1\n      type-2\n      type-3\n      type-4\n      type-5\n      type-6\n      type-7\n      type-8\n      ...\n      type-4_fraction\n      type-2_fraction\n      type-3_fraction\n      type-5_fraction\n      type-0_fraction\n      type-6_fraction\n      type-9_fraction\n      type-1_fraction\n      type-8_fraction\n      type-7_fraction\n    \n  \n  \n    \n      0\n      AK\n      97.0\n      151.0\n      199.0\n      27.0\n      54.0\n      296.0\n      367.0\n      81.0\n      74.0\n      ...\n      0.036315\n      0.133826\n      0.018157\n      0.199059\n      0.065232\n      0.246806\n      0.094822\n      0.101547\n      0.049765\n      0.054472\n    \n    \n      1\n      AL\n      571.0\n      1648.0\n      507.0\n      838.0\n      351.0\n      476.0\n      741.0\n      384.0\n      592.0\n      ...\n      0.051121\n      0.073842\n      0.122051\n      0.069327\n      0.083163\n      0.107923\n      0.110690\n      0.240023\n      0.086222\n      0.055928\n    \n    \n      2\n      AR\n      624.0\n      220.0\n      290.0\n      529.0\n      900.0\n      379.0\n      211.0\n      503.0\n      460.0\n      ...\n      0.203943\n      0.065715\n      0.119873\n      0.085883\n      0.141400\n      0.047813\n      0.067754\n      0.049853\n      0.104237\n      0.113981\n    \n    \n      3\n      AZ\n      1039.0\n      1042.0\n      694.0\n      933.0\n      1491.0\n      916.0\n      1245.0\n      1845.0\n      805.0\n      ...\n      0.141273\n      0.065757\n      0.088403\n      0.086792\n      0.098446\n      0.117965\n      0.051544\n      0.098730\n      0.076274\n      0.174815\n    \n    \n      4\n      CA\n      4389.0\n      4365.0\n      2883.0\n      6017.0\n      5247.0\n      3312.0\n      7537.0\n      4247.0\n      6770.0\n      ...\n      0.104105\n      0.057201\n      0.119383\n      0.065713\n      0.087082\n      0.149541\n      0.111724\n      0.086605\n      0.134323\n      0.084264\n    \n    \n      5\n      CO\n      950.0\n      749.0\n      788.0\n      1552.0\n      1231.0\n      510.0\n      885.0\n      756.0\n      998.0\n      ...\n      0.130999\n      0.083857\n      0.165159\n      0.054273\n      0.101096\n      0.094179\n      0.104182\n      0.079706\n      0.106204\n      0.080451\n    \n    \n      6\n      CT\n      675.0\n      1532.0\n      603.0\n      821.0\n      741.0\n      480.0\n      845.0\n      668.0\n      544.0\n      ...\n      0.087300\n      0.071041\n      0.096725\n      0.056550\n      0.079524\n      0.099552\n      0.186027\n      0.180490\n      0.064090\n      0.078699\n    \n    \n      7\n      DC\n      297.0\n      247.0\n      119.0\n      259.0\n      760.0\n      219.0\n      311.0\n      49.0\n      108.0\n      ...\n      0.268836\n      0.042094\n      0.091617\n      0.077467\n      0.105058\n      0.110011\n      0.162717\n      0.087372\n      0.038203\n      0.017333\n    \n    \n      8\n      DE\n      104.0\n      142.0\n      82.0\n      118.0\n      94.0\n      196.0\n      165.0\n      101.0\n      81.0\n      ...\n      0.049604\n      0.043272\n      0.062269\n      0.103430\n      0.054881\n      0.087071\n      0.427968\n      0.074934\n      0.042744\n      0.053298\n    \n    \n      9\n      FL\n      3809.0\n      2894.0\n      3251.0\n      3181.0\n      2327.0\n      2957.0\n      3158.0\n      2878.0\n      2529.0\n      ...\n      0.074409\n      0.103955\n      0.101717\n      0.094554\n      0.121798\n      0.100982\n      0.137243\n      0.092540\n      0.080868\n      0.092028\n    \n    \n      10\n      GA\n      1345.0\n      1029.0\n      851.0\n      968.0\n      1713.0\n      1017.0\n      2118.0\n      1185.0\n      2477.0\n      ...\n      0.119008\n      0.059122\n      0.067250\n      0.070654\n      0.093442\n      0.147145\n      0.117480\n      0.071488\n      0.172086\n      0.082326\n    \n    \n      11\n      GU\n      NaN\n      14.0\n      NaN\n      16.0\n      NaN\n      46.0\n      12.0\n      2.0\n      38.0\n      ...\n      NaN\n      NaN\n      0.083770\n      0.240838\n      NaN\n      0.062827\n      0.324607\n      0.073298\n      0.198953\n      0.010471\n    \n    \n      12\n      HI\n      128.0\n      249.0\n      109.0\n      292.0\n      57.0\n      200.0\n      313.0\n      501.0\n      92.0\n      ...\n      0.027550\n      0.052682\n      0.141131\n      0.096665\n      0.061866\n      0.151281\n      0.064282\n      0.120348\n      0.044466\n      0.242146\n    \n    \n      13\n      IA\n      430.0\n      637.0\n      481.0\n      527.0\n      594.0\n      313.0\n      353.0\n      1604.0\n      691.0\n      ...\n      0.099132\n      0.080274\n      0.087951\n      0.052236\n      0.071762\n      0.058912\n      0.060080\n      0.106308\n      0.115320\n      0.267690\n    \n    \n      14\n      ID\n      122.0\n      237.0\n      153.0\n      302.0\n      735.0\n      143.0\n      203.0\n      146.0\n      293.0\n      ...\n      0.293062\n      0.061005\n      0.120415\n      0.057018\n      0.048644\n      0.080941\n      0.069378\n      0.094498\n      0.116826\n      0.058214\n    \n    \n      15\n      IL\n      1818.0\n      1108.0\n      3446.0\n      4130.0\n      2675.0\n      2771.0\n      1611.0\n      3314.0\n      939.0\n      ...\n      0.115606\n      0.148926\n      0.178487\n      0.119755\n      0.078569\n      0.069623\n      0.057349\n      0.047885\n      0.040581\n      0.143221\n    \n    \n      16\n      IN\n      966.0\n      1627.0\n      1152.0\n      1937.0\n      910.0\n      907.0\n      922.0\n      1604.0\n      720.0\n      ...\n      0.075563\n      0.095657\n      0.160840\n      0.075313\n      0.080213\n      0.076559\n      0.107780\n      0.135099\n      0.059786\n      0.133189\n    \n    \n      17\n      KS\n      344.0\n      527.0\n      442.0\n      606.0\n      320.0\n      839.0\n      294.0\n      478.0\n      923.0\n      ...\n      0.060298\n      0.083286\n      0.114189\n      0.158093\n      0.064820\n      0.055399\n      0.100245\n      0.099303\n      0.173921\n      0.090070\n    \n    \n      18\n      KY\n      873.0\n      493.0\n      630.0\n      921.0\n      556.0\n      1525.0\n      563.0\n      845.0\n      713.0\n      ...\n      0.073129\n      0.082862\n      0.121136\n      0.200579\n      0.114823\n      0.074050\n      0.064054\n      0.064843\n      0.093779\n      0.111140\n    \n    \n      19\n      LA\n      1465.0\n      512.0\n      459.0\n      516.0\n      1019.0\n      367.0\n      339.0\n      608.0\n      1598.0\n      ...\n      0.137628\n      0.061994\n      0.069692\n      0.049568\n      0.197866\n      0.045786\n      0.070367\n      0.069152\n      0.215829\n      0.082118\n    \n    \n      20\n      MA\n      1277.0\n      923.0\n      4208.0\n      2903.0\n      3522.0\n      1574.0\n      1943.0\n      1074.0\n      1636.0\n      ...\n      0.172774\n      0.206426\n      0.142409\n      0.077214\n      0.062644\n      0.095315\n      0.064901\n      0.045278\n      0.080255\n      0.052686\n    \n    \n      21\n      MD\n      961.0\n      1438.0\n      978.0\n      1725.0\n      1449.0\n      542.0\n      719.0\n      846.0\n      1393.0\n      ...\n      0.131084\n      0.088475\n      0.156052\n      0.049032\n      0.086937\n      0.065044\n      0.090465\n      0.130089\n      0.126018\n      0.076533\n    \n    \n      22\n      ME\n      366.0\n      388.0\n      579.0\n      154.0\n      172.0\n      302.0\n      355.0\n      544.0\n      140.0\n      ...\n      0.051343\n      0.172836\n      0.045970\n      0.090149\n      0.109254\n      0.105970\n      0.104776\n      0.115821\n      0.041791\n      0.162388\n    \n    \n      23\n      MI\n      2933.0\n      1505.0\n      1926.0\n      3382.0\n      2148.0\n      1346.0\n      1346.0\n      1731.0\n      1712.0\n      ...\n      0.112969\n      0.101294\n      0.177869\n      0.070790\n      0.154255\n      0.070790\n      0.052067\n      0.079152\n      0.090039\n      0.091038\n    \n    \n      24\n      MN\n      900.0\n      2058.0\n      1111.0\n      674.0\n      2928.0\n      464.0\n      2612.0\n      1384.0\n      924.0\n      ...\n      0.206313\n      0.078284\n      0.047492\n      0.032694\n      0.063416\n      0.184047\n      0.080045\n      0.145011\n      0.065107\n      0.097520\n    \n    \n      25\n      MO\n      661.0\n      1295.0\n      464.0\n      1416.0\n      1132.0\n      684.0\n      1106.0\n      1445.0\n      1407.0\n      ...\n      0.095786\n      0.039262\n      0.119817\n      0.057878\n      0.055932\n      0.093586\n      0.186749\n      0.109579\n      0.119056\n      0.122271\n    \n    \n      26\n      MP\n      NaN\n      2.0\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n      24.0\n      NaN\n      ...\n      NaN\n      0.100000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.066667\n      NaN\n      0.800000\n    \n    \n      27\n      MS\n      413.0\n      809.0\n      424.0\n      287.0\n      498.0\n      766.0\n      283.0\n      267.0\n      162.0\n      ...\n      0.112466\n      0.095754\n      0.064815\n      0.172990\n      0.093270\n      0.063911\n      0.117209\n      0.182701\n      0.036585\n      0.060298\n    \n    \n      28\n      MT\n      59.0\n      280.0\n      100.0\n      114.0\n      243.0\n      346.0\n      505.0\n      205.0\n      163.0\n      ...\n      0.112604\n      0.046339\n      0.052827\n      0.160334\n      0.027340\n      0.234013\n      0.065802\n      0.129750\n      0.075533\n      0.094995\n    \n    \n      29\n      NC\n      3337.0\n      2987.0\n      1144.0\n      1844.0\n      2396.0\n      997.0\n      917.0\n      1382.0\n      823.0\n      ...\n      0.138146\n      0.065959\n      0.106319\n      0.057484\n      0.192401\n      0.052871\n      0.087581\n      0.172221\n      0.047452\n      0.079682\n    \n    \n      30\n      ND\n      20.0\n      243.0\n      53.0\n      26.0\n      189.0\n      35.0\n      31.0\n      219.0\n      488.0\n      ...\n      0.106659\n      0.029910\n      0.014673\n      0.019752\n      0.011287\n      0.017494\n      0.264108\n      0.137133\n      0.275395\n      0.123589\n    \n    \n      31\n      NE\n      199.0\n      551.0\n      328.0\n      310.0\n      446.0\n      280.0\n      822.0\n      234.0\n      214.0\n      ...\n      0.122730\n      0.090259\n      0.085305\n      0.077050\n      0.054761\n      0.226197\n      0.069070\n      0.151624\n      0.058888\n      0.064392\n    \n    \n      32\n      NH\n      102.0\n      120.0\n      409.0\n      192.0\n      1004.0\n      293.0\n      698.0\n      192.0\n      163.0\n      ...\n      0.292030\n      0.118965\n      0.055846\n      0.085224\n      0.029668\n      0.203025\n      0.077661\n      0.034904\n      0.047411\n      0.055846\n    \n    \n      33\n      NJ\n      1713.0\n      1855.0\n      2045.0\n      1654.0\n      1257.0\n      2387.0\n      1575.0\n      1073.0\n      1094.0\n      ...\n      0.074489\n      0.121185\n      0.098015\n      0.141452\n      0.101511\n      0.093333\n      0.131793\n      0.109926\n      0.064830\n      0.063585\n    \n    \n      34\n      NM\n      165.0\n      180.0\n      155.0\n      308.0\n      903.0\n      164.0\n      250.0\n      277.0\n      164.0\n      ...\n      0.273222\n      0.046899\n      0.093192\n      0.049622\n      0.049924\n      0.075643\n      0.223903\n      0.054463\n      0.049622\n      0.083812\n    \n    \n      35\n      NV\n      287.0\n      653.0\n      245.0\n      151.0\n      453.0\n      500.0\n      449.0\n      406.0\n      179.0\n      ...\n      0.127822\n      0.069131\n      0.042607\n      0.141084\n      0.080982\n      0.126693\n      0.062359\n      0.184255\n      0.050508\n      0.114560\n    \n    \n      36\n      NY\n      3585.0\n      4936.0\n      3706.0\n      6552.0\n      3442.0\n      4181.0\n      4869.0\n      2951.0\n      3202.0\n      ...\n      0.083990\n      0.090432\n      0.159879\n      0.102023\n      0.087480\n      0.118811\n      0.086772\n      0.120446\n      0.078134\n      0.072009\n    \n    \n      37\n      OH\n      2430.0\n      3649.0\n      2993.0\n      2230.0\n      1841.0\n      2251.0\n      2583.0\n      1246.0\n      2229.0\n      ...\n      0.079970\n      0.130012\n      0.096868\n      0.097780\n      0.105556\n      0.112202\n      0.068242\n      0.158507\n      0.096825\n      0.054124\n    \n    \n      38\n      OK\n      336.0\n      670.0\n      1255.0\n      469.0\n      456.0\n      550.0\n      412.0\n      391.0\n      662.0\n      ...\n      0.078485\n      0.216007\n      0.080723\n      0.094664\n      0.057831\n      0.070912\n      0.104991\n      0.115318\n      0.113941\n      0.067298\n    \n    \n      39\n      OR\n      1331.0\n      648.0\n      589.0\n      681.0\n      1332.0\n      929.0\n      438.0\n      697.0\n      934.0\n      ...\n      0.162142\n      0.071698\n      0.082897\n      0.113086\n      0.162021\n      0.053317\n      0.076811\n      0.078880\n      0.113694\n      0.084845\n    \n    \n      40\n      PA\n      1605.0\n      2503.0\n      2967.0\n      3275.0\n      1731.0\n      4294.0\n      3635.0\n      2951.0\n      3961.0\n      ...\n      0.061437\n      0.105306\n      0.116238\n      0.152405\n      0.056965\n      0.129015\n      0.044508\n      0.088838\n      0.140586\n      0.104738\n    \n    \n      41\n      PR\n      26.0\n      69.0\n      46.0\n      234.0\n      127.0\n      39.0\n      99.0\n      93.0\n      74.0\n      ...\n      0.131063\n      0.047472\n      0.241486\n      0.040248\n      0.026832\n      0.102167\n      0.166151\n      0.071207\n      0.076367\n      0.095975\n    \n    \n      42\n      RI\n      695.0\n      115.0\n      472.0\n      158.0\n      179.0\n      63.0\n      177.0\n      205.0\n      362.0\n      ...\n      0.068321\n      0.180153\n      0.060305\n      0.024046\n      0.265267\n      0.067557\n      0.073282\n      0.043893\n      0.138168\n      0.078244\n    \n    \n      43\n      SC\n      440.0\n      585.0\n      417.0\n      783.0\n      614.0\n      914.0\n      1428.0\n      964.0\n      1625.0\n      ...\n      0.073043\n      0.049607\n      0.093148\n      0.108732\n      0.052344\n      0.169879\n      0.075541\n      0.069593\n      0.193314\n      0.114680\n    \n    \n      44\n      SD\n      607.0\n      97.0\n      58.0\n      39.0\n      78.0\n      159.0\n      351.0\n      93.0\n      268.0\n      ...\n      0.041890\n      0.031149\n      0.020945\n      0.085392\n      0.325994\n      0.188507\n      0.059076\n      0.052095\n      0.143931\n      0.049946\n    \n    \n      45\n      TN\n      865.0\n      1214.0\n      1255.0\n      914.0\n      821.0\n      2205.0\n      570.0\n      990.0\n      811.0\n      ...\n      0.076564\n      0.117038\n      0.085237\n      0.205633\n      0.080668\n      0.053157\n      0.100532\n      0.113215\n      0.075632\n      0.092325\n    \n    \n      46\n      TX\n      3827.0\n      2717.0\n      2125.0\n      4114.0\n      4022.0\n      2357.0\n      2889.0\n      5088.0\n      4525.0\n      ...\n      0.117606\n      0.062136\n      0.120296\n      0.068920\n      0.111904\n      0.084476\n      0.074096\n      0.079447\n      0.132314\n      0.148776\n    \n    \n      47\n      UT\n      985.0\n      1512.0\n      133.0\n      242.0\n      361.0\n      142.0\n      133.0\n      276.0\n      143.0\n      ...\n      0.081600\n      0.030063\n      0.054702\n      0.032098\n      0.222649\n      0.030063\n      0.111890\n      0.341772\n      0.032324\n      0.062387\n    \n    \n      48\n      VA\n      879.0\n      573.0\n      1578.0\n      1062.0\n      3026.0\n      1965.0\n      804.0\n      884.0\n      1877.0\n      ...\n      0.218705\n      0.114050\n      0.076756\n      0.142021\n      0.063530\n      0.058109\n      0.085863\n      0.041414\n      0.135661\n      0.063891\n    \n    \n      49\n      VI\n      12.0\n      2.0\n      1.0\n      NaN\n      2.0\n      5.0\n      9.0\n      NaN\n      1.0\n      ...\n      0.051282\n      0.025641\n      NaN\n      0.128205\n      0.307692\n      0.230769\n      0.153846\n      0.051282\n      0.025641\n      NaN\n    \n    \n      50\n      VT\n      82.0\n      60.0\n      89.0\n      656.0\n      108.0\n      65.0\n      52.0\n      125.0\n      41.0\n      ...\n      0.072338\n      0.059612\n      0.439384\n      0.043537\n      0.054923\n      0.034829\n      0.145345\n      0.040188\n      0.027461\n      0.083724\n    \n    \n      51\n      WA\n      3503.0\n      635.0\n      986.0\n      721.0\n      858.0\n      786.0\n      1212.0\n      1572.0\n      1053.0\n      ...\n      0.060683\n      0.069736\n      0.050994\n      0.055591\n      0.247754\n      0.085720\n      0.198953\n      0.044911\n      0.074475\n      0.111182\n    \n    \n      52\n      WI\n      941.0\n      895.0\n      2228.0\n      1997.0\n      912.0\n      1103.0\n      2995.0\n      1420.0\n      1341.0\n      ...\n      0.063140\n      0.154251\n      0.138258\n      0.076364\n      0.065148\n      0.207353\n      0.042301\n      0.061963\n      0.092841\n      0.098311\n    \n    \n      53\n      WV\n      212.0\n      732.0\n      484.0\n      491.0\n      147.0\n      304.0\n      194.0\n      163.0\n      257.0\n      ...\n      0.045342\n      0.149291\n      0.151450\n      0.093769\n      0.065392\n      0.059840\n      0.079581\n      0.225787\n      0.079272\n      0.050278\n    \n    \n      54\n      WY\n      120.0\n      148.0\n      130.0\n      41.0\n      93.0\n      40.0\n      143.0\n      127.0\n      128.0\n      ...\n      0.084316\n      0.117860\n      0.037171\n      0.036265\n      0.108794\n      0.129646\n      0.119674\n      0.134180\n      0.116047\n      0.115141\n    \n  \n\n55 rows × 22 columns\n\n\n\n\n\nCode\n# could also add per capita stats\n\n\n\n\n\nWith the growing interest in social determinants of health and community representation, our project has an opportunity to provide unique insights on how and whether physicians reflect the communities they serve. While our research will most likely focus on place of birth and race/ethnicity, we will instead use age and gender as a stand-in since these can be readily obtained from public data.\nBreakdown by Characteristics: Specialty Category, Organization Type\nBreakdown by Geography: Nation, State, CBSA\n\n\nCode\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# use drop duplicates to ensure we only count a physician once per state\nphys_estab_state = phys_estab[['NPI', 'st', 'gndr', 'age_imputed']].copy().drop_duplicates()\n\n# topcode ages to 85\nphys_estab_state[\"age_imputed\"] = np.where(phys_estab_state[\"age_imputed\"] > 85, 85, phys_estab_state[\"age_imputed\"])\n\n# put ages into bins of 10 years\nage_bins = np.arange(0, 90, 10)\nidxs = np.digitize(phys_estab_state[\"age_imputed\"], age_bins)\n\n# add bin labels to dataset\nphys_estab_state[\"bin\"] = idxs\n\nerrors = []\nfor epsilon in np.arange(0.1, 2, 0.1):\n    for i in range(5):\n        phys_age_sex_dp, rmse = diff_count(phys_estab_state, [\"st\", \"gndr\", \"bin\"], \"NPI\", epsilon)\n        errors.append([epsilon, rmse])\n\n# plot state error\ndf = pd.DataFrame(errors, columns=[\"eps\", \"rmse\"])\nplt.scatter(df.eps, df.rmse)\nplt.show()\n\n\n\n\n\n\n\nCode\n# group by state and bins and get noisy populations\nphys_age_sex_dp = phys_estab_state.groupby([\"st\", \"gndr\", \"bin\"]).agg({\"NPI\": lambda x: dp_agg(x, epsilon), \"age_imputed\": \"min\"}).reset_index()\nphys_age_sex_dp.drop(columns=[\"bin\"], inplace=True)\n\n# add state data\nphys_age_sex_pop = pd.merge(phys_age_sex_dp, state_agesex, how=\"left\", left_on=[\"st\", \"gndr\", \"age_imputed\"], right_on=[\"abr\", \"sex\", \"bin_min\"])\n\n# filter fields\nphys_age_sex_pop = phys_age_sex_pop[[\"st\", \"gndr\", \"bin_min\", \"NPI\", \"pop\"]]\n\n# compute per 10k stat\nphys_age_sex_pop[\"phy_per_10k\"] = phys_age_sex_pop[\"NPI\"] / (phys_age_sex_pop[\"pop\"]/10000)\nphys_age_sex_pop\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      bin_min\n      NPI\n      pop\n      phy_per_10k\n    \n  \n  \n    \n      0\n      AK\n      F\n      30.0\n      145\n      51272.0\n      28.280543\n    \n    \n      1\n      AK\n      F\n      40.0\n      168\n      40674.0\n      41.304027\n    \n    \n      2\n      AK\n      F\n      50.0\n      130\n      43451.0\n      29.918759\n    \n    \n      3\n      AK\n      F\n      60.0\n      88\n      39819.0\n      22.100003\n    \n    \n      4\n      AK\n      F\n      70.0\n      17\n      19719.0\n      8.621127\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      650\n      WY\n      M\n      40.0\n      273\n      34530.0\n      79.061685\n    \n    \n      651\n      WY\n      M\n      50.0\n      250\n      35040.0\n      71.347032\n    \n    \n      652\n      WY\n      M\n      60.0\n      199\n      37838.0\n      52.592632\n    \n    \n      653\n      WY\n      M\n      70.0\n      72\n      21138.0\n      34.061879\n    \n    \n      654\n      WY\n      M\n      80.0\n      11\n      9006.0\n      12.214080\n    \n  \n\n655 rows × 6 columns\n\n\n\nSince the analyses below runs on noisy data, no additional noise needs to be added to maintain DP\n\n\nCode\n\"\"\"\npermutation tests\nhttps://www.cs.cornell.edu/courses/cs1380/2018sp/textbook/chapters/16/1/two-categorical-distributions.html\n\"\"\"\n# function to get proportion of a class in the data\ndef proportions(array):\n    return array/np.sum(array)\n\n# total variation distance to measure how different categorical distributions are\ndef tvd(dist1, dist2):\n    return 0.5*(np.sum(np.abs(dist1 - dist2)))\n\n# get the data associated with physicians\nphys = phys_age_sex_pop[[\"st\", \"gndr\", \"bin_min\", \"NPI\"]].copy()\n\n# get the data associated with the general public\npopulation = phys_age_sex_pop[[\"st\", \"gndr\", \"bin_min\", \"pop\"]].copy()\n\n# add a column to flag whether the data is for phys or not\nphys[\"phy\"] = \"1\"\npopulation[\"phy\"] = \"0\"\n\n# merge the two datasets\nmerged = pd.DataFrame(np.vstack([phys, population]), columns=[\"st\", \"gndr\", \"bin\", \"pop\", \"phy\"]).fillna(0)\n\n# combine all categories into a single column\nmerged[\"cat\"] = merged[\"gndr\"] + merged[\"bin\"].astype(str)\nmerged.drop(columns=[\"gndr\", \"bin\"], inplace=True)\n\n# get a subset for testing\nsubset = merged[merged[\"st\"] == \"AK\"]\n\nsubset\n\n\n\n\n\n\n  \n    \n      \n      st\n      pop\n      phy\n      cat\n    \n  \n  \n    \n      0\n      AK\n      145.0\n      1\n      F30.0\n    \n    \n      1\n      AK\n      168.0\n      1\n      F40.0\n    \n    \n      2\n      AK\n      130.0\n      1\n      F50.0\n    \n    \n      3\n      AK\n      88.0\n      1\n      F60.0\n    \n    \n      4\n      AK\n      17.0\n      1\n      F70.0\n    \n    \n      5\n      AK\n      5.0\n      1\n      F0.0\n    \n    \n      6\n      AK\n      139.0\n      1\n      M30.0\n    \n    \n      7\n      AK\n      274.0\n      1\n      M40.0\n    \n    \n      8\n      AK\n      292.0\n      1\n      M50.0\n    \n    \n      9\n      AK\n      196.0\n      1\n      M60.0\n    \n    \n      10\n      AK\n      95.0\n      1\n      M70.0\n    \n    \n      11\n      AK\n      24.0\n      1\n      M80.0\n    \n    \n      655\n      AK\n      51272.0\n      0\n      F30.0\n    \n    \n      656\n      AK\n      40674.0\n      0\n      F40.0\n    \n    \n      657\n      AK\n      43451.0\n      0\n      F50.0\n    \n    \n      658\n      AK\n      39819.0\n      0\n      F60.0\n    \n    \n      659\n      AK\n      19719.0\n      0\n      F70.0\n    \n    \n      660\n      AK\n      0.0\n      0\n      F0.0\n    \n    \n      661\n      AK\n      52633.0\n      0\n      M30.0\n    \n    \n      662\n      AK\n      43363.0\n      0\n      M40.0\n    \n    \n      663\n      AK\n      47407.0\n      0\n      M50.0\n    \n    \n      664\n      AK\n      41558.0\n      0\n      M60.0\n    \n    \n      665\n      AK\n      20398.0\n      0\n      M70.0\n    \n    \n      666\n      AK\n      6421.0\n      0\n      M80.0\n    \n  \n\n\n\n\n\n\nCode\ndef permutation_test(df, category, classes, repetitions):\n\n    # create a pivot table finding the number of phys/not in each category\n    counts = pd.pivot_table(df, columns=[classes], index=category, aggfunc=np.size)\n    \n    # stop here if data is malformed\n    if \"0\" not in counts.columns or \"1\" not in counts.columns:\n        return (0,0)\n\n    # calculate tvd\n    observed_tvd = tvd(proportions(counts[\"0\"]), proportions(counts[\"1\"]))\n\n    tvds = []\n    # shuffle data x times based on input\n    for i in range(repetitions):\n        # shuffle phys/not label\n        shuffled_var = df[[classes]].sample(frac=1).to_numpy()\n        # add new label to df\n        df[\"shuffled\"] = shuffled_var\n\n        # recalculate pivot based on shuffled labels\n        shuffled_counts = pd.pivot_table(df[[category, \"shuffled\"]], columns=[\"shuffled\"], index=category, aggfunc=np.size)\n\n        # calculate tvd based on shuffled counts\n        new_tvd = tvd(proportions(shuffled_counts[\"0\"]), proportions(shuffled_counts[\"1\"]))\n        \n        # keep track of calculated values\n        tvds.append(new_tvd)\n\n    # calculate p-value\n    tvds = np.array(tvds)\n    emp_p = np.count_nonzero(tvds >= observed_tvd)/repetitions\n\n    return (observed_tvd, emp_p)\n\n\n\n\nCode\n# CAREFUL: takes forever (~hour for all 50 states)\nresults = []\nstates = merged[\"st\"].unique()\nfor name, group in merged.groupby(\"st\"):\n    # format the data so it is one row per person\n    group = group.loc[group.index.repeat(group[\"pop\"])]\n    # run function with relevant columns\n    vals = permutation_test(group[[\"phy\", \"cat\"]], \"cat\", \"phy\", 50)\n    results.append([name, vals[0], vals[1]])\n    print([name, vals[0], vals[1]])\n\nprint(results)\n\n\n/usr/local/lib/python3.9/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._set_item(key, value)\n\n\n['AK', 0.18021522997958397, 0.0]\n['AL', 0.2932136955124203, 0.0]\n['AR', 0.2937022718135871, 0.0]\n['AZ', 0.2606198561672938, 0.0]\n['CA', 0.2347407528633154, 0.0]\n['CO', 0.2288251123980192, 0.0]\n['CT', 0.23527136107987995, 0.0]\n['DC', 0, 0]\n['DE', 0.25029933860194475, 0.0]\n['FL', 0.3029553778100085, 0.0]\n['GA', 0.27281678989061126, 0.0]\n['GU', 0, 0]\n['HI', 0.19974944091922722, 0.0]\n['IA', 0.23998560753313636, 0.0]\n['ID', 0.24630874345728387, 0.0]\n['IL', 0.23441423101332284, 0.0]\n['IN', 0.2697486816663744, 0.0]\n['KS', 0.26336729446962004, 0.0]\n['KY', 0.2722768267897496, 0.0]\n['LA', 0.27347980078737105, 0.0]\n['MA', 0.1978436360546689, 0.0]\n['MD', 0.2289315610239334, 0.0]\n['ME', 0.21749939235094626, 0.0]\n['MI', 0.21914869692714858, 0.0]\n['MN', 0.21875506804397793, 0.0]\n['MO', 0.2580233284315929, 0.0]\n['MP', 0, 0]\n['MS', 0.3189530580657671, 0.0]\n['MT', 0.2821870597873727, 0.0]\n['NC', 0.25963133381598574, 0.0]\n['ND', 0.21149760129583914, 0.0]\n['NE', 0.2530966429205866, 0.0]\n['NH', 0.21119041454015075, 0.0]\n['NJ', 0.24876949889183236, 0.0]\n['NM', 0.2186982696570639, 0.0]\n['NV', 0.2855833315796024, 0.0]\n['NY', 0.24237199347297012, 0.0]\n['OH', 0.2520410830889507, 0.0]\n['OK', 0.2840563221741394, 0.0]\n['OR', 0.24334216904386435, 0.0]\n['PA', 0.25059056617991216, 0.0]\n['PR', 0, 0]\n['RI', 0.18583370830677687, 0.0]\n['SC', 0.27903244780381925, 0.0]\n['SD', 0.23387317242589084, 0.0]\n['TN', 0.2920888997854446, 0.0]\n['TX', 0.2648182719557806, 0.0]\n['UT', 0.30165251189624265, 0.0]\n['VA', 0.25963604454610856, 0.0]\n['VI', 0, 0]\n['VT', 0.17382601049143895, 0.0]\n['WA', 0.2198138521463306, 0.0]\n['WI', 0.2408151134735927, 0.0]\n['WV', 0.25620053560926886, 0.0]\n['WY', 0.29211924877081863, 0.0]\n[['AK', 0.18021522997958397, 0.0], ['AL', 0.2932136955124203, 0.0], ['AR', 0.2937022718135871, 0.0], ['AZ', 0.2606198561672938, 0.0], ['CA', 0.2347407528633154, 0.0], ['CO', 0.2288251123980192, 0.0], ['CT', 0.23527136107987995, 0.0], ['DC', 0, 0], ['DE', 0.25029933860194475, 0.0], ['FL', 0.3029553778100085, 0.0], ['GA', 0.27281678989061126, 0.0], ['GU', 0, 0], ['HI', 0.19974944091922722, 0.0], ['IA', 0.23998560753313636, 0.0], ['ID', 0.24630874345728387, 0.0], ['IL', 0.23441423101332284, 0.0], ['IN', 0.2697486816663744, 0.0], ['KS', 0.26336729446962004, 0.0], ['KY', 0.2722768267897496, 0.0], ['LA', 0.27347980078737105, 0.0], ['MA', 0.1978436360546689, 0.0], ['MD', 0.2289315610239334, 0.0], ['ME', 0.21749939235094626, 0.0], ['MI', 0.21914869692714858, 0.0], ['MN', 0.21875506804397793, 0.0], ['MO', 0.2580233284315929, 0.0], ['MP', 0, 0], ['MS', 0.3189530580657671, 0.0], ['MT', 0.2821870597873727, 0.0], ['NC', 0.25963133381598574, 0.0], ['ND', 0.21149760129583914, 0.0], ['NE', 0.2530966429205866, 0.0], ['NH', 0.21119041454015075, 0.0], ['NJ', 0.24876949889183236, 0.0], ['NM', 0.2186982696570639, 0.0], ['NV', 0.2855833315796024, 0.0], ['NY', 0.24237199347297012, 0.0], ['OH', 0.2520410830889507, 0.0], ['OK', 0.2840563221741394, 0.0], ['OR', 0.24334216904386435, 0.0], ['PA', 0.25059056617991216, 0.0], ['PR', 0, 0], ['RI', 0.18583370830677687, 0.0], ['SC', 0.27903244780381925, 0.0], ['SD', 0.23387317242589084, 0.0], ['TN', 0.2920888997854446, 0.0], ['TX', 0.2648182719557806, 0.0], ['UT', 0.30165251189624265, 0.0], ['VA', 0.25963604454610856, 0.0], ['VI', 0, 0], ['VT', 0.17382601049143895, 0.0], ['WA', 0.2198138521463306, 0.0], ['WI', 0.2408151134735927, 0.0], ['WV', 0.25620053560926886, 0.0], ['WY', 0.29211924877081863, 0.0]]\n\n\n\n\nCode\n# load results into a pandas df\ntvd = pd.DataFrame(results, columns = [\"st\", \"tvd\", \"pval\"])\n\n# add it to the physician data by state\nage_tvd = pd.merge(phys_age_sex_pop, tvd, on=\"st\", how=\"left\")\n\nage_tvd\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      bin_min\n      NPI\n      pop\n      phy_per_10k\n      tvd\n      pval\n    \n  \n  \n    \n      0\n      AK\n      F\n      30.0\n      145\n      51272.0\n      28.280543\n      0.180215\n      0.0\n    \n    \n      1\n      AK\n      F\n      40.0\n      168\n      40674.0\n      41.304027\n      0.180215\n      0.0\n    \n    \n      2\n      AK\n      F\n      50.0\n      130\n      43451.0\n      29.918759\n      0.180215\n      0.0\n    \n    \n      3\n      AK\n      F\n      60.0\n      88\n      39819.0\n      22.100003\n      0.180215\n      0.0\n    \n    \n      4\n      AK\n      F\n      70.0\n      17\n      19719.0\n      8.621127\n      0.180215\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      813\n      WY\n      M\n      50.0\n      250\n      35040.0\n      71.347032\n      0.292119\n      0.0\n    \n    \n      814\n      WY\n      M\n      60.0\n      199\n      37838.0\n      52.592632\n      0.292119\n      0.0\n    \n    \n      815\n      WY\n      M\n      70.0\n      72\n      21138.0\n      34.061879\n      0.292119\n      0.0\n    \n    \n      816\n      WY\n      M\n      80.0\n      8\n      9006.0\n      8.882967\n      0.292119\n      0.0\n    \n    \n      817\n      WY\n      M\n      NaN\n      3\n      NaN\n      NaN\n      0.292119\n      0.0\n    \n  \n\n818 rows × 8 columns\n\n\n\n\n\n\nA key advantage of our data is that we can explicitly identify common ownership and individual places of work. Using data on common ownership – what are called firms, which are roughly equivalent to the concept of “health systems” in the health care arena – we can calculate the level of market concentration for specific geographies.\nCalculation: HHI (or consider CRn), using employed physicians as the measure of market power\nBreakdown by Geography: State, CBSA\nFollow-up question of interest: Market concentration of employeed physicians by certain specialties\n\n\nCode\n# merge physician data with establishment data\nphys_estab = physicians.merge(phys2estab[[\"NPI\", \"establishment_id\"]], on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')[[\"NPI\", \"st\", \"org_pac_id\"]]\n\n# add practice and then firm data\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\nphys_firm = phys_prac.merge(firms, on=\"health_sys_id\", how=\"left\")\n\n# get relevant columns for measuring market concentration\nphys_firm = phys_firm[[\"NPI\", \"health_sys_name\", \"health_sys_state\"]]\n\n\n\n\nCode\ndef sdi(data):\n    def p(n, N):\n        \"\"\" Relative abundance \"\"\"\n        if n == 0:\n            return 0\n        else:\n            return float(n)/N\n    N = sum(data)\n    return 1 - (sum(p(n, N)**2 for n in data if n != 0))\n\n# get count of physicians per health system and state\nphys_firm_cts, rmse = diff_count(phys_firm, [\"health_sys_name\", \"health_sys_state\"], \"NPI\", epsilon)\n\n# calculate diversity of state based on Simpson's Index\nphy_diversity = phys_firm_cts.groupby(\"health_sys_state\")[\"NPI\"].apply(list).apply(sdi).reset_index(name=\"sdi\")\n\n\n\n\nCode\nphy_diversity\n\n\n\n\n\n\n  \n    \n      \n      health_sys_state\n      sdi\n    \n  \n  \n    \n      0\n      AL\n      0.749003\n    \n    \n      1\n      AR\n      0.779147\n    \n    \n      2\n      AZ\n      0.476747\n    \n    \n      3\n      CA\n      0.859679\n    \n    \n      4\n      CO\n      0.555365\n    \n    \n      5\n      CT\n      0.680188\n    \n    \n      6\n      DC\n      0.043838\n    \n    \n      7\n      DE\n      0.286735\n    \n    \n      8\n      FL\n      0.852224\n    \n    \n      9\n      GA\n      0.886339\n    \n    \n      10\n      HI\n      0.584060\n    \n    \n      11\n      IA\n      0.641867\n    \n    \n      12\n      ID\n      0.233077\n    \n    \n      13\n      IL\n      0.816225\n    \n    \n      14\n      IN\n      0.814847\n    \n    \n      15\n      KS\n      0.575245\n    \n    \n      16\n      KY\n      0.805964\n    \n    \n      17\n      LA\n      0.790216\n    \n    \n      18\n      MA\n      0.837692\n    \n    \n      19\n      MD\n      0.852337\n    \n    \n      20\n      ME\n      0.684787\n    \n    \n      21\n      MI\n      0.787364\n    \n    \n      22\n      MN\n      0.813374\n    \n    \n      23\n      MO\n      0.787279\n    \n    \n      24\n      MS\n      0.827021\n    \n    \n      25\n      MT\n      0.771104\n    \n    \n      26\n      NC\n      0.818240\n    \n    \n      27\n      ND\n      0.458282\n    \n    \n      28\n      NE\n      0.787449\n    \n    \n      29\n      NH\n      0.751812\n    \n    \n      30\n      NJ\n      0.886927\n    \n    \n      31\n      NM\n      0.506308\n    \n    \n      32\n      NV\n      0.412152\n    \n    \n      33\n      NY\n      0.890892\n    \n    \n      34\n      OH\n      0.866450\n    \n    \n      35\n      OK\n      0.677279\n    \n    \n      36\n      OR\n      0.786595\n    \n    \n      37\n      PA\n      0.905693\n    \n    \n      38\n      RI\n      0.328761\n    \n    \n      39\n      SC\n      0.839665\n    \n    \n      40\n      SD\n      0.467802\n    \n    \n      41\n      TN\n      0.850914\n    \n    \n      42\n      TX\n      0.888860\n    \n    \n      43\n      UT\n      0.321171\n    \n    \n      44\n      VA\n      0.852326\n    \n    \n      45\n      VT\n      0.323371\n    \n    \n      46\n      WA\n      0.695122\n    \n    \n      47\n      WI\n      0.825265\n    \n    \n      48\n      WV\n      0.672337\n    \n    \n      49\n      WY\n      0.561602\n    \n  \n\n\n\n\n\n\n\nBased on the algorithm outlined in the PSEO paper.\nFinal formula for noisy percentile: y* = y + Spx(D)*Gamma(1/εx)\ny* is the protected percentile value\ny is the true value\nSpx(D) is the smooth sensitivity\nGamma(1/εx) is a draw from the gamma distribution\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndata_dir = \"\"\n\n# read files\nphysicians = pd.read_csv(data_dir + \"physicians.csv\")\nestablishments = pd.read_csv(data_dir + \"establishments.csv\", dtype={'zip': str, 'zip9': str, 'zip3': str})\npractices = pd.read_csv(data_dir + \"practices.csv\")\nfirms = pd.read_csv(data_dir + \"firms.csv\")\nphys2estab = pd.read_csv(data_dir + \"rel_phys2estab.csv\")\n\n# link physicians to establishments\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# add practices and firms\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\nphys_firm = phys_prac.merge(firms, on=\"health_sys_id\", how=\"left\")\n\n# define the columns we want to keep\ncolumns = [\"NPI\", \"st\", \"gndr\", \"age_imputed\", \"tin_name\", \"practice_type_random\", \"health_sys_state\", \"pri_spec_category\", \"practice_scope\", \"practice_size\", \"medicare_charges\"]\n\n# remove all other columns\ndataset = phys_firm[columns].drop_duplicates()\n\n# clean a few values\ndataset[\"age_imputed\"] = dataset[\"age_imputed\"].fillna(0)\ndataset[\"age_imputed\"] = np.where(dataset[\"age_imputed\"] > 85, 85, dataset[\"age_imputed\"].astype(int))\n\n# cast as int\ndataset[\"medicare_charges\"] = dataset[\"medicare_charges\"].astype(int)\n\n\n\n\nCode\n# percentiles we want to calculate\npctiles = [25, 50, 75]\n\n# num elements in dataset\nl = len(dataset)\n\n# define epsilon\n# epsilon will be evenly divided between each percentile we want to calculate\neps = 1\n\n# to keep track of final values\nprotected_pctiles = []\n\nfor pctile in pctiles:\n    # percentile position (index in the dataset where the percentile would be found)\n    ptile = int(l * pctile/100)\n\n    # complement of percentile position\n    ptile_comp = int(l * (100 - pctile) / 100)\n\n    # n is the number of observations between the given percentile and the end of the distribution (top or bottom)\n    # cap of 50\n    n1 = l - ptile - 2\n    n2 = l - ptile_comp - 2\n    n3 = 50\n    n = min(n1, n2, n3)\n\n    # define beta based on epsilon\n    beta = (eps/(100*len(pctiles)))/4\n\n    # order column we want percentiles of\n    X = sorted(dataset[\"medicare_charges\"])\n\n    # get sensitivity\n    # https://github.com/andrewfoote/pseo_dp_paper/blob/v2.0.0/macros/compute_ss_iteration.sas\n    A = []\n    for k in range(n):\n        cur = 0\n        for t in range(k+2):\n            x1 = X[ptile+t]\n            x2 = X[ptile+t-k-1]\n            if x1-x2 > cur:\n                cur=x1-x2\n        i = k*beta\n        A.append(cur/(math.exp(i)))\n\n    # find maximum sensitivity\n    sens = max(A)\n\n    # add noise based on sensitivity to true percentile\n    # https://github.com/andrewfoote/pseo_dp_paper/blob/v2.0.0/macros/dp_iteration.sas\n\n    # true value at percentile\n    true_val = np.percentile(X, pctile)\n\n    # add noise based on smooth sensitivity and gamme distribution\n    protected_pctile = true_val + (np.random.gamma(1) * sens)\n    \n    protected_pctiles.append([pctile, true_val, protected_pctile])\n\n\n\n\nCode\nprotected_pctiles\n\n\n[[25, 88243.75, 88302.00459038059],\n [50, 243214.5, 243226.8812943136],\n [75, 549093.0, 549110.4070669264]]"
  },
  {
    "objectID": "Examples - Synthetic Data.html",
    "href": "Examples - Synthetic Data.html",
    "title": "Privacy Models for E-Health",
    "section": "",
    "text": "CDF Summer Project 2022\n\n\n\n\nCode\n# load data science libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_dir = \"\"\n\n\n\n\nCode\nphysicians = pd.read_csv(data_dir + \"physicians.csv\")\nestablishments = pd.read_csv(data_dir + \"establishments.csv\", dtype={'zip': str, 'zip9': str, 'zip3': str})\npractices = pd.read_csv(data_dir + \"practices.csv\")\nfirms = pd.read_csv(data_dir + \"firms.csv\")\nphys2estab = pd.read_csv(data_dir + \"rel_phys2estab.csv\")\nphys2prac = pd.read_csv(data_dir + \"rel_phys2prac.csv\")\n\n\n\n\nThis portion of the notebook is meant to be run from your console/command prompt.\n\n\nI had issues with an SSL error the first time I tried installing CenSyn, and I solved that by updating pip:\npython -m pip install --upgrade --force-reinstall pip\n\n\n\nFrom the directory with the CenSyn wheel file, run:\npip install censyn-0.7.0-py3-none-any.whl\n\n\n\n\n\n\nThe core functions used in MST are from a github repo called private-pgm. They can be installed with:\npip install git+https://github.com/ryan112358/private-pgm.git\n\n\n\nMST uses a function called cdp2adp that comes from a github repo published by IBM. You can get them with:\ngit clone https://github.com/IBM/discrete-gaussian-differential-privacy.git\nmv discrete-gaussian-differential-privacy ibm\nThis will put the files needed for the function in a directory called ibm.\nYou’ll need to add this to your python file to actually use cdp2adp:\nimport sys\nimport os\nsys.path.append(os.getcwd() + \"/ibm\")\n\n\n\n\n\n\n\nCode\n# link physicians to establishments\nphys_estab = physicians.merge(phys2estab, on='NPI', how='left')\nphys_estab = phys_estab.merge(establishments, on='establishment_id', how='left')\n\n# add practices and firms\nphys_prac = phys_estab.merge(practices, on=\"org_pac_id\", how=\"left\")\nphys_firm = phys_prac.merge(firms, on=\"health_sys_id\", how=\"left\")\n\n# define the columns we want to keep\ncolumns = [\"NPI\", \"st\", \"gndr\", \"age_imputed\", \"tin_name\", \"practice_type_random\", \"health_sys_state\", \"pri_spec_category\", \"practice_scope\", \"practice_size\", \"medicare_charges\"]\n\n# remove all other columns\ndataset = phys_firm[columns].drop_duplicates()\n\n# clean a few values\ndataset[\"age_imputed\"] = dataset[\"age_imputed\"].fillna(0)\ndataset[\"age_imputed\"] = np.where(dataset[\"age_imputed\"] > 85, 85, dataset[\"age_imputed\"].astype(int))\n\n# bin the medicare charges\nlabels = [\"{0} - {1}\".format(i, i + 999999) for i in range(0, 55000000, 1000000)]\ndataset[\"medicare_charges\"] = dataset[\"medicare_charges\"].astype(int)\ndataset[\"medicare_charges\"] = pd.cut(dataset.medicare_charges, bins=np.arange(0, 56000000, 1000000), right=False, labels=labels)\n\n# get rid of nulls in non-numeric columns\nstring_cols = dataset.select_dtypes(include=[np.object]).columns\nfor col in string_cols:\n    dataset[col] = dataset[col].fillna(\"nan\")\n\n\n\n\nCode\ndataset\n\n\n\n\n\n\n  \n    \n      \n      NPI\n      st\n      gndr\n      age_imputed\n      tin_name\n      practice_type_random\n      health_sys_state\n      pri_spec_category\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      1003000126\n      VA\n      M\n      54\n      Commonwealth Hospitalist Group Llc\n      type-4\n      TN\n      Primary care\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      1\n      1003000134\n      IL\n      M\n      45\n      Northshore University Healthsystem Faculty Pra...\n      type-2\n      IL\n      Hospital based\n      multi-unit, single-cbsa\n      large (20-99)\n      1000000 - 1999999\n    \n    \n      9\n      1003000142\n      OH\n      M\n      49\n      Mercy Health Physicians - North, Llc.\n      type-2\n      OH\n      Hospital based\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      13\n      1003000407\n      PA\n      M\n      45\n      Brookville Hospital\n      type-3\n      PA\n      Primary care\n      single-unit\n      medium (6-19)\n      0 - 999999\n    \n    \n      14\n      1003000423\n      OH\n      F\n      41\n      Lake Obstetrics & Gynecology, Inc.\n      type-5\n      nan\n      OBGYN\n      multi-unit, single-cbsa\n      medium (6-19)\n      0 - 999999\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1203604\n      1992998736\n      FL\n      M\n      41\n      Premier Family & Sports Medicine, Llc\n      type-9\n      nan\n      Primary care\n      single-unit\n      small (2-5)\n      0 - 999999\n    \n    \n      1203605\n      1992999031\n      CA\n      M\n      42\n      Austin J Ma Md A Professional Corporation\n      type-8\n      nan\n      Medical specialty\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      1203606\n      1992999122\n      FL\n      M\n      65\n      nan\n      nan\n      nan\n      Primary care\n      nan\n      nan\n      0 - 999999\n    \n    \n      1203608\n      1992999551\n      CA\n      F\n      44\n      Graybill Medical Group Inc\n      type-9\n      nan\n      Primary care\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      1203609\n      1992999825\n      WA\n      M\n      43\n      Virginia Mason Medical Center\n      type-9\n      WA\n      Surgery specialty\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n  \n\n672614 rows × 11 columns\n\n\n\n\n\n\nThe remaining steps in the synthesis process can be seen in the E-Health DP Guide notebook. I’m going to assume you’ve run those steps and have synthetic datasets saved in the appropriate locations\n\n\nCode\nfrom sklearn import preprocessing\n\n# drop identifiers\ndataset_presyn = dataset.drop(columns=[\"NPI\", \"tin_name\"])\n\n# get a list of the columns in the dataset\ncols = dataset_presyn.columns\n\n# create a dict to store the mappings\nmapping = {}\n\nfor col in cols:\n    # create a mapping between the dataset's values and numbers\n    le = preprocessing.LabelEncoder()\n    le.fit(dataset_presyn[col])\n    \n    # keep track of the mapping\n    mapping[col] = [le, le.classes_]\n\n# read MST data from csv\nmst_synth = pd.read_csv(\"synth-data.csv\")\n\n# read CenSyn data from parquet\ncen_synth = pd.read_parquet(\"syn_data/syn.parquet\")\n\n# create empty DFs for decoded data\nsynth_df_mst = pd.DataFrame(index=range(len(mst_synth)), columns=cols)\nsynth_df_cen = pd.DataFrame(index=range(len(cen_synth)), columns=cols)\n\n# decode MST data using mappings\nfor key, value in mapping.items():\n    synth_df_mst[key] = value[0].inverse_transform(mst_synth[key].astype(int))\n\n# decode CenSyn data with the mappings\nfor key, value in mapping.items():\n    synth_df_cen[key] = value[0].inverse_transform(cen_synth[key].astype(int))\n\n\n\n\nCode\nsynth_df_mst\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      age_imputed\n      practice_type_random\n      health_sys_state\n      pri_spec_category\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      FL\n      F\n      36\n      type-2\n      FL\n      Primary care\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      1\n      TX\n      M\n      36\n      type-3\n      nan\n      Surgery specialty\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      2\n      NJ\n      F\n      38\n      type-9\n      nan\n      OBGYN\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      3\n      NY\n      M\n      58\n      type-8\n      NY\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      4\n      DC\n      F\n      51\n      type-4\n      nan\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      672629\n      TX\n      M\n      63\n      type-5\n      nan\n      Medical specialty\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      672630\n      NJ\n      M\n      55\n      type-2\n      nan\n      Hospital based\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      672631\n      CT\n      M\n      40\n      type-5\n      nan\n      Medical specialty\n      multi-unit, single-cbsa\n      medium (6-19)\n      0 - 999999\n    \n    \n      672632\n      IN\n      F\n      45\n      type-0\n      IN\n      Primary care\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      672633\n      LA\n      M\n      48\n      type-1\n      nan\n      Medical specialty\n      single-unit\n      large (20-99)\n      0 - 999999\n    \n  \n\n672634 rows × 9 columns\n\n\n\n\n\nCode\nsynth_df_cen\n\n\n\n\n\n\n  \n    \n      \n      st\n      gndr\n      age_imputed\n      practice_type_random\n      health_sys_state\n      pri_spec_category\n      practice_scope\n      practice_size\n      medicare_charges\n    \n  \n  \n    \n      0\n      NY\n      M\n      60\n      type-0\n      nan\n      Medical specialty\n      single-unit\n      medium (6-19)\n      0 - 999999\n    \n    \n      1\n      TX\n      M\n      85\n      type-7\n      TX\n      Medical specialty\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      2\n      GA\n      M\n      38\n      type-6\n      GA\n      Hospital based\n      multi-unit, single-state\n      large (20-99)\n      0 - 999999\n    \n    \n      3\n      NY\n      M\n      56\n      type-4\n      NY\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      4\n      NY\n      F\n      44\n      type-8\n      MI\n      Medical specialty\n      multi-unit, single-cbsa\n      large (20-99)\n      0 - 999999\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      672609\n      AK\n      M\n      70\n      type-3\n      nan\n      Surgery specialty\n      multi-unit, single-state\n      small (2-5)\n      1000000 - 1999999\n    \n    \n      672610\n      PA\n      M\n      58\n      type-5\n      PA\n      Medical specialty\n      multi-unit, multi-state\n      large (20-99)\n      0 - 999999\n    \n    \n      672611\n      PR\n      M\n      69\n      nan\n      nan\n      Primary care\n      nan\n      nan\n      0 - 999999\n    \n    \n      672612\n      SC\n      M\n      58\n      type-8\n      SC\n      Surgery specialty\n      single-unit\n      large (20-99)\n      0 - 999999\n    \n    \n      672613\n      MA\n      M\n      42\n      type-2\n      MA\n      Hospital based\n      multi-unit, single-cbsa\n      medium (6-19)\n      0 - 999999\n    \n  \n\n672614 rows × 9 columns\n\n\n\n\n\n\nWhile this research question can be answered using existing public data (including Census ACS), it’s included here as an example of a simple aggregate table that can be generated at different geographic levels. Note that the data above already loads population counts for cbsa and state-level statistics.\nBreakdown by Characteristics: Specialty Category\nBreakdown by Geography: Nation, State, CBSA\n\n\n\n\nCode\n# get relevant columns from original and synth datasets\nphys_estab_state = dataset[['NPI', 'st', 'pri_spec_category']]\nphys_estab_state_mst = synth_df_mst[['st', 'pri_spec_category']]\nphys_estab_state_cen = synth_df_cen[['st', 'pri_spec_category']]\n\n# phys per specialty\nphys_by_spec = phys_estab_state.groupby(\"pri_spec_category\").NPI.count()\n\n# phys per specialty from synth data\nphys_by_spec_mst = phys_estab_state_mst.groupby(\"pri_spec_category\").count()\nphys_by_spec_cen = phys_estab_state_cen.groupby(\"pri_spec_category\").count()\n\nmerged = pd.concat([phys_by_spec, phys_by_spec_mst, phys_by_spec_cen], axis=1)\nmerged.columns = [\"true\", \"mst\", \"cen\"]\nmerged\n\n\n\n\n\n\n  \n    \n      \n      true\n      mst\n      cen\n    \n    \n      pri_spec_category\n      \n      \n      \n    \n  \n  \n    \n      Hospital based\n      162826\n      162856\n      165826\n    \n    \n      Medical specialty\n      140489\n      140469\n      139313\n    \n    \n      OBGYN\n      29471\n      29497\n      29540\n    \n    \n      Primary care\n      202206\n      202194\n      201603\n    \n    \n      Psychiatry\n      23062\n      23037\n      23072\n    \n    \n      Surgery specialty\n      114560\n      114581\n      113260\n    \n  \n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt \n\nx_len = np.arange(len(merged))\nplt.bar(x_len - 0.2, merged[\"true\"], 0.2, label = \"True Count\")\nplt.bar(x_len, merged[\"mst\"], 0.2, label = \"MST Count\")\nplt.bar(x_len + 0.2, merged[\"cen\"], 0.2, label = \"CenSyn Count\")\n  \nplt.xticks(x_len, list(merged.index))\nplt.xlabel(\"Specialties\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\nrmse = mean_squared_error(merged[\"true\"], merged[\"mst\"])\nprint(\"MST RMSE: \", rmse)\n\nrmse = mean_squared_error(merged[\"true\"], merged[\"cen\"])\nprint(\"CenSyn RMSE: \", rmse)\n\n\nMST RMSE:  531.0\nCenSyn RMSE:  2073574.3333333333\n\n\n\n\n\n\n\nCode\n# calculate specialty per state\nphys_by_state_spec = phys_estab_state.groupby([\"st\", \"pri_spec_category\"]).NPI.count()\n\n# replicate calculation with synth data\nphys_by_state_spec_mst = phys_estab_state_mst.groupby([\"st\", \"pri_spec_category\"]).size()\nphys_by_state_spec_cen = phys_estab_state_cen.groupby([\"st\", \"pri_spec_category\"]).size()\n\n# put the true and the synth data together\nmerged = pd.concat([phys_by_state_spec, phys_by_state_spec_mst, phys_by_state_spec_cen], axis=1).fillna(0)\nmerged.columns = [\"true\", \"mst\", \"cen\"]\nmerged\n\n\n\n\n\n\n  \n    \n      \n      \n      true\n      mst\n      cen\n    \n    \n      st\n      pri_spec_category\n      \n      \n      \n    \n  \n  \n    \n      AK\n      Hospital based\n      503.0\n      477\n      422\n    \n    \n      Medical specialty\n      259.0\n      340\n      317\n    \n    \n      OBGYN\n      78.0\n      77\n      71\n    \n    \n      Primary care\n      583.0\n      495\n      570\n    \n    \n      Psychiatry\n      52.0\n      63\n      50\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      WY\n      Medical specialty\n      228.0\n      267\n      232\n    \n    \n      OBGYN\n      53.0\n      52\n      45\n    \n    \n      Primary care\n      361.0\n      379\n      342\n    \n    \n      Psychiatry\n      25.0\n      35\n      29\n    \n    \n      Surgery specialty\n      272.0\n      224\n      229\n    \n  \n\n330 rows × 3 columns\n\n\n\n\n\nCode\n# get subset to make graph manageable\nmerged_h = merged.head(20)\n\nx_len = np.arange(len(merged_h))\nplt.bar(x_len - 0.2, merged_h[\"true\"], 0.2, label = \"True Count\")\nplt.bar(x_len, merged_h[\"mst\"], 0.2, label = \"MST Count\")\nplt.bar(x_len + 0.2, merged_h[\"cen\"], 0.2, label = \"CenSyn Count\")\n  \nplt.xticks(x_len, list(merged_h.index))\nplt.xlabel(\"Specialty + St\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nrmse = mean_squared_error(merged[\"true\"], merged[\"mst\"])\nprint(\"MST RMSE: \", rmse)\n\nrmse = mean_squared_error(merged[\"true\"], merged[\"cen\"])\nprint(\"CenSyn RMSE: \", rmse)\n\n\nMST RMSE:  71889.23636363636\nCenSyn RMSE:  40125.24848484848\n\n\n\n\nCode\n# Calculate the number of physicians per 10,000 residents for each CBSA\n# (this would be done the same way as the state data, but would need to resynthesize data with CBSA)\n\n\n\n\n\n\nOur work will provide insights on the institutional structures through which physicians provide care. These structures have changed dramatically over time as a consequence of intensive vertical integration in the healthcare industry.\nThis research question is mostly centered on creating counts or percent breakdowns by state and/or metropolitan area.\nThe main task is to translate business characteristics into distinct “types” by which to group physicians.\nFollow-up question of interest: Rural/urban split (note: need to obtain data)\n\n\nCode\n# get practice_type column, location, and identifier\nphys_prac_state = dataset[['NPI', 'st', 'practice_type_random']]\nphys_prac_state_mst = synth_df_mst[['st', 'practice_type_random']]\nphys_prac_state_cen = synth_df_cen[['st', 'practice_type_random']]\n\n# calculate practice type per state\nphys_by_state_prac = phys_prac_state.groupby([\"st\", \"practice_type_random\"]).NPI.count()\n\n# replicate calculation with synth data\nphys_by_state_prac_mst = phys_prac_state_mst.groupby([\"st\", \"practice_type_random\"]).size()\nphys_by_state_prac_cen = phys_prac_state_cen.groupby([\"st\", \"practice_type_random\"]).size()\n\n# put the true and the synth data together\nmerged = pd.concat([phys_by_state_prac, phys_by_state_prac_mst, phys_by_state_prac_cen], axis=1).fillna(0)\nmerged.columns = [\"true\", \"mst\", \"cen\"]\nmerged\n\n\n\n\n\n\n  \n    \n      \n      \n      true\n      mst\n      cen\n    \n    \n      st\n      practice_type_random\n      \n      \n      \n    \n  \n  \n    \n      AK\n      nan\n      255.0\n      460\n      218\n    \n    \n      type-0\n      97.0\n      133\n      107\n    \n    \n      type-1\n      156.0\n      127\n      133\n    \n    \n      type-2\n      221.0\n      131\n      199\n    \n    \n      type-3\n      27.0\n      147\n      26\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      WY\n      type-5\n      40.0\n      105\n      43\n    \n    \n      type-6\n      144.0\n      97\n      138\n    \n    \n      type-7\n      127.0\n      111\n      127\n    \n    \n      type-8\n      128.0\n      90\n      123\n    \n    \n      type-9\n      132.0\n      94\n      118\n    \n  \n\n605 rows × 3 columns\n\n\n\n\n\nCode\nmerged_h = merged.head(20)\n\nx_len = np.arange(len(merged_h))\nplt.bar(x_len - 0.2, merged_h[\"true\"], 0.2, label = \"True Count\")\nplt.bar(x_len, merged_h[\"mst\"], 0.2, label = \"MST Count\")\nplt.bar(x_len + 0.2, merged_h[\"cen\"], 0.2, label = \"CenSyn Count\")\n  \nplt.xticks(x_len, list(merged_h.index))\nplt.xlabel(\"Practice Type + St\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nrmse = mean_squared_error(merged[\"true\"], merged[\"mst\"])\nprint(\"MST RMSE: \", rmse)\n\nrmse = mean_squared_error(merged[\"true\"], merged[\"cen\"])\nprint(\"CenSyn RMSE: \", rmse)\n\n\nMST RMSE:  275849.94049586775\nCenSyn RMSE:  10311.61652892562\n\n\n\n\nCode\n# could also add per capita stats"
  }
]